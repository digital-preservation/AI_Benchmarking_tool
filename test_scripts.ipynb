{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading docx**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In reviewing the correspondence it is clear that t.docx\n"
     ]
    }
   ],
   "source": [
    "F = 'C:/Users/svenkata/Desktop\\In reviewing the correspondence it is clear that the substance of your complaint regarding the wall boards is a continuation of the issues raised in your previous complaints.docx'\n",
    "\n",
    "\n",
    "filename = F.split('\\\\')[-1]\n",
    "\n",
    "temp = filename.split('.')\n",
    "newName = temp[0][0:50] +'.'+temp[-1]\n",
    "os.rename(r'C:/Users/svenkata/Desktop\\In reviewing the correspondence it is clear that the substance of your complaint regarding the wall boards is a continuation of the issues raised in your previous complaints.docx',\n",
    "          r'C:/Users/svenkata/Desktop\\\\'+newName)\n",
    "print(newName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='C:/Users/svenkata/Desktop/In reviewing the correspondence it is clear that t.docx' mode='r' encoding='cp1252'>\n"
     ]
    }
   ],
   "source": [
    "F = 'C:/Users/svenkata/Desktop/'+newName\n",
    "with open(F) as f:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In reviewing the correspondence it is clear that the substance of your complaint regarding the wall boards is a continuation of the issues raised in your previous complaints. Changes were made to all three boards as a result of your previous complaints, Slavery (2010), Jean Barbot (2013) and Power and the People (2016). It is clear from your correspondence that you are still dissatisfied with the content of these specific wall boards, but a further review would not be appropriate as the boards have been reviewed and amended, and the content in the Keeper’s Gallery has not changed significantly enough to warrant a re-review. The letter from Jodi Berg, Independent Complaints Reviewer, dated 7th October 2016 (attached for your reference) outlines that ‘TNA responded to say that it takes the view that your complaint has been investigated exhaustively and [that] matters are now at a close in terms of internal process’. I would draw you attention to section 10 of the , which explains that we can refuse to investigate ‘if they [the complainant] refuse to accept that their complaint has been dealt with despite a thorough investigation on our part’. Appendix A of our  policy details circumstances when we could invoke that policy to manage an enquiry that is repeated. I appreciate that you will be disappointed that my review will not re-examine the content of the wall boards.\n",
      "\n",
      "In reviewing the correspondence it is clear that the substance of your complaint regarding the wall boards is a continuation of the issues raised in your previous complaints. The letter from Jodi Berg, Independent Complaints Reviewer, dated 7th October 2016 (attached for your reference) outlines that ‘TNA responded to say that it takes the view that your complaint has been investigated exhaustively and [that] matters are now at a close in terms of internal process’. Changes were made to both the Slavery and Power and the People boards as a result of your previous two complaints. It is clear from your correspondence that you are still dissatisfied with the content of the wall boards, but a further review would not be appropriate as the boards have been reviewed and amended, and the content in the Keeper’s Gallery has not changed significantly enough to warrant a re-review. I would draw you attention to section 10 of the , which explains that we can refuse to investigate ‘if they [the complainant] refuse to accept that their complaint has been dealt with despite a thorough investigation on our part’. Appendix A of our  policy details circumstances when we could invoke that policy to manage an enquiry that is repeated. I appreciate that you will be disappointed that my review will not re-examine the content of the wall boards.\n",
      "As previously explained the captions are limited to fifty words and the text provides a short introduction or brief overview on particular display topics. The caption does not provide a comprehensive analysis or a definite narrative of any aspect of history, but provides a taster to inspire visitors to want to find out more.\n",
      ".\n",
      "\n",
      "\n",
      "In your complaint you stated that ‘It is simply impossible to see how this card could be deemed to have been adequately researched’ and that ‘It is inaccurate and meaningless to suggest that officials decided on Independence and difficult to credit that anybody versed in the subject would write this. Where is the evidence that international sentiment affected the decision to grant independence in 1947?’   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The Records Specialist from our Collections, Expertise and Engagement Department, explained that:\n",
      "This is a contextualising sentence which makes no claim to causation. It merely states that international opinion as evidenced by, for example, the Atlantic Charter, which was in favour of the self-determination of nations, and the United Nations Charter, Article 73b of which states:\n",
      "“To develop self-government, to take due account of the political aspirations of the peoples, and to assist them in the progressive development of their free political institutions, according to the particular circumstances of each territory and its peoples and their varying stages of advancement.”\n",
      "ran counter to the idea of formal empire.\n",
      "It is not claimed that officials decided that India would be independent, but that at that time Indian independence (but not partition) was seen within the British government as inevitable. Indeed, the new Prime Minister, Clement Attlee, regarded independence as desirable. \n",
      "The complainant correctly writes that discussions about Indian self-government had a long history, but the point here is that it was in the immediate post war years that this was achieved.\n",
      "\n",
      "Within this parliamentary democracy, officials, under the direction of ministers, may research and draft policy documents which may be considered and decided upon by ministers. Before such decisions pass into law they are generally decided upon by Parliament, as was the Independence of India Act, 1947. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "As previously explained the captions are limited to fifty words and are designed to give a taster to a subject. There is no scope to fully explain the subject explored, the desire is to encourage visitors to explore the subject in more detail and draw their own conclusions.\n",
      "The Records Specialist from our Collections, Expertise and Engagement Department, explained that:\n",
      "This is a contextualising sentence which states that international opinion ran counter to the idea of formal empire. This can be evidenced by, for example, the Atlantic Charter and , Article 73b. It is not claimed that officials decided that India would be independent, but that at that time Indian independence (but not partition) was seen within the British government as inevitable. The new Prime Minister, Clement Attlee, regarded independence as desirable. It is well documented that within a parliamentary democracy, officials, under the direction of ministers, may research and draft policy documents, which may be considered and decided upon by ministers. Before such decisions pass into law they are generally decided upon by Parliament, as the Independence of India Act, 1947 was. The discussion about Indian self-government has a long history, but the point here is that it was in the immediate post war years that this was achieved.\n",
      "I discussed the content of the caption with a Records Specialist from our Collections, Expertise and Engagement Department and I am satisfied that the caption was created from good quality research, which gives sufficient contextualisation to the display. A number of points and issues were considered when compiling the sentence:\n",
      "International opinion ran counter to the idea of formal empire, this can be evidenced if you consider examples like, the Atlantic Charter and , Article 73b\n",
      "At that time Indian independence (but not partition) was seen within the British government as inevitable\n",
      "And the new Prime Minister, Clement Attlee, regarded independence as desirable.\n",
      "It is well documented that within a parliamentary democracy, officials, under the direction of ministers, may research and draft policy documents, which may be considered and decided upon by ministers. Before such decisions pass into law they are generally decided upon by Parliament, as the Independence of India Act, 1947 was\n",
      "The discussion about Indian self-government has a long history, but the point here is that it was in the immediate post war years that this was achieved.\n",
      "The Records Specialist from our Collections, Expertise and Engagement Department, explained that:\n",
      "This is a contextualising sentence which states that international opinion ran counter to the idea of formal empire. This can be evidenced by, for example, the Atlantic Charter and United Nations Charter, Article 73b. It is not claimed that officials decided that India would be independent, but that at that time Indian independence (but not partition) was seen within the British government as inevitable. The new Prime Minister, Clement Attlee, regarded independence as desirable. It is well documented that within a parliamentary democracy, officials, under the direction of ministers, may research and draft policy documents, which may be considered and decided upon by ministers. Before such decisions pass into law they are generally decided upon by Parliament, as the Independence of India Act, 1947 was. The discussion about Indian self-government has a long history, but the point here is that it was in the immediate post war years that this was achieved.\n",
      "I am satisfied that the caption was well researched and contextualised the subject at hand, it gave enough information to allow a visitor explore the subject in more detail.\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "document = Document(F)\n",
    "for p in document.paragraphs:\n",
    "    print (p.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In reviewing the correspondence it is clear that the substance of your complaint regarding the wall boards is a continuation of the issues raised in your previous complaints. Changes were made to all three boards as a result of your previous complaints, Slavery (2010), Jean Barbot (2013) and Power and the People (2016). It is clear from your correspondence that you are still dissatisfied with the content of these specific wall boards, but a further review would not be appropriate as the boards have been reviewed and amended, and the content in the Keeper’s Gallery has not changed significantly enough to warrant a re-review. The letter from Jodi Berg, Independent Complaints Reviewer, dated 7th October 2016 (attached for your reference) outlines that ‘TNA responded to say that it takes the view that your complaint has been investigated exhaustively and [that] matters are now at a close in terms of internal process’. I would draw you attention to section 10 of the , which explains that we can refuse to investigate ‘if they [the complainant] refuse to accept that their complaint has been dealt with despite a thorough investigation on our part’. Appendix A of our  policy details circumstances when we could invoke that policy to manage an enquiry that is repeated. I appreciate that you will be disappointed that my review will not re-examine the content of the wall boards.\\n\\nIn reviewing the correspondence it is clear that the substance of your complaint regarding the wall boards is a continuation of the issues raised in your previous complaints. The letter from Jodi Berg, Independent Complaints Reviewer, dated 7th October 2016 (attached for your reference) outlines that ‘TNA responded to say that it takes the view that your complaint has been investigated exhaustively and [that] matters are now at a close in terms of internal process’. Changes were made to both the Slavery and Power and the People boards as a result of your previous two complaints. It is clear from your correspondence that you are still dissatisfied with the content of the wall boards, but a further review would not be appropriate as the boards have been reviewed and amended, and the content in the Keeper’s Gallery has not changed significantly enough to warrant a re-review. I would draw you attention to section 10 of the , which explains that we can refuse to investigate ‘if they [the complainant] refuse to accept that their complaint has been dealt with despite a thorough investigation on our part’. Appendix A of our  policy details circumstances when we could invoke that policy to manage an enquiry that is repeated. I appreciate that you will be disappointed that my review will not re-examine the content of the wall boards.\\nAs previously explained the captions are limited to fifty words and the text provides a short introduction or brief overview on particular display topics. The caption does not provide a comprehensive analysis or a definite narrative of any aspect of history, but provides a taster to inspire visitors to want to find out more.\\n.\\n\\n\\nIn your complaint you stated that ‘It is simply impossible to see how this card could be deemed to have been adequately researched’ and that ‘It is inaccurate and meaningless to suggest that officials decided on Independence and difficult to credit that anybody versed in the subject would write this. Where is the evidence that international sentiment affected the decision to grant independence in 1947?’   \\n\\n\\n\\n\\nThe Records Specialist from our Collections, Expertise and Engagement Department, explained that:\\nThis is a contextualising sentence which makes no claim to causation. It merely states that international opinion as evidenced by, for example, the Atlantic Charter, which was in favour of the self-determination of nations, and the United Nations Charter, Article 73b of which states:\\n“To develop self-government, to take due account of the political aspirations of the peoples, and to assist them in the progressive development of their free political institutions, according to the particular circumstances of each territory and its peoples and their varying stages of advancement.”\\nran counter to the idea of formal empire.\\nIt is not claimed that officials decided that India would be independent, but that at that time Indian independence (but not partition) was seen within the British government as inevitable. Indeed, the new Prime Minister, Clement Attlee, regarded independence as desirable. \\nThe complainant correctly writes that discussions about Indian self-government had a long history, but the point here is that it was in the immediate post war years that this was achieved.\\n\\nWithin this parliamentary democracy, officials, under the direction of ministers, may research and draft policy documents which may be considered and decided upon by ministers. Before such decisions pass into law they are generally decided upon by Parliament, as was the Independence of India Act, 1947. \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAs previously explained the captions are limited to fifty words and are designed to give a taster to a subject. There is no scope to fully explain the subject explored, the desire is to encourage visitors to explore the subject in more detail and draw their own conclusions.\\nThe Records Specialist from our Collections, Expertise and Engagement Department, explained that:\\nThis is a contextualising sentence which states that international opinion ran counter to the idea of formal empire. This can be evidenced by, for example, the Atlantic Charter and , Article 73b. It is not claimed that officials decided that India would be independent, but that at that time Indian independence (but not partition) was seen within the British government as inevitable. The new Prime Minister, Clement Attlee, regarded independence as desirable. It is well documented that within a parliamentary democracy, officials, under the direction of ministers, may research and draft policy documents, which may be considered and decided upon by ministers. Before such decisions pass into law they are generally decided upon by Parliament, as the Independence of India Act, 1947 was. The discussion about Indian self-government has a long history, but the point here is that it was in the immediate post war years that this was achieved.\\nI discussed the content of the caption with a Records Specialist from our Collections, Expertise and Engagement Department and I am satisfied that the caption was created from good quality research, which gives sufficient contextualisation to the display. A number of points and issues were considered when compiling the sentence:\\nInternational opinion ran counter to the idea of formal empire, this can be evidenced if you consider examples like, the Atlantic Charter and , Article 73b\\nAt that time Indian independence (but not partition) was seen within the British government as inevitable\\nAnd the new Prime Minister, Clement Attlee, regarded independence as desirable.\\nIt is well documented that within a parliamentary democracy, officials, under the direction of ministers, may research and draft policy documents, which may be considered and decided upon by ministers. Before such decisions pass into law they are generally decided upon by Parliament, as the Independence of India Act, 1947 was\\nThe discussion about Indian self-government has a long history, but the point here is that it was in the immediate post war years that this was achieved.\\nThe Records Specialist from our Collections, Expertise and Engagement Department, explained that:\\nThis is a contextualising sentence which states that international opinion ran counter to the idea of formal empire. This can be evidenced by, for example, the Atlantic Charter and United Nations Charter, Article 73b. It is not claimed that officials decided that India would be independent, but that at that time Indian independence (but not partition) was seen within the British government as inevitable. The new Prime Minister, Clement Attlee, regarded independence as desirable. It is well documented that within a parliamentary democracy, officials, under the direction of ministers, may research and draft policy documents, which may be considered and decided upon by ministers. Before such decisions pass into law they are generally decided upon by Parliament, as the Independence of India Act, 1947 was. The discussion about Indian self-government has a long history, but the point here is that it was in the immediate post war years that this was achieved.\\nI am satisfied that the caption was well researched and contextualised the subject at hand, it gave enough information to allow a visitor explore the subject in more detail.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import docx\n",
    "\n",
    "def getText(filename):\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return '\\n'.join(fullText)\n",
    "\n",
    "getText(F)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading pdf**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PyPDF2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-82ce933afad3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Reading pdf using pypdf2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'C:/Users/svenkata/Dropbox/AI_for_Selection/Literature/1708.02709.pdf'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpdfFileObj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpdfReader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPyPDF2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPdfFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpdfFileObj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'PyPDF2'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reading pdf using pypdf2\n",
    "f = 'C:/Users/svenkata/Dropbox/AI_for_Selection/Literature/1708.02709.pdf'\n",
    "import PyPDF2\n",
    "pdfFileObj = open(f, 'rb')\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "\n",
    "print(pdfReader.numPages)\n",
    "\n",
    "pageObj = pdfReader.getPage(0)\n",
    "pageObj.extractText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfminer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "1\n",
      "0\n",
      "2\n",
      "\n",
      " \n",
      "\n",
      "v\n",
      "o\n",
      "N\n",
      "5\n",
      "2\n",
      "\n",
      " \n",
      "\n",
      " \n",
      " \n",
      "]\n",
      "L\n",
      "C\n",
      ".\n",
      "s\n",
      "c\n",
      "[\n",
      " \n",
      " \n",
      "\n",
      "8\n",
      "v\n",
      "9\n",
      "0\n",
      "7\n",
      "2\n",
      "0\n",
      "\n",
      ".\n",
      "\n",
      "8\n",
      "0\n",
      "7\n",
      "1\n",
      ":\n",
      "v\n",
      "i\n",
      "X\n",
      "r\n",
      "a\n",
      "\n",
      "Recent Trends in Deep Learning Based\n",
      "\n",
      "Natural Language Processing\n",
      "\n",
      "Tom Young†≡, Devamanyu Hazarika‡≡, Soujanya Poria⊕≡, Erik Cambria(cid:53)∗\n",
      "\n",
      "1\n",
      "\n",
      "† School of Information and Electronics, Beijing Institute of Technology, China\n",
      "\n",
      "‡ School of Computing, National University of Singapore, Singapore\n",
      "⊕ Temasek Laboratories, Nanyang Technological University, Singapore\n",
      "\n",
      "(cid:53) School of Computer Science and Engineering, Nanyang Technological University, Singapore\n",
      "\n",
      "Abstract\n",
      "\n",
      "Deep learning methods employ multiple processing layers to learn hierarchical representations of data, and have produced\n",
      "state-of-the-art results in many domains. Recently, a variety of model designs and methods have blossomed in the context of\n",
      "natural language processing (NLP). In this paper, we review signiﬁcant deep learning related models and methods that have been\n",
      "employed for numerous NLP tasks and provide a walk-through of their evolution. We also summarize, compare and contrast the\n",
      "various models and put forward a detailed understanding of the past, present and future of deep learning in NLP.\n",
      "\n",
      "Natural Language Processing, Deep Learning, Word2Vec, Attention, Recurrent Neural Networks, Convolutional Neural Net-\n",
      "works, LSTM, Sentiment Analysis, Question Answering, Dialogue Systems, Parsing, Named-Entity Recognition, POS Tagging,\n",
      "Semantic Role Labeling\n",
      "\n",
      "Index Terms\n",
      "\n",
      "I. INTRODUCTION\n",
      "\n",
      "Natural language processing (NLP) is a theory-motivated range of computational techniques for the automatic analysis and\n",
      "representation of human language. NLP research has evolved from the era of punch cards and batch processing, in which the\n",
      "analysis of a sentence could take up to 7 minutes, to the era of Google and the likes of it, in which millions of webpages can\n",
      "be processed in less than a second [1]. NLP enables computers to perform a wide range of natural language related tasks at\n",
      "all levels, ranging from parsing and part-of-speech (POS) tagging, to machine translation and dialogue systems.\n",
      "\n",
      "Deep learning architectures and algorithms have already made impressive advances in ﬁelds such as computer vision and\n",
      "pattern recognition. Following this trend, recent NLP research is now increasingly focusing on the use of new deep learning\n",
      "methods (see Figure 1). For decades, machine learning approaches targeting NLP problems have been based on shallow models\n",
      "(e.g., SVM and logistic regression) trained on very high dimensional and sparse features. In the last few years, neural networks\n",
      "based on dense vector representations have been producing superior results on various NLP tasks. This trend is sparked by\n",
      "the success of word embeddings [2, 3] and deep learning methods [4]. Deep learning enables multi-level automatic feature\n",
      "representation learning. In contrast, traditional machine learning based NLP systems liaise heavily on hand-crafted features.\n",
      "Such hand-crafted features are time-consuming and often incomplete.\n",
      "\n",
      "Collobert et al. [5] demonstrated that a simple deep learning framework outperforms most state-of-the-art approaches in\n",
      "several NLP tasks such as named-entity recognition (NER), semantic role labeling (SRL), and POS tagging. Since then,\n",
      "numerous complex deep learning based algorithms have been proposed to solve difﬁcult NLP tasks. We review major deep\n",
      "learning related models and methods applied to natural language tasks such as convolutional neural networks (CNNs), recurrent\n",
      "neural networks (RNNs), and recursive neural networks. We also discuss memory-augmenting strategies, attention mechanisms\n",
      "and how unsupervised models, reinforcement learning methods and recently, deep generative models have been employed for\n",
      "language-related tasks.\n",
      "\n",
      "To the best of our knowledge, this work is the ﬁrst of its type to comprehensively cover the most popular deep learning\n",
      "methods in NLP research today 1. The work by Goldberg [6] only presented the basic principles for applying neural networks\n",
      "to NLP in a tutorial manner. We believe this paper will give readers a more comprehensive idea of current practices in this\n",
      "domain.\n",
      "\n",
      "The structure of the paper is as follows: Section II introduces the concept of distributed representation, the basis of\n",
      "sophisticated deep learning models; next, Sections III, IV, and V discuss popular models such as convolutional, recurrent,\n",
      "and recursive neural networks, as well as their use in various NLP tasks; following, Section VI lists recent applications of\n",
      "reinforcement learning in NLP and new developments in unsupervised sentence representation learning; later, Section VII\n",
      "\n",
      "≡ means authors contributed equally\n",
      "∗ Corresponding author (e-mail: cambria@ntu.edu.sg)\n",
      "1We intend to update this article with time as and when signiﬁcant advances are proposed and used by the community\n",
      "\n",
      "\f",
      "2\n",
      "\n",
      "Fig. 1: Percentage of deep learning papers in ACL, EMNLP, EACL, NAACL over the last 6 years (long papers).\n",
      "\n",
      "illustrates the recent trend of coupling deep learning models with memory modules; ﬁnally, Section VIII summarizes the\n",
      "performance of a series of deep learning methods on standard datasets about major NLP topics.\n",
      "\n",
      "Statistical NLP has emerged as the primary option for modeling complex natural language tasks. However, in its beginning,\n",
      "it often used to suffer from the notorious curse of dimensionality while learning joint probability functions of language models.\n",
      "This led to the motivation of learning distributed representations of words existing in low-dimensional space [7].\n",
      "\n",
      "II. DISTRIBUTED REPRESENTATION\n",
      "\n",
      "A. Word Embeddings\n",
      "\n",
      "Distributional vectors or word embeddings (Fig. 2) essentially follow the distributional hypothesis, according to which words\n",
      "with similar meanings tend to occur in similar context. Thus, these vectors try to capture the characteristics of the neighbors of a\n",
      "word. The main advantage of distributional vectors is that they capture similarity between words. Measuring similarity between\n",
      "vectors is possible, using measures such as cosine similarity. Word embeddings are often used as the ﬁrst data processing layer\n",
      "in a deep learning model. Typically, word embeddings are pre-trained by optimizing an auxiliary objective in a large unlabeled\n",
      "corpus, such as predicting a word based on its context [8, 3], where the learned word vectors can capture general syntactical\n",
      "and semantic information. Thus, these embeddings have proven to be efﬁcient in capturing context similarity, analogies and\n",
      "due to its smaller dimensionality, are fast and efﬁcient in processing core NLP tasks.\n",
      "\n",
      "Over the years, the models that create such embeddings have been shallow neural networks and there has not been need\n",
      "for deep networks to create good embeddings. However, deep learning based NLP models invariably represent their words,\n",
      "phrases and even sentences using these embeddings. This is in fact a major difference between traditional word count based\n",
      "models and deep learning based models. Word embeddings have been responsible for state-of-the-art results in a wide range\n",
      "of NLP tasks [9, 10, 11, 12].\n",
      "\n",
      "For example, Glorot et al. [13] used embeddings along with stacked denoising autoencoders for domain adaptation in senti-\n",
      "ment classiﬁcation and Hermann and Blunsom [14] presented combinatory categorial autoencoders to learn the compositionality\n",
      "of sentence. Their wide usage across the recent literature shows their effectiveness and importance in any deep learning model\n",
      "performing a NLP task.\n",
      "\n",
      "Distributed representations (embeddings) are mainly learned through context. During 1990s, several research develop-\n",
      "ments [15] marked the foundations of research in distributional semantics. A more detailed summary of these early trends is\n",
      "\n",
      "Fig. 2: Distributional vectors represented by a D-dimensional vector where D << V, where V is size of Vocabulary. Figure\n",
      "Source: http://veredshwartz.blogspot.sg.\n",
      "\n",
      "King(-) Man(+) WomanQueen\f",
      "3\n",
      "\n",
      "Fig. 3: Neural Language Model (Figure reproduced from Bengio et al. [7]). C(i) is the ith word embedding.\n",
      "\n",
      "provided in [16, 17]. Later developments were adaptations of these early works, which led to creation of topic models like\n",
      "latent Dirichlet allocation [18] and language models [7]. These works laid out the foundations of representation learning in\n",
      "natural language.\n",
      "\n",
      "In 2003, Bengio et al. [7] proposed a neural language model which learned distributed representations for words (Fig. 3).\n",
      "Authors argued that these word representations, once compiled into sentence representations using joint probability of word\n",
      "sequences, achieved an exponential number of semantically neighboring sentences. This, in turn, helped in generalization\n",
      "since unseen sentences could now gather higher conﬁdence if word sequences with similar words (in respect to nearby word\n",
      "representation) were already seen.\n",
      "\n",
      "Collobert and Weston [19] were the ﬁrst work to show the utility of pre-trained word embeddings. They proposed a neural\n",
      "network architecture that forms the foundation to many current approaches. The work also establishes word embeddings as\n",
      "a useful tool for NLP tasks. However, the immense popularization of word embeddings was arguably due to Mikolov et al.\n",
      "[3] who proposed the continuous bag-of-words (CBOW) and skip-gram models to efﬁciently construct high-quality distributed\n",
      "vector representations. Propelling their popularity was the unexpected side effect of the vectors exhibiting compositionality, i.e.,\n",
      "adding two word vectors results in a vector that is a semantic composite of the individual words, e.g., ‘man’ + ‘royal’ = ‘king’.\n",
      "The theoretical justiﬁcation for this behavior was recently given by Gittens et al. [20], which stated that compositionality is\n",
      "seen only when certain assumptions are held, e.g., the assumption that words need to be uniformly distributed in the embedding\n",
      "space.\n",
      "\n",
      "Glove by Pennington et al. [21] is another famous word embedding method which is essentially a “count-based” model.\n",
      "Here, the word co-occurrence count matrix is pre-processed by normalizing the counts and log-smoothing operation. This\n",
      "matrix is then factorized to get lower dimensional representations which is done by minimizing a “reconstruction loss”.\n",
      "\n",
      "Below, we provide a brief description of the word2vec method proposed by Mikolov et al. [3].\n",
      "\n",
      "B. Word2vec\n",
      "\n",
      "Word embeddings were revolutionized by Mikolov et al. [8, 3] who proposed the CBOW and skip-gram models. CBOW\n",
      "computes the conditional probability of a target word given the context words surrounding it across a window of size k. On\n",
      "the other hand, the skip-gram model does the exact opposite of the CBOW model, by predicting the surrounding context words\n",
      "given the central target word. The context words are assumed to be located symmetrically to the target words within a distance\n",
      "equal to the window size in both directions. In unsupervised settings, the word embedding dimension is determined by the\n",
      "accuracy of prediction. As the embedding dimension increases, the accuracy of prediction also increases until it converges at\n",
      "some point, which is considered the optimal embedding dimension as it is the shortest without compromising accuracy.\n",
      "\n",
      "Let us consider a simpliﬁed version of the CBOW model where only one word is considered in the context. This essentially\n",
      "\n",
      "replicates a bigram language model.\n",
      "\n",
      "As shown in Fig. 4, the CBOW model is a simple fully connected neural network with one hidden layer. The input layer,\n",
      "which takes the one-hot vector of context word has V neurons while the hidden layer has N neurons. The output layer is softmax\n",
      "probability over all words in the vocabulary. The layers are connected by weight matrix W ∈ RV ×N and W\n",
      "(cid:48) ∈ RH×V ,\n",
      "\n",
      "Table look-up using matrix Cword indexword indexithoutput=P(wt=i∣context)C(wt−n+1)C(wt−1)wt−n+1wt−1Softmax classiﬁcationTanh activationConcatenation\f",
      "4\n",
      "\n",
      "Fig. 4: Model for CBOW (Figure source: Rong [22])\n",
      "\n",
      "respectively. Each word from the vocabulary is ﬁnally represented as two learned vectors vc and vw, corresponding to context\n",
      "and target word representations, respectively. Thus, kth word in the vocabulary will have\n",
      "\n",
      "vc = W(k,.) and vw = W\n",
      "\n",
      "(cid:48)\n",
      "(.,k)\n",
      "\n",
      "(1)\n",
      "\n",
      "Overall, for any word wi with given context word c as input,\n",
      "\n",
      "(2)\n",
      "The parameters θ = {vw, vc}w,c ∈ Vocab are learned by deﬁning the objective function as the log-likelihood and ﬁnding its\n",
      "gradient as\n",
      "\n",
      "where, ui = vT\n",
      "wi\n",
      "\n",
      "i=1 eui\n",
      "\n",
      "= yi =\n",
      "\n",
      ".vc\n",
      "\n",
      "p\n",
      "\n",
      "c\n",
      "\n",
      "(cid:16) wi\n",
      "\n",
      "(cid:17)\n",
      "\n",
      "eui(cid:80)V\n",
      "(cid:88)\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "(cid:16) w\n",
      "(cid:17)(cid:17)\n",
      "(cid:17)(cid:17)\n",
      "(cid:16) w\n",
      "\n",
      "p\n",
      "\n",
      "c\n",
      "\n",
      "l(θ) =\n",
      "\n",
      "log\n",
      "\n",
      "w∈Vocab\n",
      "\n",
      "∂l(θ)\n",
      "∂vw\n",
      "\n",
      "= vc\n",
      "\n",
      "1 − p\n",
      "\n",
      "c\n",
      "\n",
      "(cid:16)\n",
      "\n",
      "In the general CBOW model, all the one-hot vectors of context words are taken as input simultaneously, i.e,\n",
      "\n",
      "h = WT(x1 + x2 + ... + xc)\n",
      "\n",
      "One limitation of individual word embeddings is their inability to represent phrases [3], where the combination of two or\n",
      "more words – e.g., idioms like “hot potato” or named entities such as “Boston Globe’ – does not represent the combination\n",
      "of meanings of individual words. One solution to this problem, as explored by Mikolov et al. [3], is to identify such phrases\n",
      "based on word co-occurrence and train embeddings for them separately. Later methods have explored directly learning n-gram\n",
      "embeddings from unlabeled data [23].\n",
      "\n",
      "Another limitation comes from learning embeddings based only on a small window of surrounding words, sometimes\n",
      "words such as good and bad share almost the same embedding [24], which is problematic if used in tasks such as sentiment\n",
      "analysis [25]. At times these embeddings cluster semantically-similar words which have opposing sentiment polarities. This\n",
      "leads the downstream model used for the sentiment analysis task to be unable to identify this contrasting polarities leading to\n",
      "poor performance. Tang et al. [26] addressed this problem by proposing sentiment speciﬁc word embedding (SSWE). Authors\n",
      "incorporated the supervised sentiment polarity of text in their loss functions while learning the embeddings.\n",
      "\n",
      "A general caveat for word embeddings is that they are highly dependent on the applications in which it is used. Labutov\n",
      "and Lipson [27] proposed task speciﬁc embeddings which retrain the word embeddings to align them in the current task space.\n",
      "This is very important as training embeddings from scratch requires large amount of time and resource. Mikolov et al. [8] tried\n",
      "to address this issue by proposing negative sampling which does frequency-based sampling of negative terms while training\n",
      "the word2vec model.\n",
      "\n",
      "Traditional word embedding algorithms assign a distinct vector to each word. This makes them unable to account for\n",
      "polysemy. In a recent work, Upadhyay et al. [28] provided an innovative way to address this deﬁcit. The authors leveraged\n",
      "multilingual parallel data to learn multi-sense word embeddings. For example, the English word bank, when translated to\n",
      "French provides two different words: banc and banque representing ﬁnancial and geographical meanings, respectively. Such\n",
      "multilingual distributional information helped them in accounting for polysemy.\n",
      "\n",
      "Table I provides a directory of existing frameworks that are frequently used for creating embeddings which are further\n",
      "\n",
      "incorporated into deep learning models.\n",
      "\n",
      "(3)\n",
      "\n",
      "(4)\n",
      "\n",
      "(5)\n",
      "\n",
      "\f",
      "5\n",
      "\n",
      "Language\n",
      "\n",
      "Java\n",
      "Java\n",
      "Python\n",
      "Python\n",
      "Python\n",
      "Python\n",
      "Python\n",
      "\n",
      "URL\n",
      "\n",
      "https://github.com/fozziethebeat/S-Space\n",
      "\n",
      "https://github.com/semanticvectors/\n",
      "https://radimrehurek.com/gensim/\n",
      "\n",
      "https://github.com/jimmycallin/pydsm\n",
      "\n",
      "http://clic.cimec.unitn.it/composes/toolkit/\n",
      "\n",
      "https://fasttext.cc/\n",
      "\n",
      "https://tfhub.dev/google/elmo/2\n",
      "\n",
      "Framework\n",
      "\n",
      "S-Space\n",
      "\n",
      "Semanticvectors\n",
      "\n",
      "Gensim\n",
      "Pydsm\n",
      "Dissect\n",
      "FastText\n",
      "\n",
      "Elmo\n",
      "\n",
      "TABLE I: Frameworks providing word embedding tools and methods.\n",
      "\n",
      "C. Character Embeddings\n",
      "\n",
      "Word embeddings are able to capture syntactic and semantic information, yet for tasks such as POS-tagging and NER, intra-\n",
      "word morphological and shape information can also be very useful. Generally speaking, building natural language understanding\n",
      "systems at the character level has attracted certain research attention [29, 30, 31, 32]. Better results on morphologically rich\n",
      "languages are reported in certain NLP tasks. Santos and Guimaraes [31] applied character-level representations, along with\n",
      "word embeddings for NER, achieving state-of-the-art results in Portuguese and Spanish corpora. Kim et al. [29] showed positive\n",
      "results on building a neural language model using only character embeddings. Ma et al. [33] exploited several embeddings,\n",
      "including character trigrams, to incorporate prototypical and hierarchical information for learning pre-trained label embeddings\n",
      "in the context of NER.\n",
      "\n",
      "A common phenomenon for languages with large vocabularies is the unknown word issue, also known as out-of-vocabulary\n",
      "(OOV) words. Character embeddings naturally deal with it since each word is considered as no more than a composition\n",
      "of individual letters. In languages where text is not composed of separated words but individual characters and the semantic\n",
      "meaning of words map to its compositional characters (such as Chinese), building systems at the character level is a natural\n",
      "choice to avoid word segmentation [34]. Thus, works employing deep learning applications on such languages tend to prefer\n",
      "character embeddings over word vectors [35]. For example, Peng et al. [36] proved that radical-level processing could greatly\n",
      "improve sentiment classiﬁcation performance. In particular, the authors proposed two types of Chinese radical-based hierarchical\n",
      "embeddings, which incorporate not only semantics at radical and character level, but also sentiment information. Bojanowski\n",
      "et al. [37] also tried to improve the representation of words by using character-level information in morphologically-rich\n",
      "languages. They approached the skip-gram method by representing words as bag-of-character n-grams. Their work thus had\n",
      "the effectiveness of the skip-gram model along with addressing some persistent issues of word embeddings. The method was\n",
      "also fast, which allowed training models on large corpora quickly. Popularly known as FastText, such a method stands out over\n",
      "previous methods in terms of speed, scalability, and effectiveness.\n",
      "\n",
      "Apart from character embeddings, different approaches have been proposed for OOV handling. Herbelot and Baroni [38]\n",
      "provided on-the-ﬂy OOV handling by initializing the unknown words as the sum of the context words and reﬁning these\n",
      "words with a high learning rate. However, their approach is yet to be tested on typical NLP tasks. Pinter et al. [39] provided\n",
      "an interesting approach of training a character-based model to recreate pre-trained embeddings. This allowed them to learn a\n",
      "compositional mapping form character to word embedding, thus tackling the OOV problem.\n",
      "\n",
      "Despite the ever growing popularity of distributional vectors, recent discussions on their relevance in the long run have\n",
      "cropped up. For example, Lucy and Gauthier [40] has recently tried to evaluate how well the word vectors capture the\n",
      "necessary facets of conceptual meaning. The authors have discovered severe limitations in perceptual understanding of the\n",
      "concepts behind the words, which cannot be inferred from distributional semantics alone. A possible direction for mitigating\n",
      "these deﬁciencies will be grounded learning, which has been gaining popularity in this research domain.\n",
      "\n",
      "D. Contextualized Word Embeddings\n",
      "\n",
      "The quality of word representations is generally gauged by its ability to encode syntactical information and handle polysemic\n",
      "behavior (or word senses). These properties result in improved semantic word representations. Recent approaches in this area\n",
      "encode such information into its embeddings by leveraging the context. These methods provide deeper networks that calculate\n",
      "word representations as a function of its context.\n",
      "\n",
      "Traditional word embedding methods such as Word2Vec and Glove consider all the sentences where a word is present in\n",
      "order to create a global vector representation of that word. However, a word can have completely different senses or meanings\n",
      "in the contexts. For example, lets consider these two sentences - 1) “The bank will not be accepting cash on Saturdays” 2)\n",
      "“The river overﬂowed the bank.”. The word senses of bank are different in these two sentences depending on its context.\n",
      "Reasonably, one might want two different vector representations of the word bank based on its two different word senses.\n",
      "The new class of models adopt this reasoning by diverging from the concept of global word representations and proposing\n",
      "contextual word embeddings instead.\n",
      "\n",
      "Embedding from Language Model (ELMo) [41] is one such method that provides deep contextual embeddings. ELMo\n",
      "produces word embeddings for each context where the word is used, thus allowing different representations for varying senses\n",
      "\n",
      "\f",
      "6\n",
      "\n",
      "(6)\n",
      "\n",
      "(7)\n",
      "\n",
      "(8)\n",
      "\n",
      "k\n",
      "\n",
      "of the same word. Speciﬁcally, for N different sentences where a word w is present, ELMo generates N different representations\n",
      "of w i.e., w1, w2, ˙,wN .\n",
      "\n",
      "The mechanism of ELMo is based on the representation obtained from a bidirectional language model. A bidirectional\n",
      "language model (biLM) constitutes of two language models (LM) 1) forward LM and 2) backward LM. A forward LM takes\n",
      "input representation xLM\n",
      "for each of the kth token and passes it through L layers of forward LSTM to get representations\n",
      "−→\n",
      "(cid:81)N\n",
      "k,j where j = 1, . . . , L. Each of these representations, being hidden representations of recurrent neural networks, is context\n",
      "h LM\n",
      "dependent. A forward LM can be seen as a method to model the joint probability of a sequence of tokens: p (t1, t2, . . . , tN ) =\n",
      "k=1 p (tk|t1, t2, . . . , tk−1). At a timestep k− 1 the forward LM predicts the next token tk given the previous observed tokens\n",
      "tokens: p (t1, t2, . . . , tN ) =(cid:81)N\n",
      "t1, t2, ..., tk. This is typically achieved by placing a softmax layer on top of the ﬁnal LSTM in a forward LM. On the other\n",
      "hand, a backward LM models the same joint probability of the sequence by predicting the previous token given the future\n",
      "k=1 p (tk|tk+1, tk+2, . . . , tN ). In other words, a backward LM is similar to forward LM which\n",
      "processes a sequence with the order being reversed. The training of the biLM model involves modeling the log-likelihood of\n",
      "both the sentence orientations. Finally, hidden representations from both LMs are concetenated to compose the ﬁnal token\n",
      "vectors [42].\n",
      "\n",
      "For each tokem, ELMo extracts the intermediate layer representations from the biLM and performs a linear combination\n",
      "\n",
      "based on the given downstream task. A L-layer biLM contains 2L + 1 set of representations as shown below -\n",
      "\n",
      "(cid:111)\n",
      "\n",
      "←−\n",
      "k,j |j = 1, . . . , L\n",
      "h LM\n",
      "\n",
      "Rk =\n",
      "\n",
      ",\n",
      "\n",
      "k\n",
      "\n",
      "xLM\n",
      "\n",
      "−→\n",
      "h LM\n",
      "k,j ,\n",
      "\n",
      "(cid:110)\n",
      "=(cid:8)hLM\n",
      "k,j |j = 0, . . . , L(cid:9)\n",
      "(cid:104)−→\n",
      "\n",
      "←−\n",
      "h LM\n",
      "k,j\n",
      "\n",
      "(cid:105) ∀j = 1, . . . , L.\n",
      "L(cid:88)\n",
      "\n",
      "k = E(cid:0)Rk; Θtask(cid:1) = γtask\n",
      "\n",
      "ELMotask\n",
      "\n",
      "stask\n",
      "j hLM\n",
      "k,j\n",
      "\n",
      "k,0 is the token representation at the lowest level. One can use either character or word embeddings to initialize\n",
      "\n",
      "Here, hLM\n",
      "k,0 . For other values of j,\n",
      "hLM\n",
      "\n",
      "h LM\n",
      "k,j ,\n",
      "ELMo ﬂattens all layers in R in a single vector such that -\n",
      "\n",
      "hLM\n",
      "k,j =\n",
      "\n",
      "j=0\n",
      "\n",
      "j\n",
      "\n",
      "In Eq. 8, stask\n",
      "is the softmax-normalized weight vector to combine the representations of different layers. γtask is a hyper-\n",
      "parameter which helps in optimization and task speciﬁc scaling of the ELMo representation. ELMo produces varied word\n",
      "representations for the same word in different sentences. According to Peters et al. [41], it is always beneﬁcial to combine\n",
      "ELMo word representations with standard global word representations like Glove and Word2Vec.\n",
      "\n",
      "Off-late, there has been a surge of interest in pre-trained language models for myriad of natural language tasks [43].\n",
      "Language modeling is chosen as the pre-training objective as it is widely considered to incorporate multiple traits of natual\n",
      "language understanding and generation. A good language model requires learning complex characteristics of language involving\n",
      "syntactical properties and also semantical coherence. Thus, it is believed that unsupervised training on such objectives would\n",
      "infuse better linguistic knowledge into the networks than random initialization. The generative pre-training and discriminative\n",
      "ﬁne-tuning procedure is also desirable as the pre-training is unsupervised and does not require any manual labeling.\n",
      "\n",
      "Radford et al. [44] proposed similar pre-trained model, the OpenAI-GPT, by adapting the Transformer (see section IV-E).\n",
      "Recently, Devlin et al. [45] proposed BERT which utilizes a transformer network to pre-train a language model for extracting\n",
      "contextual word embeddings. Unlike ELMo and OpenAI-GPT, BERT uses different pre-training tasks for language modeling.\n",
      "In one of the tasks, BERT randomly masks a percentage of words in the sentences and only predicts those masked words. In\n",
      "the other task, BERT predicts the next sentence given a sentence. This task in particular tries to model the relationship among\n",
      "two sentences which is supposedly not captured by traditional bidirectional language models. Consequently, this particular\n",
      "pre-training scheme helps BERT to outperform state-of-the-art techniques by a large margin on key NLP tasks such as QA,\n",
      "Natural Language Inference (NLI) where understanding relation among two sentences is very important. We discuss the impact\n",
      "of these proposed models and the performance achieved by them in section VIII-I.\n",
      "\n",
      "The described approaches for contextual word embeddings promises better quality representations for words. The pre-trained\n",
      "deep language models also provide a headstart for downstream tasks in the form of transfer learning. This approach has been\n",
      "extremely popular in computer vision tasks. Whether there would be similar trends in the NLP community, where researchers\n",
      "and practitioners would prefer such models over traditional variants remains to be seen in the future.\n",
      "\n",
      "III. CONVOLUTIONAL NEURAL NETWORKS\n",
      "\n",
      "Following the popularization of word embeddings and its ability to represent words in a distributed space, the need arose\n",
      "for an effective feature function that extracts higher-level features from constituting words or n-grams. These abstract features\n",
      "would then be used for numerous NLP tasks such as sentiment analysis, summarization, machine translation, and question\n",
      "answering (QA). CNNs turned out to be the natural choice given their effectiveness in computer vision tasks [46, 47, 48].\n",
      "\n",
      "\f",
      "7\n",
      "\n",
      "Fig. 5: CNN framework used to perform word wise class prediction (Figure source: Collobert and Weston [19])\n",
      "\n",
      "The use of CNNs for sentence modeling traces back to Collobert and Weston [19]. This work used multi-task learning to\n",
      "output multiple predictions for NLP tasks such as POS tags, chunks, named-entity tags, semantic roles, semantically-similar\n",
      "words and a language model. A look-up table was used to transform each word into a vector of user-deﬁned dimensions.\n",
      "Thus, an input sequence {s1, s2, ...sn} of n words was transformed into a series of vectors {ws1, ws2 , ...wsn} by applying\n",
      "the look-up table to each of its words (Fig. 5).\n",
      "\n",
      "This can be thought of as a primitive word embedding method whose weights were learned in the training of the network.\n",
      "In [5], Collobert extended his work to propose a general CNN-based framework to solve a plethora of NLP tasks. Both these\n",
      "works triggered a huge popularization of CNNs amongst NLP researchers. Given that CNNs had already shown their mettle\n",
      "for computer vision tasks, it was easier for people to believe in their performance.\n",
      "\n",
      "CNNs have the ability to extract salient n-gram features from the input sentence to create an informative latent semantic\n",
      "representation of the sentence for downstream tasks. This application was pioneered by Collobert et al. [5], Kalchbrenner et al.\n",
      "[49], Kim [50], which led to a huge proliferation of CNN-based networks in the succeeding literature. Below, we describe the\n",
      "working of a simple CNN-based sentence modeling network:\n",
      "\n",
      "A. Basic CNN\n",
      "\n",
      "1) Sentence Modeling: For each sentence, let wi ∈ Rd represent the word embedding for the ith word in the sentence,\n",
      "where d is the dimension of the word embedding. Given that a sentence has n words, the sentence can now be represented as\n",
      "an embedding matrix W ∈ Rn×d. Fig. 6 depicts such a sentence as an input to the CNN framework.\n",
      "Let wi:i+j refer to the concatenation of vectors wi, wi+1, ...wj. Convolution is performed on this input embedding layer.\n",
      "It involves a ﬁlter k ∈ Rhd which is applied to a window of h words to produce a new feature. For example, a feature ci is\n",
      "generated using the window of words wi:i+h−1 by\n",
      "\n",
      "(9)\n",
      "Here, b ∈ R is the bias term and f is a non-linear activation function, for example the hyperbolic tangent. The ﬁlter k is\n",
      "applied to all possible windows using the same weights to create the feature map.\n",
      "\n",
      "ci = f (wi:i+h−1.kT + b)\n",
      "\n",
      "c = [c1, c2, ..., cn−h+1]\n",
      "\n",
      "(10)\n",
      "\n",
      "In a CNN, a number of convolutional ﬁlters, also called kernels (typically hundreds), of different widths slide over the\n",
      "entire word embedding matrix. Each kernel extracts a speciﬁc pattern of n-gram. A convolution layer is usually followed by\n",
      "a max-pooling strategy, ˆc = max{c}, which subsamples the input typically by applying a max operation on each ﬁlter. This\n",
      "strategy has two primary reasons.\n",
      "\n",
      "wowN−1Input SentenceLookup tableFeature 1Feature kConvolution layerMax-pool over timeFully Connected LayerSoftmax Classiﬁcationw1\f",
      "8\n",
      "\n",
      "Fig. 6: CNN modeling on text (Figure source: Zhang and Wallace [51])\n",
      "\n",
      "Firstly, max pooling provides a ﬁxed-length output which is generally required for classiﬁcation. Thus, regardless the size of\n",
      "the ﬁlters, max pooling always maps the input to a ﬁxed dimension of outputs. Secondly, it reduces the output’s dimensionality\n",
      "while keeping the most salient n-gram features across the whole sentence. This is done in a translation invariant manner where\n",
      "each ﬁlter is now able to extract a particular feature (e.g., negations) from anywhere in the sentence and add it to the ﬁnal\n",
      "sentence representation.\n",
      "\n",
      "The word embeddings can be initialized randomly or pre-trained on a large unlabeled corpora (as in Section II). The\n",
      "latter option is sometimes found beneﬁcial to performance, especially when the amount of labeled data is limited [50]. This\n",
      "combination of convolution layer followed by max pooling is often stacked to create deep CNN networks. These sequential\n",
      "convolutions help in improved mining of the sentence to grasp a truly abstract representations comprising rich semantic\n",
      "information. The kernels through deeper convolutions cover a larger part of the sentence until ﬁnally covering it fully and\n",
      "creating a global summarization of the sentence features.\n",
      "\n",
      "2) Window Approach: The above-mentioned architecture allows for modeling of complete sentences into sentence repre-\n",
      "sentations. However, many NLP tasks, such as NER, POS tagging, and SRL, require word-based predictions. To adapt CNNs\n",
      "for such tasks, a window approach is used, which assumes that the tag of a word primarily depends on its neighboring words.\n",
      "For each word, thus, a ﬁxed-size window surrounding itself is assumed and the sub-sentence ranging within the window is\n",
      "considered. A standalone CNN is applied to this sub-sentence as explained earlier and predictions are attributed to the word\n",
      "in the center of the window. Following this approach, Poria et al. [52] employed a multi-level deep CNN to tag each word in\n",
      "a sentence as a possible aspect or non-aspect. Coupled with a set of linguistic patterns, their ensemble classiﬁer managed to\n",
      "perform well in aspect detection.\n",
      "\n",
      "The ultimate goal of word-level classiﬁcation is generally to assign a sequence of labels to the entire sentence. In such cases,\n",
      "structured prediction techniques such as conditional random ﬁeld (CRF) are sometimes employed to better capture dependencies\n",
      "between adjacent class labels and ﬁnally generate cohesive label sequence giving maximum score to the whole sentence [53].\n",
      "To get a larger contextual range, the classic window approach is often coupled with a time-delay neural network (TDNN) [54].\n",
      "Here, convolutions are performed across all windows throughout the sequence. These convolutions are generally constrained\n",
      "by deﬁning a kernel having a certain width. Thus, while the classic window approach only considers the words in the window\n",
      "around the word to be labeled, TDNN considers all windows of words in the sentence at the same time. At times, TDNN\n",
      "layers are also stacked like CNN architectures to extract local features in lower layers and global features in higher layers [5].\n",
      "\n",
      "B. Applications\n",
      "\n",
      "In this section, we present some of the crucial works that employed CNNs on NLP tasks to set state-of-the-art benchmarks\n",
      "\n",
      "in their respective times.\n",
      "\n",
      "Kim [50] explored using the above architecture for a variety of sentence classiﬁcation tasks, including sentiment, subjectivity\n",
      "and question type classiﬁcation, showing competitive results. This work was quickly adapted by researchers given its simple\n",
      "yet effective network. After training for a speciﬁc task, the randomly initialized convolutional kernels became speciﬁc n-gram\n",
      "feature detectors that were useful for that target task (Fig. 7). This simple network, however, had many shortcomings with the\n",
      "CNN’s inability to model long distance dependencies standing as the main issue.\n",
      "\n",
      "\f",
      "9\n",
      "\n",
      "(a) Figure A\n",
      "\n",
      "(b) Figure B\n",
      "\n",
      "Fig. 7: Top 7-grams by four learned 7-gram kernels; each kernel\n",
      "Source: Kalchbrenner et al. [49])\n",
      "\n",
      "is sensitive to a speciﬁc kind of 7-gram (Figure\n",
      "\n",
      "This issue was partly handled by Kalchbrenner et al. [49], who published a prominent paper where they proposed a dynamic\n",
      "convolutional neural network (DCNN) for semantic modeling of sentences. They proposed dynamic k-max pooling strategy\n",
      "which, given a sequence p selects the k most active features. The selection preserved the order of the features but was insensitive\n",
      "to their speciﬁc positions (Fig. 8). Built on the concept of TDNN, they added this dynamic k-max pooling strategy to create\n",
      "a sentence model. This combination allowed ﬁlters with small width to span across a long range within the input sentence,\n",
      "thus accumulating crucial information across the sentence. In the induced subgraph (Fig. 8), higher order features had highly\n",
      "variable ranges that could be either short and focused or global and long as the input sentence. They applied their model on\n",
      "multiple tasks, including sentiment prediction and question type classiﬁcation, achieving signiﬁcant results. Overall, this work\n",
      "commented on the range of individual kernels while trying to model contextual semantics and proposed a way to extend their\n",
      "reach.\n",
      "\n",
      "Tasks involving sentiment analysis also require effective extraction of aspects along with their sentiment polarities [55].\n",
      "Ruder et al. [56] applied a CNN where in the input they concatenated an aspect vector with the word embeddings to get\n",
      "competitive results. CNN modeling approach varies amongst different length of texts. Such differences were seen in many\n",
      "works like Johnson and Zhang [23], where performance on longer text worked well as opposed to shorter texts. Wang et al.\n",
      "[57] proposed the usage of CNN for modeling representations of short texts, which suffer from the lack of available context\n",
      "and, thus, require extra efforts to create meaningful representations. The authors proposed semantic clustering which introduced\n",
      "multi-scale semantic units to be used as external knowledge for the short texts. CNN was used to combine these units and\n",
      "form the overall representation. In fact, this requirement of high context information can be thought of as a caveat for CNN-\n",
      "based models. NLP tasks involving microtexts using CNN-based methods often require the need of additional information and\n",
      "external knowledge to perform as per expectations. This fact was also observed in [58], where authors performed sarcasm\n",
      "detection in Twitter texts using a CNN network. Auxiliary support, in the form of pre-trained networks trained on emotion,\n",
      "sentiment and personality datasets was used to achieve state-of-the-art performance.\n",
      "\n",
      "CNNs have also been extensively used in other tasks. For example, Denil et al. [59] applied DCNN to map meanings of\n",
      "words that constitute a sentence to that of documents for summarization. The DCNN learned convolution ﬁlters at both the\n",
      "sentence and document level, hierarchically learning to capture and compose low-level lexical features into high-level semantic\n",
      "concepts. The focal point of this work was the introduction of a novel visualization technique of the learned representations,\n",
      "which provided insights not only in the learning process but also for automatic summarization of texts.\n",
      "\n",
      "CNN models are also suitable for certain NLP tasks that require semantic matching beyond classiﬁcation [60]. A similar\n",
      "model to the above CNN architecture (Fig. 6) was explored in [61] for information retrieval. The CNN was used for projecting\n",
      "queries and documents to a ﬁxed-dimension semantic space, where cosine similarity between the query and documents was\n",
      "used for ranking documents regarding a speciﬁc query. The model attempted to extract rich contextual structures in a query\n",
      "or a document by considering a temporal context window in a word sequence. This captured the contextual features at the\n",
      "word n-gram level. The salient word n-grams is then discovered by the convolution and max-pooling layers which are then\n",
      "aggregated to form the overall sentence vector.\n",
      "\n",
      "In the domain of QA, Yih et al. [62] proposed to measure the semantic similarity between a question and entries in a\n",
      "knowledge base (KB) to determine what supporting fact in the KB to look for when answering a question. To create semantic\n",
      "representations, a CNN similar to the one in Fig. 6 was used. Unlike the classiﬁcation setting, the supervision signal came\n",
      "from positive or negative text pairs (e.g., query-document), instead of class labels. Subsequently, Dong et al. [63] introduced\n",
      "a multi-column CNN (MCCNN) to analyze and understand questions from multiple aspects and create their representations.\n",
      "MCCNN used multiple column networks to extract information from aspects comprising answer types and context from the\n",
      "input questions. By representing entities and relations in the KB with low-dimensional vectors, they used question-answer\n",
      "pairs to train the CNN model so as to rank candidate answers. Severyn and Moschitti [64] also used CNN network to model\n",
      "optimal representations of question and answer sentences. They proposed additional features in the embeddings in the form\n",
      "of relational information given by matching words between the question and answer pair. These parameters were tuned by the\n",
      "\n",
      "\f",
      "10\n",
      "\n",
      "Fig. 8: DCNN subgraph. With dynamic pooling, a ﬁlter with small width at the higher layers can relate phrases far apart in\n",
      "the input sentence (Figure Source: Kalchbrenner et al. [49])\n",
      "\n",
      "network. This simple network was able to produce comparable results to state-of-the-art methods.\n",
      "\n",
      "CNNs are wired in a way to capture the most important information in a sentence. Traditional max-pooling strategies\n",
      "perform this in a translation invariant form. However, this often misses valuable information present in multiple facts within\n",
      "the sentence. To overcome this loss of information for multiple-event modeling, Chen et al. [65] proposed a modiﬁed pooling\n",
      "strategy: dynamic multi-pooling CNN (DMCNN). This strategy used a novel dynamic multi-pooling layer that, as the name\n",
      "suggests, incorporates event triggers and arguments to reserve more crucial information from the pooling layer.\n",
      "\n",
      "CNNs inherently provide certain required features like local connectivity, weight sharing, and pooling. This puts forward\n",
      "some degree of invariance which is highly desired in many tasks. Speech recognition also requires such invariance and, thus,\n",
      "Abdel-Hamid et al. [66] used a hybrid CNN-HMM model which provided invariance to frequency shifts along the frequency\n",
      "axis. This variability is often found in speech signals due to speaker differences. They also performed limited weight sharing\n",
      "which led to a smaller number of pooling parameters, resulting in lower computational complexity. Palaz et al. [67] performed\n",
      "extensive analysis of CNN-based speech recognition systems when given raw speech as input. They showed the ability of\n",
      "CNNs to directly model the relationship between raw input and phones, creating a robust automatic speech recognition system.\n",
      "Tasks like machine translation require perseverance of sequential information and long-term dependency. Thus, structurally\n",
      "they are not well suited for CNN networks, which lack these features. Nevertheless, Tu et al. [68] addressed this task by\n",
      "considering both the semantic similarity of the translation pair and their respective contexts. Although this method did not\n",
      "address the sequence perseverance problem, it allowed them to get competitive results amongst other benchmarks.\n",
      "\n",
      "Overall, CNNs are extremely effective in mining semantic clues in contextual windows. However, they are very data heavy\n",
      "models. They include a large number of trainable parameters which require huge training data. This poses a problem when\n",
      "scarcity of data arises. Another persistent issue with CNNs is their inability to model long-distance contextual information and\n",
      "preserving sequential order in their representations [49, 68]. Other networks like recursive models (explained below) reveal\n",
      "themselves as better suited for such learning.\n",
      "\n",
      "IV. RECURRENT NEURAL NETWORKS\n",
      "\n",
      "RNNs [69] use the idea of processing sequential information. The term “recurrent” applies as they perform the same task\n",
      "over each instance of the sequence such that the output is dependent on the previous computations and results. Generally, a\n",
      "ﬁxed-size vector is produced to represent a sequence by feeding tokens one by one to a recurrent unit. In a way, RNNs have\n",
      "“memory” over previous computations and use this information in current processing. This template is naturally suited for\n",
      "many NLP tasks such as language modeling [2, 70, 71], machine translation [72, 73, 74], speech recognition [75, 76, 77, 78],\n",
      "image captioning [79]. This made RNNs increasingly popular for NLP applications in recent years.\n",
      "\n",
      "A. Need for Recurrent Networks\n",
      "\n",
      "In this section, we analyze the fundamental properties that favored the popularization of RNNs in a multitude of NLP tasks.\n",
      "Given that an RNN performs sequential processing by modeling units in sequence, it has the ability to capture the inherent\n",
      "sequential nature present in language, where units are characters, words or even sentences. Words in a language develop their\n",
      "semantical meaning based on the previous words in the sentence. A simple example stating this would be the difference in\n",
      "meaning between “dog” and “hot dog”. RNNs are tailor-made for modeling such context dependencies in language and similar\n",
      "sequence modeling tasks, which resulted to be a strong motivation for researchers to use RNNs over CNNs in these areas.\n",
      "\n",
      "xnx2x1xnx2x1\f",
      "11\n",
      "\n",
      "Fig. 9: Simple RNN network (Figure Source: LeCun et al. [90])\n",
      "\n",
      "Another factor aiding RNN’s suitability for sequence modeling tasks lies in its ability to model variable length of text,\n",
      "including very long sentences, paragraphs and even documents [80]. Unlike CNNs, RNNs have ﬂexible computational steps\n",
      "that provide better modeling capability and create the possibility to capture unbounded context. This ability to handle input of\n",
      "arbitrary length became one of the selling points of major works using RNNs [81].\n",
      "\n",
      "Many NLP tasks require semantic modeling over the whole sentence. This involves creating a gist of the sentence in\n",
      "a ﬁxed dimensional hyperspace. RNN’s ability to summarize sentences led to their increased usage for tasks like machine\n",
      "translation [82] where the whole sentence is summarized to a ﬁxed vector and then mapped back to the variable-length target\n",
      "sequence.\n",
      "\n",
      "RNN also provides the network support to perform time distributed joint processing. Most of the sequence labeling\n",
      "tasks like POS tagging [32] come under this domain. More speciﬁc use cases include applications such as multi-label text\n",
      "categorization [83], multimodal sentiment analysis [84, 85, 86], and subjectivity detection [87].\n",
      "\n",
      "The above points enlist some of the focal reasons that motivated researchers to opt for RNNs. However, it would be gravely\n",
      "wrong to make conclusions on the superiority of RNNs over other deep networks. Recently, several works provided contrasting\n",
      "evidence on the superiority of CNNs over RNNs. Even in RNN-suited tasks like language modeling, CNNs achieved competitive\n",
      "performance over RNNs [88]. Both CNNs and RNNs have different objectives when modeling a sentence. While RNNs try\n",
      "to create a composition of an arbitrarily long sentence along with unbounded context, CNNs try to extract the most important\n",
      "n-grams. Although they prove an effective way to capture n-gram features, which is approximately sufﬁcient in certain sentence\n",
      "classiﬁcation tasks, their sensitivity to word order is restricted locally and long-term dependencies are typically ignored.\n",
      "\n",
      "Yin et al. [89] provided interesting insights on the comparative performance between RNNs and CNNs. After testing on\n",
      "multiple NLP tasks that included sentiment classiﬁcation, QA, and POS tagging, they concluded that there is no clear winner:\n",
      "the performance of each network depends on the global semantics required by the task itself.\n",
      "\n",
      "Below, we discuss some of the RNN models extensively used in the literature.\n",
      "\n",
      "B. RNN models\n",
      "\n",
      "1) Simple RNN: In the context of NLP, RNNs are primarily based on Elman network [69] and they are originally three-\n",
      "layer networks. Fig. 9 illustrates a more general RNN which is unfolded across time to accommodate a whole sequence. In\n",
      "the ﬁgure, xt is taken as the input to the network at time step t and st represents the hidden state at the same time step.\n",
      "Calculation of st is based as per the equation:\n",
      "\n",
      "st = f (U xt + W st−1)\n",
      "\n",
      "(11)\n",
      "\n",
      "Thus, st is calculated based on the current input and the previous time step’s hidden state. The function f is taken to be a\n",
      "non-linear transformation such as tanh, ReLU and U, V, W account for weights that are shared across time. In the context of\n",
      "NLP, xt typically comprises of one-hot encodings or embeddings. At times, they can also be abstract representations of textual\n",
      "content. ot illustrates the output of the network which is also often subjected to non-linearity, especially when the network\n",
      "contains further layers downstream.\n",
      "\n",
      "The hidden state of the RNN is typically considered to be its most crucial element. As stated before, it can be considered\n",
      "as the network’s memory element that accumulates information from other time steps. In practice, however, these simple RNN\n",
      "networks suffer from the infamous vanishing gradient problem, which makes it really hard to learn and tune the parameters\n",
      "of the earlier layers in the network.\n",
      "\n",
      "This limitation was overcome by various networks such as long short-term memory (LSTM), gated recurrent units (GRUs),\n",
      "\n",
      "and residual networks (ResNets), where the ﬁrst two are the most used RNN variants in NLP applications.\n",
      "\n",
      "Wotot−1ot+1UnfoldVVVVWWUUUWUxtxt−1xt+1xohhtht−1ht+1\f",
      "12\n",
      "\n",
      "Fig. 10: Illustration of an LSTM and GRU gate (Figure Source: Chung et al. [81])\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "(cid:20) ht−1\n",
      "\n",
      "xt\n",
      "\n",
      "x =\n",
      "\n",
      "2) Long Short-Term Memory: LSTM [91, 92] (Fig. 10) has additional “forget” gates over the simple RNN. Its unique\n",
      "\n",
      "mechanism enables it to overcome both the vanishing and exploding gradient problem.\n",
      "\n",
      "Unlike the vanilla RNN, LSTM allows the error to back-propagate through unlimited number of time steps. Consisting of\n",
      "three gates: input, forget and output gates, it calculates the hidden state by taking a combination of these three gates as per\n",
      "the equations below:\n",
      "\n",
      "(12)\n",
      "\n",
      "ft = σ(Wf .x + bf )\n",
      "it = σ(Wi.x + bi)\n",
      "ot = σ(Wo.x + bo)\n",
      "\n",
      "(13)\n",
      "(14)\n",
      "(15)\n",
      "(16)\n",
      "(17)\n",
      "3) Gated Recurrent Units: Another gated RNN variant called GRU [82] (Fig. 10) of lesser complexity was invented with\n",
      "empirically similar performances to LSTM in most tasks. GRU comprises of two gates, reset gate and update gate, and handles\n",
      "the ﬂow of information like an LSTM sans a memory unit. Thus, it exposes the whole hidden content without any control.\n",
      "Being less complex, GRU can be a more efﬁcient RNN than LSTM. The working of GRU is as follows:\n",
      "\n",
      "ct = ft (cid:12) ct−1 + it (cid:12) tanh(Wc.X + bc)\n",
      "\n",
      "ht = ot (cid:12) tanh(ct)\n",
      "\n",
      "st = tanh(Uz.xt + Ws.(ht−1 (cid:12) r))\n",
      "\n",
      "z = σ(Uz.xt + Wz.ht−1)\n",
      "r = σ(Ur.xt + Wr.ht−1)\n",
      "\n",
      "(18)\n",
      "(19)\n",
      "(20)\n",
      "(21)\n",
      "Researchers often face the dilemma of choosing the appropriate gated RNN. This also extends to developers working in\n",
      "NLP. Throughout the history, most of the choices over the RNN variant tended to be heuristic. Chung et al. [81] did a critical\n",
      "comparative evaluation of the three RNN variants mentioned above, although not on NLP tasks. They evaluated their work on\n",
      "tasks relating to polyphonic music modeling and speech signal modeling. Their evaluation clearly demonstrated the superiority\n",
      "of the gated units (LSTM and GRU) over the traditional simple RNN (in their case, using tanh activation) (Fig. 11). However,\n",
      "they could not make any concrete conclusion about which of the two gating units was better. This fact has been noted in other\n",
      "works too and, thus, people often leverage on other factors like computing power while choosing between the two.\n",
      "\n",
      "ht = (1 − z) (cid:12) st + z (cid:12) ht−1\n",
      "\n",
      "C. Applications\n",
      "\n",
      "1) RNN for word-level classiﬁcation: RNNs have had a huge presence in the ﬁeld of word-level classiﬁcation. Many of\n",
      "their applications stand as state of the art in their respective tasks. Lample et al. [93] proposed to use bidirectional LSTM\n",
      "\n",
      "tanhxt˜CoiCht−1htxtszrht(1) Long Short-Term Memory(2) Gated Recurrent Unit\f",
      "13\n",
      "\n",
      "Fig. 11: Learning curves for training and validation sets of different types of units with respect to (top) the number of iterations\n",
      "and (bottom) the wall clock time. y-axis corresponds to the negative log likelihood of the model shown in log-scale (Figure\n",
      "source: Chung et al. [81])\n",
      "\n",
      "for NER. The network captured arbitrarily long context information around the target word (curbing the limitation of a ﬁxed\n",
      "window size) resulting in two ﬁxed-size vector, on top of which another fully-connected layer was built. They used a CRF\n",
      "layer at last for the ﬁnal entity tagging.\n",
      "\n",
      "RNNs have also shown considerable improvement in language modeling over traditional methods based on count statistics.\n",
      "Pioneering work in this ﬁeld was done by Graves [94], who introduced the effectiveness of RNNs in modeling complex\n",
      "sequences with long range context structures. He also proposed deep RNNs where multiple layers of hidden states were used\n",
      "to enhance the modeling. This work established the usage of RNNs on tasks beyond the context of NLP. Later, Sundermeyer\n",
      "et al. [95] compared the gain obtained by replacing a feed-forward neural network with an RNN when conditioning the\n",
      "prediction of a word on the words ahead. In their work, they proposed a typical hierarchy in neural network architectures\n",
      "where feed-forward neural networks gave considerable improvement over traditional count-based language models, which in\n",
      "turn were superseded by RNNs and later by LSTMs. An important point that they mentioned was the applicability of their\n",
      "conclusions to a variety of other tasks such as statistical machine translation [96].\n",
      "\n",
      "2) RNN for sentence-level classiﬁcation: Wang et al. [25] proposed encoding entire tweets with LSTM, whose hidden\n",
      "state is used for predicting sentiment polarity. This simple strategy proved competitive to the more complex DCNN structure\n",
      "by Kalchbrenner et al. [49] designed to endow CNN models with ability to capture long-term dependencies. In a special case\n",
      "studying negation phrase, the authors also showed that the dynamics of LSTM gates can capture the reversal effect of the word\n",
      "not.\n",
      "\n",
      "Similar to CNN, the hidden state of an RNN can also be used for semantic matching between texts. In dialogue systems,\n",
      "Lowe et al. [97] proposed to match a message with candidate responses with Dual-LSTM, which encodes both as ﬁxed-size\n",
      "vectors and then measure their inner product as the basis to rank candidate responses.\n",
      "\n",
      "3) RNN for generating language: A challenging task in NLP is generating natural language, which is another natural\n",
      "application of RNNs. Conditioned on textual or visual data, deep LSTMs have been shown to generate reasonable task-speciﬁc\n",
      "text in tasks such as machine translation, image captioning, etc. In such cases, the RNN is termed a decoder.\n",
      "\n",
      "In [74], the authors proposed a general deep LSTM encoder-decoder framework that maps a sequence to another sequence.\n",
      "One LSTM is used to encode the “source” sequence as a ﬁxed-size vector, which can be text in the original language (machine\n",
      "translation), the question to be answered (QA) or the message to be replied to (dialogue systems). The vector is used as the\n",
      "initial state of another LSTM, named the decoder. During inference, the decoder generates tokens one by one, while updating\n",
      "its hidden state with the last generated token. Beam search is often used to approximate the optimal sequence.\n",
      "\n",
      "Sutskever et al. [74] experimented with 4-layer LSTM on a machine translation task in an end-to-end fashion, showing\n",
      "competitive results. In [99], the same encoder-decoder framework is employed to model human conversations. When trained\n",
      "on more than 100 million message-response pairs, the LSTM decoder is able to generate very interesting responses in the open\n",
      "\n",
      "\f",
      "14\n",
      "\n",
      "Fig. 12: LSTM decoder combined with a CNN image embedder to generate image captioning (Figure source: Vinyals et al.\n",
      "[98])\n",
      "\n",
      "domain. It is also common to condition the LSTM decoder on additional signal to achieve certain effects. In [100], the authors\n",
      "proposed to condition the decoder on a constant persona vector that captures the personal information of an individual speaker.\n",
      "In the above cases, language is generated based mainly on the semantic vector representing textual input. Similar frameworks\n",
      "have also been successfully used in image-based language generation, where visual features are used to condition the LSTM\n",
      "decoder (Fig. 12).\n",
      "\n",
      "Visual QA is another task that requires language generation based on both textual and visual clues. Malinowski et al. [101]\n",
      "were the ﬁrst to provide an end-to-end deep learning solution where they predicted the answer as a set of words conditioned\n",
      "on the input image modeled by a CNN and text modeled by an LSTM (Fig. 13).\n",
      "\n",
      "Kumar et al. [102] tackled this problem by proposing an elaborated network termed dynamic memory network (DMN),\n",
      "which had four sub-modules. The idea was to repeatedly attend to the input text and image to form episodes of information\n",
      "improved at each iteration. Attention networks were used for ﬁne-grained focus on input text phrases.\n",
      "\n",
      "D. Attention Mechanism\n",
      "\n",
      "One potential problem that the traditional encoder-decoder framework faces is that the encoder at times is forced to encode\n",
      "information which might not be fully relevant to the task at hand. The problem arises also if the input is long or very\n",
      "information-rich and selective encoding is not possible.\n",
      "\n",
      "For example, the task of text summarization can be cast as a sequence-to-sequence learning problem, where the input is the\n",
      "original text and the output is the condensed version. Intuitively, it is unrealistic to expect a ﬁxed-size vector to encode all\n",
      "information in a piece of text whose length can potentially be very long. Similar problems have also been reported in machine\n",
      "translation [103].\n",
      "\n",
      "In tasks such as text summarization and machine translation, certain alignment exists between the input text and the output\n",
      "text, which means that each token generation step is highly related to a certain part of the input text. This intuition inspires the\n",
      "\n",
      "Fig. 13: Neural-image QA (Figure source: Malinowski et al. [101])\n",
      "\n",
      "CNNLSTMLSTMLSTMImagewN−1p1pN−1Output True Image Descriptionp2w2w1\f",
      "15\n",
      "\n",
      "attention mechanism. This mechanism attempts to ease the above problems by allowing the decoder to refer back to the input\n",
      "sequence. Speciﬁcally during decoding, in addition to the last hidden state and generated token, the decoder is also conditioned\n",
      "on a “context” vector calculated based on the input hidden state sequence. The attention mechanism can be broadly seen as\n",
      "mapping a query and a set of key-value pairs to an output, where all the mentioned components are vectors. The output is a\n",
      "combination of the values whose weights are determined by the compatibility between the query and the corresponding keys.\n",
      "This output amounts to the “context” of the input used in decoding the output.\n",
      "\n",
      "Bahdanau et al. [103] ﬁrst applied the attention mechanism to machine translation, which improved the performance especially\n",
      "for long sequences. In their work, the attention signal over the input hidden state sequence is determined with a multi-layer\n",
      "perceptron by the last hidden state of the decoder. By visualizing the attention signal over the input sequence during each\n",
      "decoding step, a clear alignment between the source and target language can be demonstrated (Fig. 14).\n",
      "\n",
      "A similar approach was applied to the task of summarization by Rush et al. [104] where each output word in the summary\n",
      "was conditioned on the input sentence through an attention mechanism. The authors performed abstractive summarization which\n",
      "is not very conventional as opposed to extractive summarization, but can be scaled up to large data with minimal linguistic\n",
      "input.\n",
      "\n",
      "In image captioning, Xu et al. [105] conditioned the LSTM decoder on different parts of the input image during each\n",
      "decoding step. Attention signal was determined by the previous hidden state and CNN features. In [106], the authors casted the\n",
      "syntactical parsing problem as a sequence-to-sequence learning task by linearizing the parsing tree. The attention mechanism\n",
      "proved to be more data-efﬁcient in this work. A further step in referring to the input sequence was to directly copy words\n",
      "or sub-sequences of the input onto the output sequence under a certain condition [107], which was useful in tasks such as\n",
      "dialogue generation and text summarization. Copying or generation was chosen at each time step during decoding [108].\n",
      "\n",
      "In aspect-based sentiment analysis, Wang et al. [109]\n",
      "proposed an attention-based solution where they used aspect\n",
      "embeddings to provide additional support during classiﬁca-\n",
      "tion (Fig. 15). The attention module focused on selective\n",
      "regions of the sentence which affected the aspect\n",
      "to be\n",
      "classiﬁed. This can be seen in Fig. 17 where, for the aspect\n",
      "service in (a), the attention module dynamically focused on\n",
      "the phrase “fastest delivery times” and in (b) with the aspect\n",
      "food, it identiﬁed multiple key-points across the sentence\n",
      "that\n",
      "included “tasteless” and “too sweet”. Recently, Ma\n",
      "et al. [110] augmented LSTM with a hierarchical atten-\n",
      "tion mechanism consisting of a target-level attention and a\n",
      "sentence-level attention to exploit commonsense knowledge\n",
      "for targeted aspect-based sentiment analysis.\n",
      "\n",
      "On the other hand, Tang et al. [111] adopted a solution\n",
      "based on a memory network (also known as MemNet [112]),\n",
      "which employed multiple-hop attention. The multiple at-\n",
      "tention computation layer on the memory led to improved\n",
      "lookup for most informational regions in the memory and\n",
      "subsequently aided the classiﬁcation. Their work stands as\n",
      "the state of the art in this domain.\n",
      "\n",
      "adopted for an increasing number of applications.\n",
      "\n",
      "Given the intuitive applicability of attention modules, they are still being actively investigated by NLP researchers and\n",
      "\n",
      "Fig. 14: Word alignment matrix (Figure source: Bahdanau et al.\n",
      "[103])\n",
      "\n",
      "E. Parallelized Attention: The Transformer\n",
      "\n",
      "Both CNNs and RNNs have been crucial in sequence transduction applications involving the encoder-decoder architecture.\n",
      "Attention-based mechanisms, as described above, have further boosted the capabilities of these models. However, one of the\n",
      "bottlenecks suffered by these architectures is the sequential processing at the encoding step. To address this, Vaswani et al.\n",
      "[113] proposed the Transformer which dispensed the recurrence and convolutions involved in the encoding step entirely and\n",
      "based models only on attention mechanisms to capture the global relations between input and output. As a result, the overall\n",
      "architecture became more parallelizable and required lesser time to train along with positive results on tasks ranging from\n",
      "translation to parsing.\n",
      "\n",
      "The Transformer consists stacked layers in both encoder and decoder components. Each layer has two sub-layers comprising\n",
      "multi-head attention layer (Figure 17) followed by a position-wise feed forward network. For set of queries Q, keys K and\n",
      "\n",
      "\f",
      "16\n",
      "\n",
      "(22)\n",
      "(23)\n",
      "\n",
      "(24)\n",
      "\n",
      "Fig. 15: Aspect classiﬁcation using attention (Figure source: Wang et al. [109])\n",
      "\n",
      "Fig. 16: Multi-head Attention: Vaswani et al. [113])\n",
      "\n",
      "values V , the multi-head attention module performs attention h times where the computation can be seen as:\n",
      "\n",
      "MultiHead(Q, K, V ) = Concat(head1, head2, . . . , headh)W o\n",
      ", V W V\n",
      "i )\n",
      "\n",
      "where headi = Attention(QW Q\n",
      "\n",
      "i , KW K\n",
      "i\n",
      "\n",
      "and Attention(Q, K, V ) = softmax\n",
      "\n",
      "(cid:19)\n",
      "\n",
      "(cid:18) QK T√\n",
      "\n",
      "dk\n",
      "\n",
      "V\n",
      "\n",
      "here, W [.]\n",
      "and W o are projection parameters. Incorporating other techniques such as residual connections [114], layer nor-\n",
      "i\n",
      "malization [115], dropouts, positional encodings, and others, the model achieves state-of-the-art results in English-German and\n",
      "English-French translation and constituency parsing.\n",
      "\n",
      "V. RECURSIVE NEURAL NETWORKS\n",
      "\n",
      "Recurrent neural networks represent a natural way to model sequences. Arguably, however, language exhibits a natural\n",
      "recursive structure, where words and sub-phrases combine into phrases in a hierarchical manner. Such structure can be\n",
      "represented by a constituency parsing tree. Thus, tree-structured models have been used to better make use of such syntactic\n",
      "interpretations of sentence structure [4]. Speciﬁcally, in a recursive neural network, the representation of each non-terminal\n",
      "node in a parsing tree is determined by the representations of all its children.\n",
      "\n",
      "Weighted CombinationLSTMLSTMLSTMHidden RepresentationInput Sentencew1wnAspect Embeddingh1hnαAttentionw2\f",
      "A. Basic model\n",
      "\n",
      "17\n",
      "\n",
      "In this section, we describe the basic structure of recursive neural networks. As shown in Fig. 18a and 18b, the network g\n",
      "deﬁnes a compositional function on the representations of phrases or words (b, c or a, p1) to compute the representation of a\n",
      "higher-level phrase (p1 or p2). The representations of all nodes take the same form.\n",
      "\n",
      "Socher et al. [4] described multiple variations of this model. In its simplest form, g is deﬁned as:\n",
      "\n",
      "p1 = tanh\n",
      "\n",
      "W\n",
      "\n",
      ", p2 = tanh\n",
      "\n",
      "W\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:21)(cid:19)\n",
      "\n",
      "(cid:20) b\n",
      "\n",
      "c\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:20) Cb\n",
      "\n",
      "(cid:21)(cid:19)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:18)\n",
      "\n",
      "(cid:21)(cid:19)\n",
      "\n",
      "(cid:20) a\n",
      "\n",
      "p1\n",
      "\n",
      "(cid:20) B\n",
      "\n",
      "(cid:21)(cid:19)\n",
      "\n",
      "in which the representation for each node is a d-dimensional vector and W ∈ RD×2D.\n",
      "\n",
      "Another variation is the MV-RNN [116]. The idea is to represent every word and phrase as both a matrix and a vector.\n",
      "\n",
      "When two constituents are combined, the matrix of one is multiplied with the vector of the other:\n",
      "\n",
      "(26)\n",
      "in which b, c, p1 ∈ RD, B, C, P1 ∈ RD×D, and WM ∈ RD×2D. Compared to the vanilla form, MV-RNN parameterizes the\n",
      "compositional function with matrices corresponding to the constituents.\n",
      "\n",
      ", P1 = tanh\n",
      "\n",
      "p1 = tanh\n",
      "\n",
      "WM\n",
      "\n",
      "Bc\n",
      "\n",
      "W\n",
      "\n",
      "C\n",
      "\n",
      "The recursive neural tensor network (RNTN) is proposed to introduce more interaction between the input vectors without\n",
      "\n",
      "making the number of parameters exceptionally large like MV-RNN. RNTN is deﬁned by:\n",
      "\n",
      "c\n",
      "where V ∈ R2D×2D×D is a tensor that deﬁnes multiple bilinear forms.\n",
      "\n",
      "c\n",
      "\n",
      "p1 = tanh\n",
      "\n",
      "V [1:D]\n",
      "\n",
      "+ W\n",
      "\n",
      "(cid:32)(cid:20) b\n",
      "\n",
      "(cid:21)T\n",
      "\n",
      "(cid:20) b\n",
      "\n",
      "(cid:21)\n",
      "\n",
      "(cid:21)(cid:33)\n",
      "\n",
      "(cid:20) b\n",
      "\n",
      "c\n",
      "\n",
      "(25)\n",
      "\n",
      "(27)\n",
      "\n",
      "B. Applications\n",
      "\n",
      "One natural application of recursive neural networks is parsing [10]. A scoring function is deﬁned on the phrase representation\n",
      "to calculate the plausibility of that phrase. Beam search is usually applied for searching the best tree. The model is trained\n",
      "with the max-margin objective [117].\n",
      "\n",
      "Based on recursive neural networks and the parsing tree, Socher et al. [4] proposed a phrase-level sentiment analysis\n",
      "\n",
      "framework (Fig. 19), where each node in the parsing tree can be assigned a sentiment label.\n",
      "\n",
      "Socher et al. [116] classiﬁed semantic relationships such as cause-effect or topic-message between nominals in a sentence by\n",
      "building a single compositional semantics for the minimal constituent including both terms. Bowman et al. [118] proposed to\n",
      "classify the logical relationship between sentences with recursive neural networks. The representations for both sentences are\n",
      "fed to another neural network for relationship classiﬁcation. They show that both vanilla and tensor versions of the recursive\n",
      "unit performed competitively in a textual entailment dataset.\n",
      "\n",
      "To avoid the gradient vanishing problem, LSTM units have also been applied to tree structures in [119]. The authors\n",
      "showed improved sentence representation over linear LSTM models, as clear improvement in sentiment analysis and sentence\n",
      "relatedness test was observed.\n",
      "\n",
      "A. Reinforcement learning for sequence generation\n",
      "\n",
      "VI. DEEP REINFORCED MODELS AND DEEP UNSUPERVISED LEARNING\n",
      "\n",
      "Reinforcement learning is a method of training an agent to perform discrete actions before obtaining a reward. In NLP, tasks\n",
      "\n",
      "concerning language generation can sometimes be cast as reinforcement learning problems.\n",
      "\n",
      "Fig. 17: Focus of attention module on the sentence for certain aspects (Figure source: Wang et al. [109])\n",
      "\n",
      "\f",
      "18\n",
      "\n",
      "(a) Recursive neural networks for phrase-level sentiment\n",
      "classiﬁcation (Figure source: Socher et al. [4])\n",
      "\n",
      "(b) Recursive neural networks iteratively form high-level\n",
      "representation from lower-level representations.\n",
      "\n",
      "Fig. 18: Recursive Neural Networks\n",
      "\n",
      "Fig. 19: Recursive neural networks applied on a sentence for sentiment classiﬁcation. Note that “but” plays a crucial role on\n",
      "determining the sentiment of the whole sentence (Figure source: Socher et al. [4])\n",
      "\n",
      "In its original formulation, RNN language generators are typically trained by maximizing the likelihood of each token in the\n",
      "ground-truth sequence given the current hidden state and the previous tokens. Termed “teacher forcing”, this training scheme\n",
      "provides the real sequence preﬁx to the generator during each generation (loss evaluation) step. At test time, however, ground-\n",
      "truth tokens are then replaced by a token generated by the model itself. This discrepancy between training and inference,\n",
      "termed “exposure bias” [120, 121], can yield errors that can accumulate quickly along the generated sequence.\n",
      "\n",
      "Another problem with the word-level maximum likelihood strategy, when training auto-regressive language generation\n",
      "models, is that the training objective is different from the test metric. It is unclear how the n-gram overlap based metrics\n",
      "(BLEU, ROUGE) used to evaluate these tasks (machine translation, dialogue systems, etc.) can be optimized with the word-\n",
      "level training strategy. Empirically, dialogue systems trained with word-level maximum likelihood also tend to produce dull\n",
      "and short-sighted responses [122], while text summarization tends to produce incoherent or repetitive summaries [108].\n",
      "\n",
      "Reinforcement learning offers a prospective to solve the above problems to a certain extent. In order to optimize the non-\n",
      "differentiable evaluation metrics directly, Ranzato et al. [121] applied the REINFORCE algorithm [123] to train RNN-based\n",
      "models for several sequence generation tasks (e.g., text summarization, machine translation and image captioning), leading to\n",
      "improvements compared to previous supervised learning methods. In such a framework, the generative model (RNN) is viewed\n",
      "as an agent, which interacts with the external environment (the words and the context vector it sees as input at every time step).\n",
      "The parameters of this agent deﬁnes a policy, whose execution results in the agent picking an action, which refers to predicting\n",
      "the next word in the sequence at each time step. After taking an action the agent updates its internal state (the hidden units\n",
      "of RNN). Once the agent has reached the end of a sequence, it observes a reward. This reward can be any developer-deﬁned\n",
      "metric tailored to a speciﬁc task. For example, Li et al. [122] deﬁned 3 rewards for a generated sentence based on ease of\n",
      "answering, information ﬂow, and semantic coherence.\n",
      "\n",
      "There are two well-known shortcomings of reinforcement learning. To make reinforcement learning tractable, it is desired\n",
      "to carefully handle the state and action space [124, 125], which in the end may restrict expressive power and learning capacity\n",
      "of the model. Secondly, the need for training the reward functions makes such models hard to design and measure at run\n",
      "time [126, 127].\n",
      "\n",
      "Another approach for sequence-level supervision is to use the adversarial training technique [128], where the training objective\n",
      "\n",
      "\f",
      "for the language generator is to fool another discriminator trained to distinguish generated sequences from real sequences. The\n",
      "generator G and the discriminator D are trained jointly in a min-max game which ideally leads to G, generating sequences\n",
      "indistinguishable from real ones. This approach can be seen as a variation of generative adversarial networks in [128], where\n",
      "G and D are conditioned on certain stimuli (for example, the source image in the task of image captioning). In practice,\n",
      "the above scheme can be realized under the reinforcement learning paradigm with policy gradient. For dialogue systems, the\n",
      "discriminator is analogous to a human Turing tester, who discriminates between human and machine-produced dialogues [129].\n",
      "\n",
      "B. Unsupervised sentence representation learning\n",
      "\n",
      "19\n",
      "\n",
      "Similar to word embeddings, distributed representation for sentences can also be learned in an unsupervised fashion. The\n",
      "result of such unsupervised learning are “sentence encoders”, which map arbitrary sentences to ﬁxed-size vectors that can\n",
      "capture their semantic and syntactic properties. Usually an auxiliary task has to be deﬁned for the learning process.\n",
      "\n",
      "Similar to the skip-gram model [8] for learning word embeddings, the skip-thought model [130] was proposed for learning\n",
      "sentence representation, where the auxiliary task was to predict two adjacent sentences (before and after) based on the given\n",
      "sentence. The seq2seq model was employed for this learning task. One LSTM encoded the sentence to a vector (distributed\n",
      "representation). Two other LSTMs decoded such representation to generate the target sequences. Standard seq2seq training\n",
      "process was used. After training, the encoder could be seen as a generic feature extractor (word embeddings were also learned\n",
      "in the same time).\n",
      "\n",
      "Kiros et al. [130] veriﬁed the quality of the learned sentence encoder on a range of sentence classiﬁcation tasks, showing\n",
      "competitive results with a simple linear model based on the static feature vectors. However, the sentence encoder can also\n",
      "be ﬁne-tuned in the supervised learning task as part of the classiﬁer. Dai and Le [43] investigated the use of the decoder to\n",
      "reconstruct the encoded sentence itself, which resembled an autoencoder [131].\n",
      "\n",
      "Language modeling could also be used as an auxiliary task when training LSTM encoders, where the supervision signal\n",
      "came from the prediction of the next token. Dai and Le [43] conducted experiments on initializing LSTM models with learned\n",
      "parameters on a variety of tasks. They showed that pre-training the sentence encoder on a large unsupervised corpus yielded\n",
      "better accuracy than only pre-training word embeddings. Also, predicting the next token turned out to be a worse auxiliary\n",
      "objective than reconstructing the sentence itself, as the LSTM hidden state was only responsible for a rather short-term objective.\n",
      "\n",
      "C. Deep generative models\n",
      "\n",
      "Recent success in generating realistic images has driven a series of efforts on applying deep generative models to text data.\n",
      "The promise of such research is to discover rich structure in natural language while generating realistic sentences from a latent\n",
      "code space. In this section, we review recent research on achieving this goal with variational autoencoders (VAEs) [132] and\n",
      "generative adversarial networks (GANs) [128].\n",
      "\n",
      "Fig. 20: RNN-based VAE for sentence generation (Figure source: Bowman et al. [133])\n",
      "\n",
      "Standard sentence autoencoders, as in the last section, do not impose any constraint on the latent space, as a result, they\n",
      "fail when generating realistic sentences from arbitrary latent representations [133]. The representations of these sentences may\n",
      "often occupy a small region in the hidden space and most of regions in the hidden space do not necessarily map to a realistic\n",
      "sentence [134]. They cannot be used to assign probabilities to sentences or to sample novel sentences [133].\n",
      "\n",
      "The VAE imposes a prior distribution on the hidden code space which makes it possible to draw proper samples from\n",
      "the model. It modiﬁes the autoencoder architecture by replacing the deterministic encoder function with a learned posterior\n",
      "recognition model. The model consists of encoder and generator networks which encode data examples to latent representation\n",
      "and generate samples from the latent space, respectively. It is trained by maximizing a variational lower bound on the log-\n",
      "likelihood of observed data under the generative model.\n",
      "\n",
      "LSTMLSTMlinearlinearLSTMμσzLSTMLSTMLSTMx1x2x3<EOS><EOS>y1y2y1y2Decoder Encoder \f",
      "20\n",
      "\n",
      "Fig. 21: Multiple supporting facts were retrieved from the memory in order to answer a speciﬁc question using an attention\n",
      "mechanism. The ﬁrst hop uncovered the need for additional hops (Figure source: Sukhbaatar et al. [138])\n",
      "\n",
      "Bowman et al. [133] proposed an RNN-based variational autoencoder generative model that incorporated distributed latent\n",
      "representations of entire sentences (Fig. 20). Unlike vanilla RNN language models, this model worked from an explicit global\n",
      "sentence representation. Samples from the prior over these sentence representations produced diverse and well-formed sentences.\n",
      "Hu et al. [135] proposed generating sentences whose attributes are controlled by learning disentangled latent representations\n",
      "with designated semantics. The authors augmented the latent code in the VAE with a set of structured variables, each targeting\n",
      "a salient and independent semantic feature of sentences. The model incorporated VAE and attribute discriminators, in which\n",
      "the VAE component trained the generator to reconstruct real sentences for generating plausible text, while the discriminators\n",
      "forced the generator to produce attributes coherent with the structured code. When trained on a large number of unsupervised\n",
      "sentences and a small number of labeled sentences, Hu et al. [135] showed that the model was able to generate plausible\n",
      "sentences conditioned on two major attributes of English: tense and sentiment.\n",
      "\n",
      "GAN is another class of generative model composed of two competing networks. A generative neural network decodes latent\n",
      "representation to a data instance, while the discriminative network is simultaneously taught to discriminate between instances\n",
      "from the true data distribution and synthesized instances produced by the generator. GAN does not explicitly represent the true\n",
      "data distribution p(x).\n",
      "\n",
      "Zhang et al. [134] proposed a framework for employing LSTM and CNN for adversarial training to generate realistic text. The\n",
      "latent code z was fed to the LSTM generator at every time step. CNN acted as a binary sentence classiﬁer which discriminated\n",
      "between real data and generated samples. One problem with applying GAN to text is that the gradients from the discriminator\n",
      "cannot properly back-propagate through discrete variables. In [134], this problem was solved by making the word prediction\n",
      "at every time “soft” at the word embedding space. Yu et al. [136] proposed to bypass this problem by modeling the generator\n",
      "as a stochastic policy. The reward signal came from the GAN discriminator judged on a complete sequence, and was passed\n",
      "back to the intermediate state-action steps using Monte Carlo search.\n",
      "\n",
      "The evaluation of deep generative models has been challenging. For text, it is possible to create oracle training data from a\n",
      "ﬁxed set of grammars and then evaluate generative models based on whether (or how well) the generated samples agree with\n",
      "the predeﬁned grammar [137]. Another strategy is to evaluate BLEU scores of samples on a large amount of unseen test data.\n",
      "The ability to generate similar sentences to unseen real data is considered a measurement of quality [136].\n",
      "\n",
      "VII. MEMORY-AUGMENTED NETWORKS\n",
      "\n",
      "The attention mechanism stores a series of hidden vectors of the encoder, which the decoder is allowed to access during the\n",
      "generation of each token. Here, the hidden vectors of the encoder can be seen as entries of the model’s “internal memory”.\n",
      "Recently, there has been a surge of interest in coupling neural networks with a form of memory, which the model can interact\n",
      "with.\n",
      "\n",
      "In [112], the authors proposed memory networks for QA tasks. In synthetic QA, a series of statements (memory entries)\n",
      "were provided to the model as potential supporting facts to the question. The model learned to retrieve one entry at a time\n",
      "from memory based on the question and previously retrieved memory. In large-scale realistic QA, a large set of commonsense\n",
      "knowledge in the form of (subject, relation, object) triples were used as memory. Sukhbaatar et al. [138] extended this work and\n",
      "proposed end-to-end memory networks, where memory entries were retrieved in a “soft” manner with attention mechanism,\n",
      "thus enabling end-to-end training. Multiple rounds (hops) of information retrieval from memory were shown to be essential to\n",
      "good performance and the model was able to retrieve and reason about several supporting facts to answer a speciﬁc question\n",
      "(Fig. 21). Sukhbaatar et al. [138] also showed a special use of the model for language modeling, where each word in the\n",
      "sentence was seen as a memory entry. With multiple hops, the model yielded results comparable to deep LSTM models.\n",
      "\n",
      "Furthermore, dynamic memory networks (DMN) [102] improved upon previous memory-based models by employing neural\n",
      "network models for input representation, attention, and answer mechanisms. The resulting model was applicable to a wide\n",
      "\n",
      "\f",
      "21\n",
      "\n",
      "range of NLP tasks (QA, POS tagging, and sentiment analysis), as every task could be cast to the <memory, question, answer>\n",
      "triple format. Xiong et al. [139] applied the same model to visual QA and proved that the memory module was applicable to\n",
      "visual signals.\n",
      "\n",
      "VIII. PERFORMANCE OF DIFFERENT MODELS ON DIFFERENT NLP TASKS\n",
      "\n",
      "We summarize the performance of a series of deep learning methods on standard datasets developed in recent years on some\n",
      "major NLP topics. Our goal is to show the readers common datasets used in the community and state-of-the-art results with\n",
      "different models.\n",
      "\n",
      "A. POS tagging\n",
      "\n",
      "The WSJ-PTB (the Wall Street Journal part of the Penn Treebank Dataset) corpus contains 1.17 million tokens and has been\n",
      "widely used for developing and evaluating POS tagging systems. Gim´enez and Marquez [140] employed one-against-all SVM\n",
      "based on manually-deﬁned features within a seven-word window, in which some basic n-gram patterns were evaluated to form\n",
      "binary features such as: “previous word is the”, ”two preceding tags are DT NN”, etc. One characteristic of the POS tagging\n",
      "problem was the strong dependency between adjacent tags. With a simple left-to-right tagging scheme, this method modeled\n",
      "dependencies between adjacent tags only by feature engineering. In an effort to reduce feature engineering, Collobert et al. [5]\n",
      "relied on only word embeddings within the word window by a multi-layer perceptron. Incorporating CRF was proven useful\n",
      "in [5]. Santos and Zadrozny [32] concatenated word embeddings with character embeddings to better exploit morphological\n",
      "clues. In [32], the authors did not consider CRF, but since word-level decision was made on a context window, it can be\n",
      "seen that dependencies were modeled implicitly. Huang et al. [141] concatenated word embeddings and manually-designed\n",
      "word-level features and employed bidirectional LSTM to model arbitrarily long context. A series of ablative analysis suggested\n",
      "that bi-directionality and CRF both boosted performance. Andor et al. [142] showed a transition-based approach that produces\n",
      "competitive result with a simple feed-forward neural network. When applied to sequence tagging tasks, DMNs [102] essentially\n",
      "allowed for attending over the context multiple times by treating each RNN hidden state as a memory entry, each time focusing\n",
      "on different parts of the context.\n",
      "\n",
      "TABLE II: POS tagging\n",
      "\n",
      "Paper\n",
      "\n",
      "Model\n",
      "\n",
      "Gim´enez and Marquez [140]\n",
      "\n",
      "Collobert et al. [5]\n",
      "\n",
      "Santos and Zadrozny [32]\n",
      "\n",
      "SVM with manual feature pattern\n",
      "MLP with word embeddings + CRF\n",
      "MLP with character+word embeddings\n",
      "\n",
      "Huang et al. [141]\n",
      "Huang et al. [141]\n",
      "Huang et al. [141]\n",
      "Huang et al. [141]\n",
      "Andor et al. [142]\n",
      "Kumar et al. [102]\n",
      "\n",
      "LSTM\n",
      "\n",
      "Bidirectional LSTM\n",
      "\n",
      "LSTM-CRF\n",
      "\n",
      "Bidirectional LSTM-CRF\n",
      "\n",
      "Transition-based neural network\n",
      "\n",
      "DMN\n",
      "\n",
      "WSJ-PTB (per-token accuracy %)\n",
      "\n",
      "97.16\n",
      "97.29\n",
      "97.32\n",
      "97.29\n",
      "97.40\n",
      "97.54\n",
      "97.55\n",
      "97.45\n",
      "97.56\n",
      "\n",
      "B. Parsing\n",
      "\n",
      "There are two types of parsing: dependency parsing, which connects individual words with their relations, and constituency\n",
      "parsing, which iteratively breaks text into sub-phrases. Transition-based methods are a popular choice since they are linear in\n",
      "the length of the sentence. The parser makes a series of decisions that read words sequentially from a buffer and combine them\n",
      "incrementally into the syntactic structure [143]. At each time step, the decision is made based on a stack containing available\n",
      "tree nodes, a buffer containing unread words and the obtained set of dependency arcs. Chen and Manning [143] modeled the\n",
      "decision making at each time step with a neural network with one hidden layer. The input layer contained embeddings of\n",
      "certain words, POS tags and arc labels, which came from the stack, the buffer and the set of arc labels.\n",
      "\n",
      "Tu et al. [68] extended the work of Chen and Manning [143] by employing a deeper model with 2 hidden layers. However,\n",
      "both Tu et al. [68] and Chen and Manning [143] relied on manual feature selecting from the parser state, and they only took\n",
      "into account a limited number of latest tokens. Dyer et al. [144] proposed stack-LSTMs to model arbitrarily long history. The\n",
      "end pointer of the stack changed position as the stack of tree nodes could be pushed and popped. Zhou et al. [145] integrated\n",
      "beam search and contrastive learning for better optimization.\n",
      "\n",
      "Transition-based models were applied to constituency parsing as well. Zhu et al. [146] based each transition action on\n",
      "features such as the POS tags and constituent labels of the top few words of the stack and the buffer. By uniquely representing\n",
      "the parsing tree with a linear sequence of labels, Vinyals et al. [106] applied the seq2seq learning method to this problem.\n",
      "\n",
      "\f",
      "TABLE III: Parsing (UAS/LAS = Unlabeled/labeled Attachment Score; WSJ = The Wall Street Journal Section of Penn Treebank)\n",
      "\n",
      "22\n",
      "\n",
      "Parsing type\n",
      "\n",
      "Dependency Parsing\n",
      "\n",
      "Constituency Parsing\n",
      "\n",
      "Paper\n",
      "\n",
      "Chen and Manning [143]\n",
      "\n",
      "Weiss et al. [147]\n",
      "Dyer et al. [144]\n",
      "Zhou et al. [145]\n",
      "Petrov et al. [148]\n",
      "Socher et al. [10]\n",
      "Zhu et al. [146]\n",
      "\n",
      "Vinyals et al. [106]\n",
      "\n",
      "Model\n",
      "\n",
      "Fully-connected NN with features including POS\n",
      "\n",
      "Deep fully-connected NN with features including POS\n",
      "\n",
      "Stack-LSTM\n",
      "\n",
      "Beam contrastive model\n",
      "\n",
      "Probabilistic context-free grammars (PCFG)\n",
      "\n",
      "Recursive neural networks\n",
      "\n",
      "Feature-based transition parsing\n",
      "\n",
      "seq2seq learning with LSTM+Attention\n",
      "\n",
      "WSJ\n",
      "\n",
      "91.8/89.6 (UAS/LAS)\n",
      "94.3/92.4 (UAS/LAS)\n",
      "93.1/90.9 (UAS/LAS)\n",
      "93.31/92.37 (UAS/LAS)\n",
      "\n",
      "91.8 (F1 Score)\n",
      "90.29 (F1 Score)\n",
      "91.3 (F1 Score)\n",
      "93.5 (F1 Score)\n",
      "\n",
      "TABLE IV: Named-Entity Recognition\n",
      "\n",
      "Chiu and Nichols [150]\n",
      "\n",
      "Bi-LSTM with word+char+lexicon embeddings\n",
      "\n",
      "Paper\n",
      "\n",
      "Collobert et al. [5]\n",
      "Passos et al. [149]\n",
      "\n",
      "Luo et al. [151]\n",
      "Lample et al. [93]\n",
      "Lample et al. [93]\n",
      "Strubell et al. [152]\n",
      "\n",
      "Model\n",
      "\n",
      "MLP with word embeddings+gazetteer\n",
      "Lexicon Infused Phrase Embeddings\n",
      "\n",
      "Semi-CRF jointly trained with linking\n",
      "\n",
      "Bi-LSTM-CRF with word+char embeddings\n",
      "\n",
      "Bi-LSTM with word+char embeddings\n",
      "\n",
      "Dilated CNN with CRF\n",
      "\n",
      "CoNLL 2003 (F1 %)\n",
      "\n",
      "89.59\n",
      "90.90\n",
      "90.77\n",
      "91.20\n",
      "90.94\n",
      "89.15\n",
      "90.54\n",
      "\n",
      "C. Named-Entity Recognition\n",
      "\n",
      "CoNLL 2003 has been a standard English dataset for NER, which concentrates on four types of named entities: people,\n",
      "locations, organizations and miscellaneous entities. NER is one of the NLP problems where lexicons can be very useful.\n",
      "Collobert et al. [5] ﬁrst achieved competitive results with neural structures augmented by gazetteer features. Chiu and Nichols\n",
      "[150] concatenated lexicon features, character embeddings and word embeddings and fed them as input to a bidirectional\n",
      "LSTM. On the other hand, Lample et al. [93] only relied on character and word embeddings, with pre-training embeddings\n",
      "on large unsupervised corpora, they achieved competitive results without using any lexicon. Similar to POS tagging, CRF also\n",
      "boosted the performance of NER, as demonstrated by the comparison in [93]. Overall, we see that bidirectional LSTM with\n",
      "CRF acts as a strong model for NLP problems related to structured prediction.\n",
      "\n",
      "Passos et al. [149] proposed to modify skip-gram models to better learn entity-type related word embeddings that can leverage\n",
      "information from relevant lexicons. Luo et al. [151] jointly optimized the entities and the linking of entities to a KB. Strubell\n",
      "et al. [152] proposed to use dilated convolutions, deﬁned over a wider effective input width by skipping over certain inputs at\n",
      "a time, for better parallelization and context modeling. The model showed signiﬁcant speedup while retaining accuracy.\n",
      "\n",
      "D. Semantic Role Labeling\n",
      "\n",
      "Semantic role labeling (SRL) aims to discover the predicate-argument structure of each predicate in a sentence. For each\n",
      "target verb (predicate), all constituents in the sentence which take a semantic role of the verb are recognized. Typical semantic\n",
      "arguments include Agent, Patient, Instrument, etc., and also adjuncts such as Locative, Temporal, Manner, Cause, etc. [153].\n",
      "Table V shows the performance of different models on the CoNLL 2005 and 2012 datasets.\n",
      "\n",
      "Traditional SRL systems consist of several stages: producing a parse tree, identifying which parse tree nodes represent the\n",
      "arguments of a given verb, and ﬁnally classifying these nodes to determine the corresponding SRL tags. Each classiﬁcation\n",
      "process usually entails extracting numerous features and feeding them into statistical models [5].\n",
      "\n",
      "Given a predicate, T¨ackstr¨om et al. [154] scored a constituent span and its possible role to that predicate with a series of\n",
      "features based on the parse tree. They proposed a dynamic programming algorithm for efﬁcient inference. Collobert et al. [5]\n",
      "achieved comparable results with a convolution neural networks augmented by parsing information provided in the form of\n",
      "additional look-up tables. Zhou and Xu [153] proposed to use bidirectional LSTM to model arbitrarily long context, which\n",
      "proved to be successful without any parsing tree information. He et al. [155] further extended this work by introducing highway\n",
      "connections [156], more advanced regularization and ensemble of multiple experts.\n",
      "\n",
      "TABLE V: Semantic Role Labeling\n",
      "\n",
      "Paper\n",
      "\n",
      "Collobert et al. [5]\n",
      "\n",
      "T¨ackstr¨om et al. [154]\n",
      "Zhou and Xu [153]\n",
      "\n",
      "He et al. [155]\n",
      "\n",
      "Model\n",
      "\n",
      "CNN with parsing features\n",
      "\n",
      "Manual features with DP for inference\n",
      "\n",
      "Bidirectional LSTM\n",
      "\n",
      "Bidirectional LSTM with highway connections\n",
      "\n",
      "CoNLL2005 (F1 %)\n",
      "\n",
      "76.06\n",
      "78.6\n",
      "81.07\n",
      "83.2\n",
      "\n",
      "CoNLL2012 (F1 %)\n",
      "\n",
      "79.4\n",
      "81.27\n",
      "83.4\n",
      "\n",
      "\f",
      "E. Sentiment Classiﬁcation\n",
      "\n",
      "23\n",
      "\n",
      "The Stanford Sentiment Treebank (SST) dataset contains sentences taken from the movie review website Rotten Tomatoes.\n",
      "It was proposed by Pang and Lee [157] and subsequently extended by Socher et al. [4]. The annotation scheme has inspired\n",
      "a new dataset for sentiment analysis, called CMU-MOSI, where sentiment is studied in a multimodal setup [158].\n",
      "\n",
      "[4] and [119] were both recursive networks that relied on constituency parsing trees. Their difference shows the effectiveness\n",
      "of LSTM over vanilla RNN in modeling sentences. On the other hand, tree-LSTM performed better than linear bidirectional\n",
      "LSTM, implying that tree structures can potentially better capture the syntactical property of natural sentences. Yu et al. [159]\n",
      "proposed to reﬁne pre-trained word embeddings with a sentiment lexicon, observing improved results based on [119].\n",
      "\n",
      "Kim [50] and Kalchbrenner et al. [49] both used convolutional layers. The model [50] was similar to the one in Fig. 6,\n",
      "while Kalchbrenner et al. [49] constructed the model in a hierarchical manner by interweaving k-max pooling layers with\n",
      "convolutional layers.\n",
      "\n",
      "TABLE VI: Sentiment Classiﬁcation (SST-1 = Stanford Sentiment Treebank, ﬁne-grained 5 classes Socher et al. [4]; SST-2: the binary\n",
      "version of SST-1; Numbers are accuracies (%))\n",
      "\n",
      "Paper\n",
      "\n",
      "Socher et al. [4]\n",
      "\n",
      "Kim [50]\n",
      "\n",
      "Kalchbrenner et al. [49]\n",
      "\n",
      "Tai et al. [119]\n",
      "\n",
      "Le and Mikolov [160]\n",
      "\n",
      "Tai et al. [119]\n",
      "Yu et al. [159]\n",
      "\n",
      "Kumar et al. [102]\n",
      "\n",
      "Model\n",
      "\n",
      "Recursive Neural Tensor Network\n",
      "\n",
      "Multichannel CNN\n",
      "\n",
      "DCNN with k-max pooling\n",
      "\n",
      "Bidirectional LSTM\n",
      "\n",
      "Paragraph Vector\n",
      "\n",
      "Constituency Tree-LSTM\n",
      "\n",
      "Tree-LSTM with reﬁned word embeddings\n",
      "\n",
      "DMN\n",
      "\n",
      "SST-1\n",
      "45.7\n",
      "47.4\n",
      "48.5\n",
      "48.5\n",
      "48.7\n",
      "51.0\n",
      "54.0\n",
      "52.1\n",
      "\n",
      "SST-2\n",
      "85.4\n",
      "88.1\n",
      "86.8\n",
      "87.2\n",
      "87.8\n",
      "88.0\n",
      "90.3\n",
      "88.6\n",
      "\n",
      "F. Machine Translation\n",
      "\n",
      "The phrase-based SMT framework [161] factorized the translation model into the translation probabilities of matching\n",
      "phrases in the source and target sentences. Cho et al. [82] proposed to learn the translation probability of a source phrase\n",
      "to a corresponding target phrase with an RNN encoder-decoder. Such a scheme of scoring phrase pairs improved translation\n",
      "performance. Sutskever et al. [74], on the other hand, re-scored the top 1000 best candidate translations produced by an\n",
      "SMT system with a 4-layer LSTM seq2seq model. Dispensing the traditional SMT system entirely, Wu et al. [162] trained\n",
      "a deep LSTM network with 8 encoder and 8 decoder layers with residual connections as well as attention connections. Wu\n",
      "et al. [162] then reﬁned the model by using reinforcement learning to directly optimize BLEU scores, but they found that\n",
      "the improvement in BLEU scores by this method did not reﬂect in human evaluation of translation quality. Recently, Gehring\n",
      "et al. [163] proposed a CNN-based seq2seq learning model for machine translation. The representation for each word in the\n",
      "input is computed by CNN in a parallelized style for the attention mechanism. The decoder state is also determined by CNN\n",
      "with words that are already produced. Vaswani et al. [113] proposed a self-attention-based model and dispensed convolutions\n",
      "and recurrences entirely.\n",
      "\n",
      "TABLE VII: Machine translation (Numbers are BLEU scores)\n",
      "\n",
      "Model\n",
      "\n",
      "WMT2014 English2German WMT2014 English2French\n",
      "\n",
      "Paper\n",
      "\n",
      "Cho et al. [82]\n",
      "\n",
      "Sutskever et al. [74]\n",
      "\n",
      "Wu et al. [162]\n",
      "\n",
      "Gehring et al. [163]\n",
      "Vaswani et al. [113]\n",
      "\n",
      "Phrase table with neural features\n",
      "\n",
      "Reranking phrase-based SMT best list with LSTM seq2seq\n",
      "Residual LSTM seq2seq + Reinforcement learning reﬁning\n",
      "\n",
      "seq2seq with CNN\n",
      "Attention mechanism\n",
      "\n",
      "26.30\n",
      "26.36\n",
      "28.4\n",
      "\n",
      "34.50\n",
      "36.5\n",
      "41.16\n",
      "41.29\n",
      "41.0\n",
      "\n",
      "G. Question answering\n",
      "\n",
      "QA problems take many forms. Some rely on large KBs to answer open-domain questions, while others answer a question\n",
      "based on a few sentences or a paragraph (reading comprehension). For the former, we list (see Table VIII) several experiments\n",
      "conducted on a large-scale QA dataset introduced by [164], where 14M commonsense knowledge triples are considered as the\n",
      "KB. Each question can be answered with a single-relation query. For the latter, we consider (see Table VIII) (1) the synthetic\n",
      "dataset of bAbI [165], which requires the model to reason over multiple related facts to produce the right answer. It contains\n",
      "20 synthetic tasks that test a model’s ability to retrieve relevant facts and reason over them. Each task focuses on a different\n",
      "skill such as basic coreference and size reasoning. (2) The Stanford Question Answering Dataset (SQuAD) [166], consisting\n",
      "of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles. The answer to each question is a segment of\n",
      "text from the corresponding article.\n",
      "\n",
      "The central problem of learning to answer single-relation queries is to ﬁnd the single supporting fact in the database. Fader\n",
      "et al. [164] proposed to tackle this problem by learning a lexicon that maps natural language patterns to database concepts\n",
      "\n",
      "\f",
      "24\n",
      "\n",
      "(entities, relations, question patterns) based on a question paraphrasing dataset. Bordes et al. [167] embedded both questions\n",
      "and KB triples as dense vectors and scored them with inner product.\n",
      "\n",
      "Weston et al. [112] took a similar approach by treating the KB as long-term memory, while casting the problem in the\n",
      "framework of a memory network. On the bAbI dataset, Sukhbaatar et al. [138] improved upon the original memory networks\n",
      "model [112] by making the training procedure agnostic of the actual supporting fact, while Kumar et al. [102] used neural\n",
      "sequence models (GRU) instead of neural bag-of-words models as in [138] and [112] to embed memories.\n",
      "\n",
      "For models on the SQuAD dataset, the goal is to determine the start point and end point of the answer segment. Chen\n",
      "et al. [168] encoded both the question and the words in context using LSTMs and used a bilinear matrix for calculating the\n",
      "similarity between the two. Shen et al. [169] proposed Reasonet, a model that read a document repeatedly with attention on\n",
      "different parts each time until a satisfying answer is found. Yu et al. [170] replaced RNNs with convolution and self-attention\n",
      "for encoding the question and the context with signiﬁcant speed improvement.\n",
      "\n",
      "TABLE VIII: Question answering\n",
      "bAbI (Mean accuracy %)\n",
      "\n",
      "Model\n",
      "\n",
      "Paraphrase-driven lexicon learning\n",
      "\n",
      "Weekly supervised embedding\n",
      "\n",
      "Memory networks\n",
      "\n",
      "Sukhbaatar et al. [138]\n",
      "\n",
      "End-to-end memory networks\n",
      "\n",
      "Paper\n",
      "\n",
      "Fader et al. [164]\n",
      "Bordes et al. [167]\n",
      "Weston et al. [112]\n",
      "\n",
      "Kumar et al. [102]\n",
      "Chen et al. [168]\n",
      "Shen et al. [169]\n",
      "Yu et al. [170]\n",
      "\n",
      "DMN\n",
      "\n",
      "Document Reader\n",
      "\n",
      "ReasoNet\n",
      "\n",
      "QAnet\n",
      "\n",
      "93.3\n",
      "88.4\n",
      "93.6\n",
      "\n",
      "Farbes (Accuracy %)\n",
      "\n",
      "0.54\n",
      "0.73\n",
      "0.83\n",
      "\n",
      "SQuAD (EM/F1 %)\n",
      "\n",
      "70.0/79.0\n",
      "69.1/78.9\n",
      "76.2/84.6\n",
      "\n",
      "H. Dialogue Systems\n",
      "\n",
      "Two types of dialogue systems have been developed: generation-based models and retrieval-based models.\n",
      "In Table IX, the Twitter Conversation Triple Dataset is typically used for evaluating generation-based dialogue systems,\n",
      "containing 3-turn Twitter conversation instances. One commonly used evaluation metric is BLEU [171], although it is commonly\n",
      "acknowledged that most automatic evaluation metrics are not completely reliable for dialogue evaluation and additional human\n",
      "evaluation is often necessary. Ritter et al. [172] employed the phrase-based statistical machine translation (SMT) framework\n",
      "to “translate” the message to its appropriate response. Sordoni et al. [173] reranked the 1000 best responses produced by\n",
      "SMT with a context-sensitive RNN encoder-decoder framework, observing substantial gains. Li et al. [174] reported results on\n",
      "replacing the traditional maximum log likelihood training objective with the maximum mutual information training objective, in\n",
      "an effort to produce interesting and diverse responses, both of which are tested on a 4-layer LSTM encoder-decoder framework.\n",
      "The response retrieval task is deﬁned as selecting the best response from a repository of candidate responses. Such a model\n",
      "can be evaluated by the recall1@k metric, where the ground-truth response is mixed with k− 1 random responses. The Ubuntu\n",
      "dialogue dataset was constructed by scraping multi-turn Ubuntu trouble-shooting dialogues from an online chatroom [97].\n",
      "Lowe et al. [97] used LSTMs to encode the message and response, and then inner product of the two sentence embeddings is\n",
      "used to rank candidates.\n",
      "\n",
      "Zhou et al. [175] proposed to better exploit the multi-turn nature of human conversation by employing the LSTM encoder\n",
      "on top of sentence-level CNN embeddings, similar to [176]. Dodge et al. [177] cast the problem in the framework of a memory\n",
      "network, where the past conversation was treated as memory and the latest utterance was considered as a “question” to be\n",
      "responded to. The authors showed that using simple neural bag-of-word embedding for sentences can yield competitive results.\n",
      "\n",
      "TABLE IX: Dialogue systems\n",
      "\n",
      "Paper\n",
      "\n",
      "Ritter et al. [172]\n",
      "Sordoni et al. [173]\n",
      "\n",
      "Li et al. [174]\n",
      "Li et al. [174]\n",
      "Lowe et al. [97]\n",
      "Dodge et al. [177]\n",
      "Zhou et al. [175]\n",
      "\n",
      "Model\n",
      "SMT\n",
      "\n",
      "SMT+neural reranking\n",
      "\n",
      "LSTM seq2seq\n",
      "\n",
      "LSTM seq2seq with MMI objective\n",
      "\n",
      "Dual LSTM encoders for semantic matching\n",
      "\n",
      "Memory networks\n",
      "\n",
      "Sentence-level CNN-LSTM encoder\n",
      "\n",
      "Twitter Conversation\n",
      "Triple Dataset (BLEU)\n",
      "\n",
      "3.60\n",
      "4.44\n",
      "4.51\n",
      "5.22\n",
      "\n",
      "Ubuntu Dialogue\n",
      "\n",
      "Dataset (recall 1@10 %)\n",
      "\n",
      "55.22\n",
      "63.72\n",
      "66.15\n",
      "\n",
      "I. Contextual Embeddings\n",
      "\n",
      "In this section, we explore some of the recent results based on contextual embeddings as explained in section II-D. ELMo\n",
      "has contributed signiﬁcantly towards the recent advancement of NLP. In various NLP tasks, ELMo outperformed state of the\n",
      "art by signiﬁcant margin (Table X). However, latest mode BERT surpass ELMo to establish itself as the state-of-the-art in\n",
      "multiple tasks as summarized in Table XI.\n",
      "\n",
      "\f",
      "TABLE X: Comparison of ELMo + Baseline with the previous state of the art (SOTA) on various NLP tasks. The table has\n",
      "been adapted from [41]. SOTA results have been taken from [41]; SQUAD [166]: QA task; SNLI [178]: Stanford Natural\n",
      "Language Inference task; SRL [153]: Semantic Role Labelling; Coref [179]: Coreference Resolution; NER [180]: Named Entity\n",
      "Recognition; SST-5 [4]: Stanford Sentiment Treebank 5-class classiﬁcation;\n",
      "\n",
      "25\n",
      "\n",
      "Task\n",
      "SQuAD\n",
      "SNLI\n",
      "SRL\n",
      "Coref\n",
      "NER\n",
      "SST-5\n",
      "\n",
      "Previous SOTA\n",
      "Liu et al. [181]\n",
      "Qian et al. [182]\n",
      "Luheng et al. [183]\n",
      "Kenton et al. [184]\n",
      "Matthew et al. [185]\n",
      "Bryan et al. [186]\n",
      "\n",
      "Previous\n",
      "SOTA Results\n",
      "84.4\n",
      "88.6\n",
      "81.7\n",
      "67.2\n",
      "91.93 ± 0.19\n",
      "53.7\n",
      "\n",
      "Baseline\n",
      "81.1\n",
      "88.0\n",
      "81.4\n",
      "67.2\n",
      "90.15\n",
      "51.4\n",
      "\n",
      "ELMo +\n",
      "Baseline\n",
      "85.8\n",
      "88.70 ± 0.17\n",
      "84.6\n",
      "70.4\n",
      "92.22 ± 0.10\n",
      "54.7 0.5\n",
      "\n",
      "Increase\n",
      "(Absolute/Relative)\n",
      "4.7 / 24.9%\n",
      "0.7 / 5.8%\n",
      "3.2 / 17.2%\n",
      "3.2 / 9.8%\n",
      "2.06 / 21%\n",
      "3.3 / 6.8%\n",
      "\n",
      "Task\n",
      "QNLI\n",
      "SST-2\n",
      "STS-B\n",
      "RTE\n",
      "SQuAD\n",
      "NER\n",
      "\n",
      "BiLSTM+\n",
      "ELMo+Attn\n",
      "79.9\n",
      "90.9\n",
      "73.3\n",
      "56.8\n",
      "85.8\n",
      "92.2\n",
      "\n",
      "BERT\n",
      "91.1\n",
      "94.9\n",
      "86.5\n",
      "70.1\n",
      "91.1\n",
      "92.8\n",
      "\n",
      "TABLE XI: QNLI [187]: Question Natural Language Inference task; SST-2 [4]: Stanford Sentiment Treebank binary classi-\n",
      "ﬁcation; STS-B [188]: Semantic Textual Similarity Benchmark; RTE [189]: Recognizing Textual Entailment; SQUAD [166]:\n",
      "QA task; NER [180]: Named Entity Recognition.\n",
      "\n",
      "IX. CONCLUSION\n",
      "\n",
      "Deep learning offers a way to harness large amount of computation and data with little engineering by hand [90]. With\n",
      "distributed representation, various deep models have become the new state-of-the-art methods for NLP problems. Supervised\n",
      "learning is the most popular practice in recent deep learning research for NLP. In many real-world scenarios, however, we have\n",
      "unlabeled data which require advanced unsupervised or semi-supervised approaches. In cases where there is lack of labeled data\n",
      "for some particular classes or the appearance of a new class while testing the model, strategies like zero-shot learning should\n",
      "be employed. These learning schemes are still in their developing phase but we expect deep learning based NLP research to be\n",
      "driven in the direction of making better use of unlabeled data. We expect such trend to continue with more and better model\n",
      "designs. We expect to see more NLP applications that employ reinforcement learning methods, e.g., dialogue systems. We also\n",
      "expect to see more research on multimodal learning [190] as, in the real world, language is often grounded on (or correlated\n",
      "with) other signals.\n",
      "\n",
      "Finally, we expect to see more deep learning models whose internal memory (bottom-up knowledge learned from the data)\n",
      "is enriched with an external memory (top-down knowledge inherited from a KB). Coupling symbolic and sub-symbolic AI\n",
      "will be key for stepping forward in the path from NLP to natural language understanding. Relying on machine learning, in\n",
      "fact, is good to make a ‘good guess’ based on past experience, because sub-symbolic methods encode correlation and their\n",
      "decision-making process is probabilistic. Natural language understanding, however, requires much more than that. To use Noam\n",
      "Chomsky’s words, “you do not get discoveries in the sciences by taking huge amounts of data, throwing them into a computer\n",
      "and doing statistical analysis of them: that’s not the way you understand things, you have to have theoretical insights”.\n",
      "\n",
      "REFERENCES\n",
      "\n",
      "[1] E. Cambria and B. White, “Jumping NLP curves: A review of natural language processing research,” IEEE Computational\n",
      "\n",
      "Intelligence Magazine, vol. 9, no. 2, pp. 48–57, 2014.\n",
      "\n",
      "[2] T. Mikolov, M. Karaﬁ´at, L. Burget, J. Cernock`y, and S. Khudanpur, “Recurrent neural network based language model.”\n",
      "\n",
      "in Interspeech, vol. 2, 2010, p. 3.\n",
      "\n",
      "[3] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed representations of words and phrases and\n",
      "\n",
      "their compositionality,” in Advances in neural information processing systems, 2013, pp. 3111–3119.\n",
      "\n",
      "[4] R. Socher, A. Perelygin, J. Y. Wu, J. Chuang, C. D. Manning, A. Y. Ng, C. Potts et al., “Recursive deep models for\n",
      "semantic compositionality over a sentiment treebank,” in Proceedings of the conference on empirical methods in natural\n",
      "language processing (EMNLP), vol. 1631, 2013, p. 1642.\n",
      "\n",
      "[5] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa, “Natural language processing (almost)\n",
      "\n",
      "from scratch,” Journal of Machine Learning Research, vol. 12, no. Aug, pp. 2493–2537, 2011.\n",
      "\n",
      "[6] Y. Goldberg, “A primer on neural network models for natural language processing,” Journal of Artiﬁcial Intelligence\n",
      "\n",
      "[7] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural probabilistic language model,” Journal of machine learning\n",
      "\n",
      "Research, vol. 57, pp. 345–420, 2016.\n",
      "\n",
      "research, vol. 3, no. Feb, pp. 1137–1155, 2003.\n",
      "\n",
      "\f",
      "26\n",
      "\n",
      "[8] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efﬁcient estimation of word representations in vector space,” arXiv\n",
      "\n",
      "[9] J. Weston, S. Bengio, and N. Usunier, “Wsabie: Scaling up to large vocabulary image annotation,” in IJCAI, vol. 11,\n",
      "\n",
      "preprint arXiv:1301.3781, 2013.\n",
      "\n",
      "2011, pp. 2764–2770.\n",
      "\n",
      "[10] R. Socher, C. C. Lin, C. Manning, and A. Y. Ng, “Parsing natural scenes and natural language with recursive neural\n",
      "\n",
      "networks,” in Proceedings of the 28th international conference on machine learning (ICML-11), 2011, pp. 129–136.\n",
      "\n",
      "[11] P. D. Turney and P. Pantel, “From frequency to meaning: Vector space models of semantics,” Journal of artiﬁcial\n",
      "\n",
      "[12] E. Cambria, S. Poria, A. Gelbukh, and M. Thelwall, “Sentiment analysis is a big suitcase,” IEEE Intelligent Systems,\n",
      "\n",
      "intelligence research, vol. 37, pp. 141–188, 2010.\n",
      "\n",
      "vol. 32, no. 6, pp. 74–80, 2017.\n",
      "\n",
      "[13] X. Glorot, A. Bordes, and Y. Bengio, “Domain adaptation for large-scale sentiment classiﬁcation: A deep learning\n",
      "\n",
      "approach,” in Proceedings of the 28th international conference on machine learning (ICML-11), 2011, pp. 513–520.\n",
      "\n",
      "[14] K. M. Hermann and P. Blunsom, “The role of syntax in vector space models of compositional semantics,” in Proceedings\n",
      "of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association\n",
      "for Computational Linguistics, 2013.\n",
      "\n",
      "[15] J. L. Elman, “Distributed representations, simple recurrent networks, and grammatical structure,” Machine learning,\n",
      "\n",
      "vol. 7, no. 2-3, pp. 195–225, 1991.\n",
      "\n",
      "[16] A. M. Glenberg and D. A. Robertson, “Symbol grounding and meaning: A comparison of high-dimensional and embodied\n",
      "\n",
      "theories of meaning,” Journal of memory and language, vol. 43, no. 3, pp. 379–401, 2000.\n",
      "\n",
      "[17] S. T. Dumais, “Latent semantic analysis,” Annual review of information science and technology, vol. 38, no. 1, pp.\n",
      "\n",
      "[18] D. M. Blei, A. Y. Ng, and M. I. Jordan, “Latent dirichlet allocation,” Journal of machine Learning research, vol. 3, no.\n",
      "\n",
      "188–230, 2004.\n",
      "\n",
      "Jan, pp. 993–1022, 2003.\n",
      "\n",
      "[19] R. Collobert and J. Weston, “A uniﬁed architecture for natural language processing: Deep neural networks with multitask\n",
      "\n",
      "learning,” in Proceedings of the 25th international conference on Machine learning. ACM, 2008, pp. 160–167.\n",
      "\n",
      "[20] A. Gittens, D. Achlioptas, and M. W. Mahoney, “Skip-gram-zipf+ uniform= vector additivity,” in Proceedings of the\n",
      "55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), vol. 1, 2017, pp. 69–76.\n",
      "[21] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors for word representation.” in EMNLP, vol. 14,\n",
      "\n",
      "2014, pp. 1532–1543.\n",
      "\n",
      "[22] X. Rong, “word2vec parameter learning explained,” arXiv preprint arXiv:1411.2738, 2014.\n",
      "[23] R. Johnson and T. Zhang, “Semi-supervised convolutional neural networks for text categorization via region embedding,”\n",
      "\n",
      "in Advances in neural information processing systems, 2015, pp. 919–927.\n",
      "\n",
      "[24] R. Socher, J. Pennington, E. H. Huang, A. Y. Ng, and C. D. Manning, “Semi-supervised recursive autoencoders\n",
      "for predicting sentiment distributions,” in Proceedings of the conference on empirical methods in natural language\n",
      "processing. Association for Computational Linguistics, 2011, pp. 151–161.\n",
      "\n",
      "[25] X. Wang, Y. Liu, C. Sun, B. Wang, and X. Wang, “Predicting polarities of tweets by composing word embeddings with\n",
      "\n",
      "[26] D. Tang, F. Wei, N. Yang, M. Zhou, T. Liu, and B. Qin, “Learning sentiment-speciﬁc word embedding for twitter\n",
      "\n",
      "long short-term memory.” in ACL (1), 2015, pp. 1343–1353.\n",
      "\n",
      "sentiment classiﬁcation.” in ACL (1), 2014, pp. 1555–1565.\n",
      "\n",
      "[27] I. Labutov and H. Lipson, “Re-embedding words.” in ACL (2), 2013, pp. 489–493.\n",
      "[28] S. Upadhyay, K.-W. Chang, M. Taddy, A. Kalai, and J. Zou, “Beyond bilingual: Multi-sense word embeddings using\n",
      "\n",
      "multilingual context,” arXiv preprint arXiv:1706.08160, 2017.\n",
      "\n",
      "[29] Y. Kim, Y. Jernite, D. Sontag, and A. M. Rush, “Character-aware neural language models.” in AAAI, 2016, pp. 2741–2749.\n",
      "[30] C. N. Dos Santos and M. Gatti, “Deep convolutional neural networks for sentiment analysis of short texts.” in COLING,\n",
      "\n",
      "[31] C. N. d. Santos and V. Guimaraes, “Boosting named entity recognition with neural character embeddings,” arXiv preprint\n",
      "\n",
      "[32] C. D. Santos and B. Zadrozny, “Learning character-level representations for part-of-speech tagging,” in Proceedings of\n",
      "\n",
      "the 31st International Conference on Machine Learning (ICML-14), 2014, pp. 1818–1826.\n",
      "\n",
      "[33] Y. Ma, E. Cambria, and S. Gao, “Label embedding for zero-shot ﬁne-grained named entity typing,” in COLING, Osaka,\n",
      "\n",
      "[34] X. Chen, L. Xu, Z. Liu, M. Sun, and H. Luan, “Joint learning of character and word embeddings,” in Twenty-Fourth\n",
      "\n",
      "International Joint Conference on Artiﬁcial Intelligence, 2015.\n",
      "\n",
      "[35] X. Zheng, H. Chen, and T. Xu, “Deep learning for chinese word segmentation and pos tagging.” in EMNLP, 2013, pp.\n",
      "\n",
      "647–657.\n",
      "\n",
      "level,” in FLAIRS, 2017, pp. 347–352.\n",
      "\n",
      "[36] H. Peng, E. Cambria, and X. Zou, “Radical-based hierarchical embeddings for chinese sentiment analysis at sentence\n",
      "\n",
      "[37] P. Bojanowski, E. Grave, A. Joulin, and T. Mikolov, “Enriching word vectors with subword information,” arXiv preprint\n",
      "\n",
      "2014, pp. 69–78.\n",
      "\n",
      "arXiv:1505.05008, 2015.\n",
      "\n",
      "2016, pp. 171–180.\n",
      "\n",
      "\f",
      "27\n",
      "\n",
      "arXiv:1607.04606, 2016.\n",
      "\n",
      "arXiv:1707.06556, 2017.\n",
      "\n",
      "arXiv:1707.06961, 2017.\n",
      "\n",
      "[38] A. Herbelot and M. Baroni, “High-risk learning: acquiring new word vectors from tiny data,” arXiv preprint\n",
      "\n",
      "[39] Y. Pinter, R. Guthrie, and J. Eisenstein, “Mimicking word embeddings using subword rnns,” arXiv preprint\n",
      "\n",
      "[40] L. Lucy and J. Gauthier, “Are distributional representations ready for the real world? evaluating word vectors for grounded\n",
      "\n",
      "perceptual meaning,” arXiv preprint arXiv:1705.11168, 2017.\n",
      "\n",
      "[41] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer, “Deep contextualized word\n",
      "\n",
      "representations,” arXiv preprint arXiv:1802.05365, 2018.\n",
      "\n",
      "[42] A. Mousa and B. Schuller, “Contextual bidirectional long short-term memory recurrent neural network language models:\n",
      "A generative approach to sentiment analysis,” in Proceedings of the 15th Conference of the European Chapter of the\n",
      "Association for Computational Linguistics: Volume 1, Long Papers, vol. 1, 2017, pp. 1023–1032.\n",
      "\n",
      "[43] A. M. Dai and Q. V. Le, “Semi-supervised sequence learning,” in Advances in neural information processing systems,\n",
      "\n",
      "2015, pp. 3079–3087.\n",
      "\n",
      "[44] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving language understanding by generative pre-\n",
      "training,” URL https://s3-us-west-2. amazonaws. com/openai-assets/research-covers/language-unsupervised/language\n",
      "understanding paper. pdf, 2018.\n",
      "\n",
      "[45] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional transformers for language\n",
      "\n",
      "understanding,” arXiv preprint arXiv:1810.04805, 2018.\n",
      "\n",
      "[46] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation with deep convolutional neural networks,” in\n",
      "\n",
      "Advances in neural information processing systems, 2012, pp. 1097–1105.\n",
      "\n",
      "[47] A. Sharif Razavian, H. Azizpour, J. Sullivan, and S. Carlsson, “Cnn features off-the-shelf: an astounding baseline for\n",
      "recognition,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, 2014,\n",
      "pp. 806–813.\n",
      "\n",
      "[48] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick, S. Guadarrama, and T. Darrell, “Caffe: Convolutional\n",
      "architecture for fast feature embedding,” in Proceedings of the 22nd ACM international conference on Multimedia. ACM,\n",
      "2014, pp. 675–678.\n",
      "\n",
      "[49] N. Kalchbrenner, E. Grefenstette, and P. Blunsom, “A convolutional neural network for modelling sentences,”\n",
      "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, June 2014. [Online].\n",
      "Available: http://goo.gl/EsQCuC\n",
      "\n",
      "[50] Y. Kim, “Convolutional neural networks for sentence classiﬁcation,” arXiv preprint arXiv:1408.5882, 2014.\n",
      "[51] Y. Zhang and B. Wallace, “A sensitivity analysis of (and practitioners’ guide to) convolutional neural networks for\n",
      "\n",
      "sentence classiﬁcation,” arXiv preprint arXiv:1510.03820, 2015.\n",
      "\n",
      "[52] S. Poria, E. Cambria, and A. Gelbukh, “Aspect extraction for opinion mining with a deep convolutional neural network,”\n",
      "\n",
      "Knowledge-Based Systems, vol. 108, pp. 42–49, 2016.\n",
      "\n",
      "[53] A. Kirillov, D. Schlesinger, W. Forkel, A. Zelenin, S. Zheng, P. Torr, and C. Rother, “Efﬁcient likelihood learning of a\n",
      "\n",
      "generic cnn-crf model for semantic segmentation,” arXiv preprint arXiv:1511.05067, 2015.\n",
      "\n",
      "[54] A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, and K. J. Lang, “Phoneme recognition using time-delay neural networks,”\n",
      "\n",
      "IEEE transactions on acoustics, speech, and signal processing, vol. 37, no. 3, pp. 328–339, 1989.\n",
      "\n",
      "[55] A. Mukherjee and B. Liu, “Aspect extraction through semi-supervised modeling,” in Proceedings of the 50th Annual\n",
      "Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational\n",
      "Linguistics, 2012, pp. 339–348.\n",
      "\n",
      "[56] S. Ruder, P. Ghaffari, and J. G. Breslin, “Insight-1 at semeval-2016 task 5: Deep learning for multilingual aspect-based\n",
      "\n",
      "sentiment analysis,” arXiv preprint arXiv:1609.02748, 2016.\n",
      "\n",
      "[57] P. Wang, J. Xu, B. Xu, C.-L. Liu, H. Zhang, F. Wang, and H. Hao, “Semantic clustering and convolutional neural\n",
      "\n",
      "network for short text categorization.” in ACL (2), 2015, pp. 352–357.\n",
      "\n",
      "[58] S. Poria, E. Cambria, D. Hazarika, and P. Vij, “A deeper look into sarcastic tweets using deep convolutional neural\n",
      "\n",
      "networks,” in COLING, 2016, pp. 1601–1612.\n",
      "\n",
      "[59] M. Denil, A. Demiraj, N. Kalchbrenner, P. Blunsom, and N. de Freitas, “Modelling, visualising and summarising\n",
      "documents with a single convolutional neural network,” 26th International Conference on Computational Linguistics,\n",
      "pp. 1601–1612, 2014.\n",
      "\n",
      "[60] B. Hu, Z. Lu, H. Li, and Q. Chen, “Convolutional neural network architectures for matching natural language sentences,”\n",
      "\n",
      "in Advances in neural information processing systems, 2014, pp. 2042–2050.\n",
      "\n",
      "[61] Y. Shen, X. He, J. Gao, L. Deng, and G. Mesnil, “A latent semantic model with convolutional-pooling structure for\n",
      "information retrieval,” in Proceedings of the 23rd ACM International Conference on Conference on Information and\n",
      "Knowledge Management. ACM, 2014, pp. 101–110.\n",
      "\n",
      "[62] W.-t. Yih, X. He, and C. Meek, “Semantic parsing for single-relation question answering.” in ACL (2). Citeseer, 2014,\n",
      "\n",
      "pp. 643–648.\n",
      "\n",
      "\f",
      "28\n",
      "\n",
      "[63] L. Dong, F. Wei, M. Zhou, and K. Xu, “Question answering over freebase with multi-column convolutional neural\n",
      "\n",
      "[64] A. Severyn and A. Moschitti, “Modeling relational information in question-answer pairs with convolutional neural\n",
      "\n",
      "[65] Y. Chen, L. Xu, K. Liu, D. Zeng, J. Zhao et al., “Event extraction via dynamic multi-pooling convolutional neural\n",
      "\n",
      "networks.” in ACL (1), 2015, pp. 260–269.\n",
      "\n",
      "networks,” arXiv preprint arXiv:1604.01178, 2016.\n",
      "\n",
      "networks.” in ACL (1), 2015, pp. 167–176.\n",
      "\n",
      "[66] O. Abdel-Hamid, A.-r. Mohamed, H. Jiang, L. Deng, G. Penn, and D. Yu, “Convolutional neural networks for speech\n",
      "recognition,” IEEE/ACM Transactions on audio, speech, and language processing, vol. 22, no. 10, pp. 1533–1545, 2014.\n",
      "[67] D. Palaz, R. Collobert et al., “Analysis of cnn-based speech recognition system using raw speech as input,” Idiap, Tech.\n",
      "\n",
      "[68] Z. Tu, B. Hu, Z. Lu, and H. Li, “Context-dependent translation selection using convolutional neural network,” arXiv\n",
      "\n",
      "Rep., 2015.\n",
      "\n",
      "preprint arXiv:1503.02357, 2015.\n",
      "\n",
      "[69] J. L. Elman, “Finding structure in time,” Cognitive science, vol. 14, no. 2, pp. 179–211, 1990.\n",
      "[70] T. Mikolov, S. Kombrink, L. Burget, J. ˇCernock`y, and S. Khudanpur, “Extensions of recurrent neural network language\n",
      "IEEE, 2011,\n",
      "\n",
      "model,” in Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on.\n",
      "pp. 5528–5531.\n",
      "\n",
      "[71] I. Sutskever, J. Martens, and G. E. Hinton, “Generating text with recurrent neural networks,” in Proceedings of the 28th\n",
      "\n",
      "International Conference on Machine Learning (ICML-11), 2011, pp. 1017–1024.\n",
      "\n",
      "[72] S. Liu, N. Yang, M. Li, and M. Zhou, “A recursive recurrent neural network for statistical machine translation,”\n",
      "\n",
      "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pp. 1491–1500, 2014.\n",
      "\n",
      "[73] M. Auli, M. Galley, C. Quirk, and G. Zweig, “Joint language and translation modeling with recurrent neural networks.”\n",
      "\n",
      "in EMNLP, 2013, pp. 1044–1054.\n",
      "\n",
      "[74] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning with neural networks,” in Advances in neural\n",
      "\n",
      "information processing systems, 2014, pp. 3104–3112.\n",
      "\n",
      "[75] T. Robinson, M. Hochberg, and S. Renals, “The use of recurrent neural networks in continuous speech recognition,” in\n",
      "\n",
      "Automatic speech and speaker recognition. Springer, 1996, pp. 233–258.\n",
      "\n",
      "[76] A. Graves, A.-r. Mohamed, and G. Hinton, “Speech recognition with deep recurrent neural networks,” in Acoustics,\n",
      "\n",
      "speech and signal processing (icassp), 2013 ieee international conference on.\n",
      "\n",
      "IEEE, 2013, pp. 6645–6649.\n",
      "\n",
      "[77] A. Graves and N. Jaitly, “Towards end-to-end speech recognition with recurrent neural networks,” in Proceedings of the\n",
      "\n",
      "31st International Conference on Machine Learning (ICML-14), 2014, pp. 1764–1772.\n",
      "\n",
      "[78] H. Sak, A. Senior, and F. Beaufays, “Long short-term memory based recurrent neural network architectures for large\n",
      "\n",
      "vocabulary speech recognition,” arXiv preprint arXiv:1402.1128, 2014.\n",
      "\n",
      "[79] A. Karpathy and L. Fei-Fei, “Deep visual-semantic alignments for generating image descriptions,” in Proceedings of the\n",
      "\n",
      "IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 3128–3137.\n",
      "\n",
      "[80] D. Tang, B. Qin, and T. Liu, “Document modeling with gated recurrent neural network for sentiment classiﬁcation.” in\n",
      "\n",
      "[81] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation of gated recurrent neural networks on sequence\n",
      "\n",
      "EMNLP, 2015, pp. 1422–1432.\n",
      "\n",
      "modeling,” arXiv preprint arXiv:1412.3555, 2014.\n",
      "\n",
      "[82] K. Cho, B. Van Merri¨enboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, “Learning phrase\n",
      "\n",
      "representations using rnn encoder-decoder for statistical machine translation,” arXiv preprint arXiv:1406.1078, 2014.\n",
      "\n",
      "[83] G. Chen, D. Ye, E. Cambria, J. Chen, and Z. Xing, “Ensemble application of convolutional and recurrent neural networks\n",
      "\n",
      "for multi-label text categorization,” in IJCNN, 2017, pp. 2377–2383.\n",
      "\n",
      "[84] S. Poria, E. Cambria, D. Hazarika, N. Mazumder, A. Zadeh, and L.-P. Morency, “Context-dependent sentiment analysis\n",
      "\n",
      "in user-generated videos,” in ACL, 2017, pp. 873–883.\n",
      "\n",
      "[85] A. Zadeh, M. Chen, S. Poria, E. Cambria, and L.-P. Morency, “Tensor fusion network for multimodal sentiment analysis,”\n",
      "\n",
      "[86] E. Tong, A. Zadeh, and L.-P. Morency, “Combating human trafﬁcking with deep multimodal models,” in Association\n",
      "\n",
      "in Empirical Methods in NLP, 2017.\n",
      "\n",
      "for Computational Linguistics, 2017.\n",
      "\n",
      "preprint arXiv:1612.08083, 2016.\n",
      "\n",
      "preprint arXiv:1702.01923, 2017.\n",
      "\n",
      "[87] I. Chaturvedi, E. Ragusa, P. Gastaldo, R. Zunino, and E. Cambria, “Bayesian network based extreme learning machine\n",
      "\n",
      "for subjectivity detection,” Journal of The Franklin Institute, 2017.\n",
      "\n",
      "[88] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier, “Language modeling with gated convolutional networks,” arXiv\n",
      "\n",
      "[89] W. Yin, K. Kann, M. Yu, and H. Sch¨utze, “Comparative study of cnn and rnn for natural language processing,” arXiv\n",
      "\n",
      "[90] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521, no. 7553, pp. 436–444, 2015.\n",
      "[91] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997.\n",
      "[92] F. A. Gers, J. Schmidhuber, and F. Cummins, “Learning to forget: Continual prediction with lstm,” 9th International\n",
      "\n",
      "Conference on Artiﬁcial Neural Networks, pp. 850–855, 1999.\n",
      "\n",
      "\f",
      "29\n",
      "\n",
      "[93] G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, and C. Dyer, “Neural architectures for named entity\n",
      "\n",
      "recognition,” arXiv preprint arXiv:1603.01360, 2016.\n",
      "\n",
      "[94] A. Graves, “Generating sequences with recurrent neural networks,” arXiv preprint arXiv:1308.0850, 2013.\n",
      "[95] M. Sundermeyer, H. Ney, and R. Schl¨uter, “From feedforward to recurrent lstm neural networks for language modeling,”\n",
      "\n",
      "IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), vol. 23, no. 3, pp. 517–529, 2015.\n",
      "\n",
      "[96] M. Sundermeyer, T. Alkhouli, J. Wuebker, and H. Ney, “Translation modeling with bidirectional recurrent neural\n",
      "\n",
      "networks.” in EMNLP, 2014, pp. 14–25.\n",
      "\n",
      "[97] R. Lowe, N. Pow, I. Serban, and J. Pineau, “The ubuntu dialogue corpus: A large dataset for research in unstructured\n",
      "\n",
      "multi-turn dialogue systems,” arXiv preprint arXiv:1506.08909, 2015.\n",
      "\n",
      "[98] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, “Show and tell: A neural image caption generator,” in Proceedings of\n",
      "\n",
      "the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 3156–3164.\n",
      "\n",
      "[99] O. Vinyals and Q. Le, “A neural conversational model,” arXiv preprint arXiv:1506.05869, 2015.\n",
      "[100] J. Li, M. Galley, C. Brockett, G. P. Spithourakis, J. Gao, and B. Dolan, “A persona-based neural conversation model,”\n",
      "\n",
      "arXiv preprint arXiv:1603.06155, 2016.\n",
      "\n",
      "[101] M. Malinowski, M. Rohrbach, and M. Fritz, “Ask your neurons: A neural-based approach to answering questions about\n",
      "\n",
      "images,” in Proceedings of the IEEE international conference on computer vision, 2015, pp. 1–9.\n",
      "\n",
      "[102] A. Kumar, O. Irsoy, J. Su, J. Bradbury, R. English, B. Pierce, P. Ondruska, I. Gulrajani, and R. Socher, “Ask me anything:\n",
      "\n",
      "Dynamic memory networks for natural language processing,” CoRR, abs/1506.07285, 2015.\n",
      "\n",
      "[103] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly learning to align and translate,” arXiv\n",
      "\n",
      "[104] A. M. Rush, S. Chopra, and J. Weston, “A neural attention model for abstractive sentence summarization,” arXiv preprint\n",
      "\n",
      "preprint arXiv:1409.0473, 2014.\n",
      "\n",
      "arXiv:1509.00685, 2015.\n",
      "\n",
      "[105] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel, and Y. Bengio, “Show, attend and tell: Neural\n",
      "image caption generation with visual attention,” in International Conference on Machine Learning, 2015, pp. 2048–2057.\n",
      "[106] O. Vinyals, Ł. Kaiser, T. Koo, S. Petrov, I. Sutskever, and G. Hinton, “Grammar as a foreign language,” in Advances in\n",
      "\n",
      "Neural Information Processing Systems, 2015, pp. 2773–2781.\n",
      "\n",
      "[107] O. Vinyals, M. Fortunato, and N. Jaitly, “Pointer networks,” in Advances in Neural Information Processing Systems,\n",
      "\n",
      "[108] R. Paulus, C. Xiong, and R. Socher, “A deep reinforced model for abstractive summarization,” arXiv preprint\n",
      "\n",
      "[109] Y. Wang, M. Huang, X. Zhu, and L. Zhao, “Attention-based lstm for aspect-level sentiment classiﬁcation.” in EMNLP,\n",
      "\n",
      "[110] Y. Ma, H. Peng, and E. Cambria, “Targeted aspect-based sentiment analysis via embedding commonsense knowledge\n",
      "\n",
      "[111] D. Tang, B. Qin, and T. Liu, “Aspect level sentiment classiﬁcation with deep memory network,” arXiv preprint\n",
      "\n",
      "[112] J. Weston, S. Chopra, and A. Bordes, “Memory networks,” arXiv preprint arXiv:1410.3916, 2014.\n",
      "[113] A. Vaswani, N. Shazeer, N. Parmar, and J. Uszkoreit, “Attention is all you need,” arXiv preprint arXiv:1706.03762,\n",
      "\n",
      "[114] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE\n",
      "\n",
      "conference on computer vision and pattern recognition, 2016, pp. 770–778.\n",
      "\n",
      "[115] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv preprint arXiv:1607.06450, 2016.\n",
      "[116] R. Socher, B. Huval, C. D. Manning, and A. Y. Ng, “Semantic compositionality through recursive matrix-vector spaces,”\n",
      "in Proceedings of the 2012 joint conference on empirical methods in natural language processing and computational\n",
      "natural language learning. Association for Computational Linguistics, 2012, pp. 1201–1211.\n",
      "\n",
      "[117] B. Taskar, C. Guestrin, and D. Koller, “Max-margin markov networks,” in Advances in neural information processing\n",
      "\n",
      "[118] S. R. Bowman, C. Potts, and C. D. Manning, “Recursive neural networks can learn logical semantics,” arXiv preprint\n",
      "\n",
      "systems, 2004, pp. 25–32.\n",
      "\n",
      "arXiv:1406.1827, 2014.\n",
      "\n",
      "[119] K. S. Tai, R. Socher, and C. D. Manning, “Improved semantic representations from tree-structured long short-term\n",
      "\n",
      "memory networks,” arXiv preprint arXiv:1503.00075, 2015.\n",
      "\n",
      "[120] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer, “Scheduled sampling for sequence prediction with recurrent neural\n",
      "\n",
      "networks,” in Advances in Neural Information Processing Systems, 2015, pp. 1171–1179.\n",
      "\n",
      "[121] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba, “Sequence level training with recurrent neural networks,” arXiv\n",
      "\n",
      "preprint arXiv:1511.06732, 2015.\n",
      "\n",
      "arXiv preprint arXiv:1606.01541, 2016.\n",
      "\n",
      "[122] J. Li, W. Monroe, A. Ritter, M. Galley, J. Gao, and D. Jurafsky, “Deep reinforcement learning for dialogue generation,”\n",
      "\n",
      "[123] R. J. Williams, “Simple statistical gradient-following algorithms for connectionist reinforcement learning,” Machine\n",
      "\n",
      "2015, pp. 2692–2700.\n",
      "\n",
      "arXiv:1705.04304, 2017.\n",
      "\n",
      "2016, pp. 606–615.\n",
      "\n",
      "into an attentive lstm,” in AAAI, 2018.\n",
      "\n",
      "arXiv:1605.08900, 2016.\n",
      "\n",
      "2017.\n",
      "\n",
      "\f",
      "30\n",
      "\n",
      "learning, vol. 8, no. 3-4, pp. 229–256, 1992.\n",
      "\n",
      "[124] S. Young, M. Gaˇsi´c, S. Keizer, F. Mairesse, J. Schatzmann, B. Thomson, and K. Yu, “The hidden information state\n",
      "model: A practical framework for pomdp-based spoken dialogue management,” Computer Speech & Language, vol. 24,\n",
      "no. 2, pp. 150–174, 2010.\n",
      "\n",
      "[125] S. Young, M. Gaˇsi´c, B. Thomson, and J. D. Williams, “Pomdp-based statistical spoken dialog systems: A review,”\n",
      "\n",
      "Proceedings of the IEEE, vol. 101, no. 5, pp. 1160–1179, 2013.\n",
      "\n",
      "[126] P.-h. Su, V. David, D. Kim, T.-h. Wen, and S. Young, “Learning from real users: Rating dialogue success with neural\n",
      "networks for reinforcement learning in spoken dialogue systems,” in in Proceedings of Interspeech. Citeseer, 2015, pp.\n",
      "2007–2011.\n",
      "\n",
      "[127] P.-H. Su, M. Gasic, N. Mrksic, L. Rojas-Barahona, S. Ultes, D. Vandyke, T.-H. Wen, and S. Young, “On-line active\n",
      "\n",
      "reward learning for policy optimisation in spoken dialogue systems,” arXiv preprint arXiv:1605.07669, 2016.\n",
      "\n",
      "[128] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative\n",
      "\n",
      "adversarial nets,” in Advances in neural information processing systems, 2014, pp. 2672–2680.\n",
      "\n",
      "[129] J. Li, W. Monroe, T. Shi, A. Ritter, and D. Jurafsky, “Adversarial learning for neural dialogue generation,” arXiv preprint\n",
      "\n",
      "[130] R. Kiros, Y. Zhu, R. R. Salakhutdinov, R. Zemel, R. Urtasun, A. Torralba, and S. Fidler, “Skip-thought vectors,” in\n",
      "\n",
      "Advances in neural information processing systems, 2015, pp. 3294–3302.\n",
      "\n",
      "[131] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning internal representations by error propagation,” DTIC\n",
      "\n",
      "arXiv:1701.06547, 2017.\n",
      "\n",
      "Document, Tech. Rep., 1985.\n",
      "\n",
      "[132] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv preprint arXiv:1312.6114, 2013.\n",
      "[133] S. R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai, R. Jozefowicz, and S. Bengio, “Generating sentences from a continuous\n",
      "\n",
      "space,” arXiv preprint arXiv:1511.06349, 2015.\n",
      "\n",
      "[134] Y. Zhang, Z. Gan, and L. Carin, “Generating text via adversarial training,” in NIPS workshop on Adversarial Training,\n",
      "\n",
      "2016.\n",
      "\n",
      "arXiv:1703.00955, 2017.\n",
      "\n",
      "[135] Z. Hu, Z. Yang, X. Liang, R. Salakhutdinov, and E. P. Xing, “Controllable text generation,” arXiv preprint\n",
      "\n",
      "[136] L. Yu, W. Zhang, J. Wang, and Y. Yu, “Seqgan: sequence generative adversarial nets with policy gradient,” in Thirty-First\n",
      "\n",
      "AAAI Conference on Artiﬁcial Intelligence, 2017.\n",
      "\n",
      "[137] S. Rajeswar, S. Subramanian, F. Dutil, C. Pal, and A. Courville, “Adversarial generation of natural language,” arXiv\n",
      "\n",
      "preprint arXiv:1705.10929, 2017.\n",
      "\n",
      "systems, 2015, pp. 2440–2448.\n",
      "\n",
      "[138] S. Sukhbaatar, J. Weston, R. Fergus et al., “End-to-end memory networks,” in Advances in neural information processing\n",
      "\n",
      "[139] C. Xiong, S. Merity, and R. Socher, “Dynamic memory networks for visual and textual question answering,” arXiv, vol.\n",
      "\n",
      "[140] J. Gim´enez and L. Marquez, “Fast and accurate part-of-speech tagging: The svm approach revisited,” Recent Advances\n",
      "\n",
      "in Natural Language Processing III, pp. 153–162, 2004.\n",
      "\n",
      "[141] Z. Huang, W. Xu, and K. Yu, “Bidirectional lstm-crf models for sequence tagging,” arXiv preprint arXiv:1508.01991,\n",
      "\n",
      "[142] D. Andor, C. Alberti, D. Weiss, A. Severyn, A. Presta, K. Ganchev, S. Petrov, and M. Collins, “Globally normalized\n",
      "\n",
      "transition-based neural networks,” arXiv preprint arXiv:1603.06042, 2016.\n",
      "\n",
      "[143] D. Chen and C. D. Manning, “A fast and accurate dependency parser using neural networks.” in EMNLP, 2014, pp.\n",
      "\n",
      "1603, 2016.\n",
      "\n",
      "2015.\n",
      "\n",
      "740–750.\n",
      "\n",
      "[144] C. Dyer, M. Ballesteros, W. Ling, A. Matthews, and N. A. Smith, “Transition-based dependency parsing with stack long\n",
      "\n",
      "short-term memory,” arXiv preprint arXiv:1505.08075, 2015.\n",
      "\n",
      "[145] H. Zhou, Y. Zhang, C. Cheng, S. Huang, X. Dai, and J. Chen, “A neural probabilistic structured-prediction method for\n",
      "\n",
      "transition-based natural language processing,” Journal of Artiﬁcial Intelligence Research, vol. 58, pp. 703–729, 2017.\n",
      "\n",
      "[146] M. Zhu, Y. Zhang, W. Chen, M. Zhang, and J. Zhu, “Fast and accurate shift-reduce constituent parsing.” in ACL (1),\n",
      "\n",
      "[147] D. Weiss, C. Alberti, M. Collins, and S. Petrov, “Structured training for neural network transition-based parsing,” arXiv\n",
      "\n",
      "2013, pp. 434–443.\n",
      "\n",
      "preprint arXiv:1506.06158, 2015.\n",
      "\n",
      "[148] S. Petrov, L. Barrett, R. Thibaux, and D. Klein, “Learning accurate, compact, and interpretable tree annotation,” in\n",
      "Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the\n",
      "Association for Computational Linguistics. Association for Computational Linguistics, 2006, pp. 433–440.\n",
      "\n",
      "[149] A. Passos, V. Kumar, and A. McCallum, “Lexicon infused phrase embeddings for named entity resolution,” arXiv preprint\n",
      "\n",
      "arXiv:1404.5367, 2014.\n",
      "\n",
      "2015.\n",
      "\n",
      "[150] J. P. Chiu and E. Nichols, “Named entity recognition with bidirectional lstm-cnns,” arXiv preprint arXiv:1511.08308,\n",
      "\n",
      "[151] G. Luo, X. Huang, C.-Y. Lin, and Z. Nie, “Joint named entity recognition and disambiguation,” in Proc. EMNLP, 2015,\n",
      "\n",
      "\f",
      "31\n",
      "\n",
      "pp. 879–880.\n",
      "\n",
      "pp. 1127–1137.\n",
      "\n",
      "[152] E. Strubell, P. Verga, D. Belanger, and A. McCallum, “Fast and accurate sequence labeling with iterated dilated\n",
      "\n",
      "convolutions,” arXiv preprint arXiv:1702.02098, 2017.\n",
      "\n",
      "[153] J. Zhou and W. Xu, “End-to-end learning of semantic role labeling using recurrent neural networks.” in ACL (1), 2015,\n",
      "\n",
      "[154] O. T¨ackstr¨om, K. Ganchev, and D. Das, “Efﬁcient inference and structured learning for semantic role labeling,”\n",
      "\n",
      "Transactions of the Association for Computational Linguistics, vol. 3, pp. 29–41, 2015.\n",
      "\n",
      "[155] L. He, K. Lee, M. Lewis, and L. Zettlemoyer, “Deep semantic role labeling: What works and what’s next,” in Proceedings\n",
      "\n",
      "of the Annual Meeting of the Association for Computational Linguistics, 2017.\n",
      "\n",
      "[156] R. K. Srivastava, K. Greff, and J. Schmidhuber, “Training very deep networks,” in Advances in neural information\n",
      "\n",
      "processing systems, 2015, pp. 2377–2385.\n",
      "\n",
      "[157] B. Pang and L. Lee, “Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating\n",
      "scales,” in Proceedings of the 43rd annual meeting on association for computational linguistics. Association for\n",
      "Computational Linguistics, 2005, pp. 115–124.\n",
      "\n",
      "[158] A. Zadeh, R. Zellers, E. Pincus, and L.-P. Morency, “Multimodal sentiment intensity analysis in videos: Facial gestures\n",
      "\n",
      "and verbal messages,” IEEE Intelligent Systems, vol. 31, no. 6, pp. 82–88, 2016.\n",
      "\n",
      "[159] L.-C. Yu, J. Wang, K. R. Lai, and X. Zhang, “Reﬁning word embeddings for sentiment analysis,” in Proceedings of the\n",
      "\n",
      "2017 Conference on Empirical Methods in Natural Language Processing, 2017, pp. 545–550.\n",
      "\n",
      "[160] Q. Le and T. Mikolov, “Distributed representations of sentences and documents,” in Proceedings of the 31st International\n",
      "\n",
      "Conference on Machine Learning (ICML-14), 2014, pp. 1188–1196.\n",
      "\n",
      "[161] P. Koehn, F. J. Och, and D. Marcu, “Statistical phrase-based translation,” in Proceedings of the 2003 Conference of the\n",
      "North American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1.\n",
      "Association for Computational Linguistics, 2003, pp. 48–54.\n",
      "\n",
      "[162] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey et al.,\n",
      "“Google’s neural machine translation system: Bridging the gap between human and machine translation,” arXiv preprint\n",
      "arXiv:1609.08144, 2016.\n",
      "\n",
      "[163] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin, “Convolutional sequence to sequence learning,” arXiv\n",
      "\n",
      "[164] A. Fader, L. S. Zettlemoyer, and O. Etzioni, “Paraphrase-driven learning for open question answering.” in ACL (1), 2013,\n",
      "\n",
      "preprint arXiv:1705.03122, 2017.\n",
      "\n",
      "pp. 1608–1618.\n",
      "\n",
      "[165] J. Weston, A. Bordes, S. Chopra, A. M. Rush, B. van Merri¨enboer, A. Joulin, and T. Mikolov, “Towards ai-complete\n",
      "\n",
      "question answering: A set of prerequisite toy tasks,” arXiv preprint arXiv:1502.05698, 2015.\n",
      "\n",
      "[166] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, “Squad: 100,000+ questions for machine comprehension of text,”\n",
      "\n",
      "arXiv preprint arXiv:1606.05250, 2016.\n",
      "\n",
      "[167] A. Bordes, J. Weston, and N. Usunier, “Open question answering with weakly supervised embedding models,” in Joint\n",
      "\n",
      "European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, 2014, pp. 165–180.\n",
      "\n",
      "[168] D. Chen, A. Fisch, J. Weston, and A. Bordes, “Reading wikipedia to answer open-domain questions,” arXiv preprint\n",
      "\n",
      "arXiv:1704.00051, 2017.\n",
      "\n",
      "[169] Y. Shen, P.-S. Huang, J. Gao, and W. Chen, “Reasonet: Learning to stop reading in machine comprehension,” in\n",
      "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM,\n",
      "2017, pp. 1047–1055.\n",
      "\n",
      "[170] A. W. Yu, D. Dohan, M.-T. Luong, R. Zhao, K. Chen, M. Norouzi, and Q. V. Le, “Qanet: Combining local convolution\n",
      "\n",
      "with global self-attention for reading comprehension,” arXiv preprint arXiv:1804.09541, 2018.\n",
      "\n",
      "[171] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for automatic evaluation of machine translation,” in\n",
      "Proceedings of the 40th annual meeting on association for computational linguistics. Association for Computational\n",
      "Linguistics, 2002, pp. 311–318.\n",
      "\n",
      "[172] A. Ritter, C. Cherry, and W. B. Dolan, “Data-driven response generation in social media,” in Proceedings of the conference\n",
      "on empirical methods in natural language processing. Association for Computational Linguistics, 2011, pp. 583–593.\n",
      "[173] A. Sordoni, M. Galley, M. Auli, C. Brockett, Y. Ji, M. Mitchell, J.-Y. Nie, J. Gao, and B. Dolan, “A neural network\n",
      "\n",
      "approach to context-sensitive generation of conversational responses,” arXiv preprint arXiv:1506.06714, 2015.\n",
      "\n",
      "[174] J. Li, M. Galley, C. Brockett, J. Gao, and B. Dolan, “A diversity-promoting objective function for neural conversation\n",
      "\n",
      "[175] X. Zhou, D. Dong, H. Wu, S. Zhao, D. Yu, H. Tian, X. Liu, and R. Yan, “Multi-view response selection for human-\n",
      "\n",
      "models,” arXiv preprint arXiv:1510.03055, 2015.\n",
      "\n",
      "computer conversation.” in EMNLP, 2016, pp. 372–381.\n",
      "\n",
      "[176] I. V. Serban, A. Sordoni, Y. Bengio, A. C. Courville, and J. Pineau, “Building end-to-end dialogue systems using\n",
      "\n",
      "generative hierarchical neural network models.” in AAAI, 2016, pp. 3776–3784.\n",
      "\n",
      "[177] J. Dodge, A. Gane, X. Zhang, A. Bordes, S. Chopra, A. Miller, A. Szlam, and J. Weston, “Evaluating prerequisite\n",
      "\n",
      "qualities for learning end-to-end dialog systems,” arXiv preprint arXiv:1511.06931, 2015.\n",
      "\n",
      "\f",
      "32\n",
      "\n",
      "[178] S. R. Bowman, G. Angeli, C. Potts, and C. D. Manning, “A large annotated corpus for learning natural language\n",
      "\n",
      "inference,” arXiv preprint arXiv:1508.05326, 2015.\n",
      "\n",
      "[179] S. Pradhan, A. Moschitti, N. Xue, O. Uryupina, and Y. Zhang, “Conll-2012 shared task: Modeling multilingual\n",
      "unrestricted coreference in ontonotes,” in Joint Conference on EMNLP and CoNLL-Shared Task. Association for\n",
      "Computational Linguistics, 2012, pp. 1–40.\n",
      "\n",
      "[180] E. F. Tjong Kim Sang and F. De Meulder, “Introduction to the conll-2003 shared task: Language-independent named\n",
      "entity recognition,” in Proceedings of the seventh conference on Natural language learning at HLT-NAACL 2003-Volume\n",
      "4. Association for Computational Linguistics, 2003, pp. 142–147.\n",
      "\n",
      "[181] X. Liu, Y. Shen, K. Duh, and G. Jian-feng, “Stochastic answer networks for machine reading comprehension,” arXiv\n",
      "\n",
      "preprint arXiv:1712.03556, 2017.\n",
      "\n",
      "[182] C. Qian, Z. Xiao-Dan, L. Zhen-Hua, W. Si, J. Hui, and I. Diana, “Enhanced lstm for natural language inference,” In\n",
      "\n",
      "[183] H. Luheng, L. Kenton, L. Mike, and S. Z. Luke, “Deep semantic role labeling: What works and whats next,” In ACL,\n",
      "\n",
      "[184] L. Kenton, H. Luheng, L. Mike, and S. Z. Luke, “End-to-end neural coreference resolution,” In EMNLP, 2017.\n",
      "[185] E. P. Matthew, A. Waleed, B. Chandra, and P. Russell, “Semi-supervised sequence tagging with bidirectional language\n",
      "\n",
      "[186] M. Bryan, B. James, X. Caiming, and S. Richard, “Learned in translation: Contextualized word vectors,” In NIPS 2017,\n",
      "\n",
      "ACL, 2017.\n",
      "\n",
      "2017.\n",
      "\n",
      "models,” In ACL, 2017.\n",
      "\n",
      "2017.\n",
      "\n",
      "[187] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, “Glue: A multi-task benchmark and analysis\n",
      "\n",
      "platform for natural language understanding,” arXiv preprint arXiv:1804.07461, 2018.\n",
      "\n",
      "[188] D. Cer, M. Diab, E. Agirre, I. Lopez-Gazpio, and L. Specia, “Semeval-2017 task 1: Semantic textual similarity-\n",
      "\n",
      "multilingual and cross-lingual focused evaluation,” arXiv preprint arXiv:1708.00055, 2017.\n",
      "\n",
      "[189] L. Bentivogli, P. Clark, I. Dagan, and D. Giampiccolo, “The ﬁfth pascal recognizing textual entailment challenge.” in\n",
      "\n",
      "[190] T. Baltruˇsaitis, C. Ahuja, and L.-P. Morency, “Multimodal machine learning: A survey and taxonomy,” arXiv preprint\n",
      "\n",
      "TAC, 2009.\n",
      "\n",
      "arXiv:1705.09406, 2017.\n",
      "\n",
      "\f",
      "\n"
     ]
    }
   ],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "\n",
    "def convert_pdf_to_txt(path):\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    fp = open(path, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = \"\"\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos=set()\n",
    "\n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password=password,caching=caching, check_extractable=True):\n",
    "        interpreter.process_page(page)\n",
    "\n",
    "    text = retstr.getvalue()\n",
    "\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text\n",
    "\n",
    "f = 'C:/Users/svenkata/Dropbox/AI_for_Selection/Literature/1708.02709.pdf'\n",
    "print(convert_pdf_to_txt(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MESSAGE CURATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'C:/Users/svenkata/Desktop/Responsibility for leaflets passes to RISD - agreement[A912900.1].msg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H:/AI_for_Selection/a/20032008fp\\Am\\Cs\\Es\\L_5\\LNAS\\Responsibility for leaflets passes to RISD - agreement[A912900.1].msg\n",
      "H:/AI_for_Selection/a/20032008fp\\Am\\P_2\\PROCATc\\AP_2\\APC\\AHRC programme[A1011830.1].msg\n",
      "H:/AI_for_Selection/a/20032008fp\\Am\\P_2\\PROCATc\\AP_2\\APC\\conference[A1011828.1].msg\n",
      "H:/AI_for_Selection/a/20032008fp\\Am\\P_2\\PROCATc\\AP_2\\APC\\Flora ideas 806[A1011812.1].msg\n",
      "H:/AI_for_Selection/a/20032008fp\\Am\\P_2\\PROCATc\\AP_2\\APC\\Leeds 2007[A1011816.1].msg\n",
      "H:/AI_for_Selection/a/20032008fp\\Am\\P_2\\PROCATc\\AP_2\\APC\\Print leaflet[A961647.1].msg\n",
      "H:/AI_for_Selection/a/20032008fp\\Am\\P_2\\PROCATc\\AP_2\\APC\\RE_ conference[A1011817.1].msg\n",
      "H:/AI_for_Selection/a/20032008fp\\Am\\P_2\\PROCATc\\AP_2\\APC\\RE_ conference[A1011827.1].msg\n",
      "H:/AI_for_Selection/a/20032008fp\\Am\\P_2\\PROCATc\\AP_2\\APC\\RE_ E179[A1011822.1].msg\n",
      "H:/AI_for_Selection/a/20032008fp\\Am\\P_2\\PROCATc\\AP_2\\APC\\RE_ IT support for 18 November[A1011820.1].msg\n"
     ]
    }
   ],
   "source": [
    "#Just print 10 msg files\n",
    "\n",
    "folder_path_a = 'H:/AI_for_Selection/a/'\n",
    "count = 0\n",
    "for subdir, dirnames, files  in os.walk(folder_path_a):        \n",
    "        for filename in files :\n",
    "           \n",
    "            extension = filename.split('.')[-1].lower()\n",
    "            if (extension == 'msg' and count<10):\n",
    "                print(subdir+os.sep+filename)\n",
    "                count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sender: wmo1@york.ac.uk\u0000\n",
      "Sent On: Wed, 12 Jul 2006 13:06:37 +0100\n",
      "Subject: AHRC programme\u0000\n",
      "Body: \n",
      "Dear David\n",
      "\n",
      "Nice to see you yesterday.\n",
      "\n",
      "I take it that TNA is going to be bidding for the AHRC Strategic Resource\n",
      "Enhancement Programe?\n",
      "http://www.ahrc.ac.uk/apply/research/sfi/ahrcsi/strategic_resource_enhanceme\n",
      "nt_programme.asp\n",
      "\n",
      "I'm not clear whether TNA would be able to bid here in its own right or need\n",
      "an HEI partner: if the latter, perhaps you could indicate to the relevant\n",
      "authorities our willingness at York to be involved in planning?\n",
      "\n",
      "All a great rush, of course, as usual.\n",
      "\n",
      "Many thanks\n",
      "\n",
      "Mark\n",
      "\n",
      "\n",
      "______________________________________________________________________\n",
      "This email has been scanned by the MessageLabs Email Security System.\n",
      "For more information please visit http://www.messagelabs.com/email \n",
      "______________________________________________________________________\n",
      "\n",
      "\u0000\n"
     ]
    }
   ],
   "source": [
    "# This is a single message\n",
    "import extract_msg\n",
    "\n",
    "f = 'H:/AI_for_Selection/a/20032008fp\\Am\\P_2\\PROCATc\\AP_2\\APC\\AHRC programme[A1011830.1].msg'  \n",
    "#f = 'H:/AI_for_Selection/a/20032008fp\\Am\\Cs\\Es\\L_5\\LNAS\\Responsibility for leaflets passes to RISD - agreement[A912900.1].msg'\n",
    "#f = 'H:/AI_for_Selection/a/20032008fp\\Am\\P_2\\PROCATc\\AP_2\\APC\\Flora ideas 806[A1011812.1].msg'\n",
    "msg = extract_msg.Message(f)\n",
    "msg_sender = msg.sender\n",
    "msg_date = msg.date\n",
    "msg_subj = msg.subject\n",
    "msg_message = msg.body\n",
    "\n",
    "print('Sender: {}'.format(msg_sender))\n",
    "print('Sent On: {}'.format(msg_date))\n",
    "print('Subject: {}'.format(msg_subj))\n",
    "print('Body: {}'.format(msg_message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      ">\t\t\n",
      "\u000b",
      "\f",
      "\n",
      " !\"%#$'&()*+,.-/0Root Entry\u000b",
      "\n",
      "F10__nameid_version1.0(11__substg1.0_00020102*__substg1.0_00030102*H %(*-0689:;<=>?@ABCDEFGHJKLMNOPQRSTUVWXYZ\\]^_`abcdefghijklmnopqrstuvwxyz{} FTRR__substg1.0_00040102*__substg1.0_10010102*__substg1.0_10020102*__substg1.0_10090102*\t__substg1.0_100A0102*__substg1.0_100F0102*\u000b",
      "__substg1.0_10110102*__substg1.0_10120102*\n",
      "\f",
      "\tTIPM.Notemeeting 24.08.06EX:/O=PUBLIC RECORD OFFICE/OU=PUBLIC_RECORD/CN=RECIPIENTS/CN=FCROWHURST__substg1.0_101E0102*\n",
      "__substg1.0_001A001E*\u000b",
      "\t__substg1.0_0037001E*\f",
      "__substg1.0_003B0102*\n",
      "\n",
      "H__substg1.0_003D001E*__substg1.0_003F0102*]__substg1.0_0040001E*\n",
      "__substg1.0_00410102*a@B+//O=PUBLIC RECORD OFFICE/OU=PUBLIC_RECORD/CN=RECIPIENTS/CN=DCROOKCrook, David@B+//O=PUBLIC RECORD OFFICE/OU=PUBLIC_RECORD/CN=RECIPIENTS/CN=FCROWHURSTRayner, Flora@B+//O=PUBLIC RECORD OFFICE/OU=PUBLIC_RECORD/CN=RECIPIENTS/CN=DCROOK__substg1.0_0042001E*__substg1.0_00430102*]__substg1.0_0044001E*\n",
      "__substg1.0_00470102*<Crook, Davidc=GB;a= ;p=Public Record Of;l=NA-EXCH1-060824160755Z-160148EX:/O=PUBLIC RECORD OFFICE/OU=PUBLIC_RECORD/CN=RECIPIENTS/CN=DCROOKEX:/O=PUBLIC RECORD OFFICE/OU=PUBLIC_RECORD/CN=RECIPIENTS/CN=DCROOKEX/O=PUBLIC RECORD OFFICE/OU=PUBLIC_RECORD/CN=RECIPIENTS/CN=FCROWH__substg1.0_00510102*D__substg1.0_00520102*D__substg1.0_0064001E*__substg1.0_0065001E*#EURSTmeeting 24.08.06u]rG\\\n",
      "EX/O=PUBLIC RECORD OFFICE/OU=PUBLIC_RECORD/CN=RECIPIENTS/CN=DCROOKEX/O=PUBLIC RECORD OFFICE/OU=PUBLIC_RECORD/CN=RECIPIENTS/CN=DCROOK__substg1.0_0070001E*!__substg1.0_00710102*\"__substg1.0_0075001E*#__substg1.0_0076001E*!$A__substg1.0_0077001E*&__substg1.0_0078001E* \"'A__substg1.0_0C190102*)a__substg1.0_0C1A001E*++@B+//O=PUBLIC RECORD OFFICE/OU=PUBLIC_RECORD/CN=RECIPIENTS/CN=FCROWHURSTRayner, FloraEX:/O=PUBLIC RECORD OFFICE/OU=PUBLIC_RECORD/CN=RECIPIENTS/CN=FCROWHURSTEX/O=PUBLIC RECORD OFFICE/OU=PUBLIC_RECORD/CN=RECIPIENTS/CN=FCROWH__substg1.0_0C1D0102*,H__substg1.0_0C1E001E*$&.__substg1.0_0C1F001E*/E__substg1.0_0E02001E*%)1URSTCrook, David; Taylor, Nigelmeeting 24.08.06/0L~p~Dear Nigel and David\n",
      "Here are some brief notes from the meeting__substg1.0_0E03001E*2__substg1.0_0E04001E*(*3__substg1.0_0E1D001E*4__substg1.0_0E270102*'/5p__substg1.0_1000001E*7Y__substg1.0_10090102*,.I`__substg1.0_10130102*[7__substg1.0_1035001E*-3|A today:\n",
      "*\tBookings at 6\n",
      "*\tIt was agreed OCS suggested menu is fine, although will go for dessert option of half fruit / half mini danish pastries. Action: FR to confirm menu with OCS.\n",
      "*\tNT has arranged with Katie for inclusion in the next What's On.\n",
      "*\tDC will arrange for AHRB logo if it is to be used on programme.\n",
      "*\tIT - it was agreed that lap-tops would be difficult to set up and change over in the time available, and may not be allowed. DC will confirm with Matt what can be used (i.e. CD, email, memory stick) and then let the speakers know. Action: DC\n",
      "*\tNT/DC will let FR know if slide projector or OHP are required so it can be booked in advance.\n",
      "*\tDC is dealing with the enquiry from a publishers about the conference.\n",
      "*\tNext meeting - week beginning 16 October.\n",
      "\n",
      "David - I've checked and I'm sorry the email you sent me this morning hasn't arrived - please could you send again?\n",
      "Thanks, \n",
      "Flora\n",
      "\n",
      "\tFlora Rayner\n",
      "\tEvents and Exhibitions Co-ordinator\n",
      "\tThe National Archives\n",
      "\t020 8392 5272\n",
      "\twww.nationalarchives.gov.uk \n",
      "\n",
      "\n",
      "\\MLZFux2\n",
      "rcpg125&2\u000b",
      "`ng57Och\n",
      "set0 P]2}\n",
      "vIwk\u000b",
      "d4\f",
      "`cP\u000b",
      "\u000b",
      "4 De\n",
      "Nige pd^a\u000b",
      "1\n",
      "\n",
      "He[0Qsp`bf0 notR th`0\u000b",
      "gpay:\n",
      "p0ext'B7\f",
      "*lvl``A1\t@P\f",
      "0i-38@li1 Boo!g a@6I@wa A\t`OCsugs`nu \u000b",
      "e,0h`gh\"` go_@o0io &PlPPui@/'Dm\u000b",
      "i& ?p%P\n",
      "#s.0b`c&: FnR)P @ir$ #. NT'A Arr{\"+K `%\u000b",
      "c\n",
      "@&\u000b",
      "$P Wt' On,9DC%d.%AHRB \t%$@@{$Q*b`0P\"&p`\tam2!.0>-5\"y  a\u000b",
      "`p-p w`l`6Qdc;p@*!6pp34rov0\u000b",
      "p\u000b",
      "``${Qyb6B@\tw\t)3+&+M `\"Q:c6V(i.7 CD$e$;`r?#\n",
      "k)?3!?P<a1spk kpw)3*7-/3FB*GR5sp6jOHPs\tpq'\tq5C\"0ad>0 73\f",
      "$Q@+\"\tM?a) u`(G`Fd+\"AO-\\12f9p@ek6Ag\u000b",
      "s16#L6PrV\n",
      "9aI'==QO2B[.?RDr y`<0?Q$QDYS.Qn'@.i=[\u000b",
      "P +;r^5\\g\u000b",
      "q?T=qks$F\taHZq6H`**pc$rE=0 AQEx_b'& P-;n `b`N/ @a`a0 839@zwl.i!jC\n",
      "j.%v.uXFeH_\u000b",
      "1endfq<!DOCTYPE HTML PUBLIC \"-//W3C//DTD HTML 3.2//EN\">\n",
      "<HTML>\n",
      "<HEAD>\n",
      "<META HTTP-EQUIV=\"Content-Type\" CONTENT=\"text/html; charset=iso-8859-1\">\n",
      "<META NAME=\"Generator\" CONTENT=\"MS Exchange Server version 6.5.6944.0\">\n",
      "<TITLE>meeting 24.08.06</TITLE>\n",
      "</HEAD>\n",
      "<BODY>\n",
      "<!-- Converted from text/rtf format -->\n",
      "\n",
      "<P><FONT FACE=\"Arial\">Dear Nigel and David</FONT>\n",
      "\n",
      "<BR><FONT FACE=\"Arial\">Here are some brief notes from the meeting today:</FONT>\n",
      "\n",
      "<UL>\n",
      "<LI><FONT FACE=\"Arial\">Bookings at 6</FONT></LI>\n",
      "\n",
      "<LI><FONT FACE=\"Arial\">It was agreed OCS suggested menu is fine, although will go for dessert option of half fruit / half mini danish pastries.<B> Action: FR</B> to confirm menu with OCS.</FONT></LI>\n",
      "\n",
      "<LI><FONT FACE=\"Arial\">NT has arranged with Katie for inclusion in the next What's On.</FONT></LI>\n",
      "\n",
      "<LI><FONT FACE=\"Arial\">DC will arrange for AHRB logo if it is to be used on programme.</FONT></LI>\n",
      "\n",
      "<LI><FONT FACE=\"Arial\">IT - it was agreed that lap-tops would be difficult to set up and change over in the time available, and may not be allowed. DC will confirm with Matt what can be used (i.e. CD, email, memory stick) and then let the speakers know.</FONT><B> <FONT FACE=\"Arial\">Action: DC</FONT></B></LI>\n",
      "\n",
      "<LI><FONT FACE=\"Arial\">NT/DC will let FR know if slide projector or OHP are required so it can be booked in advance.</FONT></LI>\n",
      "\n",
      "<LI><FONT FACE=\"Arial\">DC is dealing with the enquiry from a publishers about the conference.</FONT></LI>\n",
      "\n",
      "<LI><FONT FACE=\"Arial\">Next meeting - week beginning 16 October.</FONT></LI>\n",
      "<BR>\n",
      "</UL>\n",
      "<P><FONT FACE=\"Arial\">David - I've checked and I'm sorry the email you sent me this morning hasn't arrived - please could you send again?</FONT>\n",
      "\n",
      "<BR><FONT FACE=\"Arial\">Thanks, </FONT>\n",
      "\n",
      "<BR><FONT FACE=\"Arial\">Flora</FONT>\n",
      "</P>\n",
      "<UL>\n",
      "<P><FONT FACE=\"Arial\">Flora Rayner</FONT>\n",
      "\n",
      "<BR><FONT FACE=\"Arial\">Events and Exhibitions Co-ordinator</FONT>\n",
      "\n",
      "<BR><FONT FACE=\"Arial\">The National Archives</FONT>\n",
      "\n",
      "<BR><FONT FACE=\"Arial\">020 8392 5272</FONT>\n",
      "\n",
      "<BR><FONT FACE=\"Arial\">www.nationalarchives.gov.uk </FONT>\n",
      "</P>\n",
      "<BR>\n",
      "</UL>\n",
      "</BODY>\n",
      "</HTML><88A6AFA61447AC4AB9F280FC6747F908035971EA@na-exch1.in.tna.local>/pxIUCrowhurst, Flora__substg1.0_300B0102*~__substg1.0_3FF8001E*02__substg1.0_3FF90102*a__substg1.0_3FFA001E*15@B+//O=PUBLIC RECORD OFFICE/OU=PUBLIC_RECORD/CN=RECIPIENTS/CN=FCROWHURSTRayner, Flora@B+//O=PUBLIC RECORD OFFICE/OU=PUBLIC_RECORD/CN=RECIPIENTS/CN=FCROWHURST9.0@01@01\\a:\\Tw__substg1.0_3FFB0102*a__substg1.0_8002001E*47__properties_version1.00__recip_version1.0_#00000000:6D;11\u000b",
      "\t\u000b",
      "#&\u000b",
      ")67@9u;H=?]@\n",
      "AaBC]D\n",
      "G<QDRDdeEpquvAwxA\f",
      "a\f",
      "\f",
      "H\f",
      "\f",
      "E\u000b",
      "\u000b",
      "@u\u000b",
      "|#'py\u000b",
      "Y\t`75A\u000b",
      "\u000b",
      "\u000b",
      "\u000b",
      "\u000b",
      "0?o?\t??a??a@@@@v@\tY@B+//O=PUBLIC RECORD OFFICE/OU=PUBLIC_RECORD/CN=RECIPIENTS/CN=DcrookCrook, DavidEX/O=PUBLIC RECORD OFFICE/OU=PUBLIC_RECORD/CN=RECIPIENTS/CN=Dcrook__substg1.0_0FF60102*__substg1.0_0FFF0102*8:]__substg1.0_3001001E*\n",
      "__substg1.0_3002001E*9?__substg1.0_3003001E*A__substg1.0_300B0102*<>D__substg1.0_39FE001E*$__substg1.0_39FF001E*=AEX:/O=PUBLIC RECORD OFFICE/OU=PUBLIC_RECORD/CN=RECIPIENTS/CN=DCROOKDavid.Crook@nationalarchives.gov.ukdcrookCrook, DavidCrook, David:hQNTNY/__substg1.0_3A20001E*\n",
      "__substg1.0_5FF6001E*@B\n",
      "__substg1.0_5FF70102*C __properties_version1.00H00A0\n",
      "\f",
      "]\u000b",
      "0D :\n",
      "\u000b",
      "\u000b",
      "@:999$q:___\n",
      "_ 0@B+//O=PUBLIC RECORD OFFICE/OU=PUBLIC_RE__recip_version1.0_#00000001:H11__substg1.0_0FF60102*__substg1.0_0FFF0102*EG^__substg1.0_3001001E*CORD/CN=RECIPIENTS/CN=NtaylorTaylor, NigelEX/O=PUBLIC RECORD OFFICE/OU=PUBLIC_RECORD/CN=RECIPIENTS/CN=NtaylorEX:/O=PUBLIC RECORD OFFICE/OU=PUBLIC_RECORD/CN=RECIPIENTS/CN=NTAYLORNigel.Taylor@nationalarchives.gov.uk__substg1.0_3002001E*FL__substg1.0_3003001E*B__substg1.0_300B0102*IKE__substg1.0_39FE001E*%__substg1.0_39FF001E*JN__substg1.0_3A20001E*__substg1.0_5FF6001E*MO__substg1.0_5FF70102*P NTaylorTaylor, NigelTaylor, Nigel:hQNTN000B0\f",
      "^\u000b",
      "0E :\u000b",
      "\u000b",
      "@:999%q:___properties_version1.00H___ 0\n"
     ]
    }
   ],
   "source": [
    "import email\n",
    "import string\n",
    "\n",
    "from email.parser import HeaderParser\n",
    "#F = 'H:/AI_for_Selection/a/20032008fp\\Am\\Cs\\Es\\L_5\\LNAS\\Responsibility for leaflets passes to RISD - agreement[A912900.1].msg'\n",
    "F = 'H:/AI_for_Selection/a/20032008fp\\Am\\P_2\\PROCATc\\AP_2\\APC\\AHRC programme[A1011830.1].msg'  \n",
    "F =  'H:/AI_for_Selection/a/20032008fp\\Am\\P_2\\PROCATc\\AP_2\\APC\\Flora ideas 806[A1011812.1].msg'\n",
    "with open(F,\"r\",encoding=\"ISO-8859-1\") as f:\n",
    "    msg=email.message_from_file(f)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print('message',msg.as_string())\n",
    "    parser = HeaderParser()\n",
    "    header = parser.parsestr(msg.as_string())\n",
    "    \n",
    "    print (header.keys())      \n",
    "\n",
    "   # The following snippet gives error\n",
    "    msgBody=msg.get_payload()\n",
    "    printable = set(string.printable)\n",
    "    msgBody = ''.join(filter(lambda x: x in printable, msgBody))\n",
    "    \n",
    "    #print(msgBody)\n",
    "    \n",
    "    a = msgBody\n",
    "    b = email.message_from_string(a)\n",
    "    if b.is_multipart():\n",
    "        for payload in b.get_payload():\n",
    "            # if payload.is_multipart(): ...\n",
    "            print( payload.get_payload())\n",
    "    else:\n",
    "        print (b.get_payload())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mailparser\n",
    "F = 'H:/AI_for_Selection/a/20032008fp\\Am\\Cs\\Es\\L_5\\LNAS\\Responsibility for leaflets passes to RISD - agreement[A912900.1].msg'\n",
    "#F = 'H:/AI_for_Selection/a/20032008fp\\Am\\P_2\\PROCATc\\AP_2\\APC\\AHRC programme[A1011830.1].msg'  \n",
    "#F =  'H:/AI_for_Selection/a/20032008fp\\Am\\P_2\\PROCATc\\AP_2\\APC\\Flora ideas 806[A1011812.1].msg'\n",
    "#mail = mailparser.parse_from_file(F)\n",
    "#mail = mailparser.parse_from_file_obj(F)\n",
    "mail = mailparser.parse_from_string(F) # raw_mail\n",
    "#mail = mailparser.parse_from_bytes(byte_mail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(mail.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
