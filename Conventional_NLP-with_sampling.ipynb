{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\svenkata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\svenkata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import spacy\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_colwidth',None) # to view the whole width of the cells in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites_txt = 'C:/Users/svenkata/Documents/websites_txt/'\n",
    "a_txt = 'C:/Users/svenkata/Documents/a_txt/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Conventional algorithms are often biased towards the majority class, not taking the data distribution into consideration*\n",
    "\n",
    "*Many times, the minority classes are treated as outliers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    stop_free = ' '.join([i for i in doc if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalised = ''.join(lemma.lemmatize(word) for word in punc_free)\n",
    "    \n",
    "    return normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "##I know this is very unprofessional but, string manipulation is very difficult otherwise!!\n",
    "def replace_chars(path):\n",
    "    to_replace = ['\\\\\\\\','\\\\']\n",
    "    string=''\n",
    "    for ch in path:\n",
    "        if ch in to_replace:\n",
    "            ch = '/'\n",
    "        elif ch == '\\v':\n",
    "            ch = '/v'\n",
    "        elif ch == '\\b':\n",
    "            ch = '/b'\n",
    "        elif ch == '\\t':\n",
    "            ch = '/t'\n",
    "        elif ch == '\\a':\n",
    "            ch = '/a'\n",
    "        elif ch == '\\f':\n",
    "            ch = '/f'\n",
    "        elif ch == '\\n':\n",
    "            ch = '/n'\n",
    "        elif ch == '\\r':\n",
    "            ch = '\\r'\n",
    "        elif ch == '\\h':\n",
    "            ch = '/h'\n",
    "        elif ch == '\\o':\n",
    "            ch = '/o'\n",
    "        string = string+ch\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import funcy as fp\n",
    "## Something quick and dirty......\n",
    "EMAIL_REGEX = re.compile(r'[a-z0-9\\.\\+_-]+@[a-z0-9\\.\\+_-]+\\.[a-z]*')\n",
    "FILTER_REGEX = re.compile(r\"[^a-z '#]\")\n",
    "TOKEN_MAPPINGS = [(EMAIL_REGEX,\"#email\"),(FILTER_REGEX,' ')]\n",
    "\n",
    "def tokenize_line(line):\n",
    "    res = line.lower()\n",
    "    for regexp, replacement in TOKEN_MAPPINGS:\n",
    "        res = regexp.sub(replacement,res)\n",
    "    return res.split()\n",
    "\n",
    "def tokenize(lines, token_size_filter = 2):\n",
    "    tokens = fp.mapcat(tokenize_line,lines)\n",
    "    return [t for t in tokens if len(t) > token_size_filter]\n",
    "   \n",
    "\n",
    "def load_doc(folder_path):\n",
    "    doc_list =[]\n",
    "    for subdir, dirnames, files  in os.walk(folder_path):         \n",
    "        for filename in files:             \n",
    "            if (filename.split('.')[-1] =='txt'):\n",
    "                #print(filename)\n",
    "                filepath = subdir+os.sep+filename\n",
    "                documentname = replace_chars(filepath)\n",
    "                try:\n",
    "                    with open(filepath, encoding='UTF-8', errors='ignore') as f:\n",
    "                        doc = f.readlines()                          \n",
    "                        tokens = tokenize(doc)\n",
    "                        tokens = clean(tokens)\n",
    "                        \n",
    "                        \n",
    "                    doc_list.append({'documentname': documentname, 'tokens':tokens  })\n",
    "                except :\n",
    "                    print(filepath)\n",
    "            else:\n",
    "                continue     \n",
    "             \n",
    "    doc_df = pd.DataFrame(doc_list)\n",
    "    \n",
    "    return doc_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/svenkata/Documents/websites_txt/Webteam\\blogs\\images from bloggers\\Martin Willis blog pics\\In reviewing the correspondence it is clear that the substance of your complaint regarding the wall boards is a continuation of the issues raised in your previous complaints.txt\n",
      "C:/Users/svenkata/Documents/websites_txt/Webteam\\Editorial\\FOI request files 2016-12-12\\Payments to he Advisory Panel on Public Sector Information [APPSI] and the Advisory Council on National Records and Archives [ACNRA] to the Confederation of British Industry and its subsidiaries.txt\n"
     ]
    }
   ],
   "source": [
    "docs_websites = load_doc(websites_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documentname</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:/Users/svenkata/Documents/websites_txt/Angel...</td>\n",
       "      <td>sheet archives sector pages broken links octob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:/Users/svenkata/Documents/websites_txt/Angel...</td>\n",
       "      <td>sheet cake contributors yes kind cake vegan gl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:/Users/svenkata/Documents/websites_txt/Angel...</td>\n",
       "      <td>https media nationalarchives gov index php yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:/Users/svenkata/Documents/websites_txt/Angel...</td>\n",
       "      <td>blog front end https blog nationalarchives gov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:/Users/svenkata/Documents/websites_txt/Angel...</td>\n",
       "      <td>https media nationalarchives gov index php col...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        documentname  \\\n",
       "0  C:/Users/svenkata/Documents/websites_txt/Angel...   \n",
       "1  C:/Users/svenkata/Documents/websites_txt/Angel...   \n",
       "2  C:/Users/svenkata/Documents/websites_txt/Angel...   \n",
       "3  C:/Users/svenkata/Documents/websites_txt/Angel...   \n",
       "4  C:/Users/svenkata/Documents/websites_txt/Angel...   \n",
       "\n",
       "                                              tokens  \n",
       "0  sheet archives sector pages broken links octob...  \n",
       "1  sheet cake contributors yes kind cake vegan gl...  \n",
       "2  https media nationalarchives gov index php yea...  \n",
       "3  blog front end https blog nationalarchives gov...  \n",
       "4  https media nationalarchives gov index php col...  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_websites.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consists of contents of websites files as tokens for developing NLP models\n",
    "docs_websites.to_pickle('H:/AI_for_Selection/websites_contents_NLP.pkl') # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/svenkata/Documents/a_txt/20032008fp\\RMDaILUa\\C_3\\DfID\\A_3\\PRAaD\\AISU note of decisions from meeting with Middle East Centre and British Empire and Commonwealth Museum forwarded to DFID for confirmation that proposed solution is acceptable[A121439.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/ASD\\Cio\\RSE\\PHC0042\\PHCIoSLO2015\\Portsmouth Library and Archive Service - Inspection of Southsea Outstore 24th February 2015  - Advisory Letter   25th January 2016 - Postponing appointment pending accreditation[A4185929.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/ASD\\CS\\AtA\\AtApm\\CaIPODPaICO\\DIGITAL MEETS CULTURE - source for digital copyright contacts_activities_ RE_ published Fwd_ _AN INVITATION_ UKAD Forum - Thursday 27 March 2014, The National Archives [UNCLASSIFIED][A3529985.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/ASD\\Rwgan\\Gb\\NLHFHLF\\CC_7\\Collecting Cultures - response to request for second meeting - separate strand for discussing archive purchasing behaviour_ RE_ Next Collecting Cultures working group meeting - July [UNCLASSIFIED][A3348051.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/CC\\CCDPC\\CCDPCM\\CCDPCMMTP\\CCDMDCFFrc201718\\RE_ TNA170224827_ Document condition feedback enquiry (Document condition feedback) - E 122_208 and TNA170224828_ Document condition feedback enquiry (Document condition feedback)[A4647752.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/CC\\CCDPC\\CCDPCM\\CCDPCMMTP\\CCDMDCFFrc201819\\Re_ Document condition feedback - Ref_ TNA1528103176O79 - BT 43_117,  Ref_ TNA1528102977G49 - BT 43_129, Ref_ TNA1528103118E12 - BT 43_141, Ref_ TNA1528103245V68 - BT 43_153[A4996157.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/CC\\CCDPC\\CCDPCM\\CCDPCMMTP\\CCDMDCFFrc201920\\FW_ Document condition feedback - Ref_ TNA1528103176O79 - BT 43_117,  Ref_ TNA1528102977G49 - BT 43_129, Ref_ TNA1528103118E12 - BT 43_141, Ref_ TNA1528103245V68 - BT 43_153.1[A5165691.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/CC\\CCDPC\\CCDPCM\\CCDPCMMTP\\CCDMDCFFrc201920\\FW_ Document condition feedback - Ref_ TNA1528103176O79 - BT 43_117,  Ref_ TNA1528102977G49 - BT 43_129, Ref_ TNA1528103118E12 - BT 43_141, Ref_ TNA1528103245V68 - BT 43_153.2[A5165693.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/GA\\PRBn\\NPRBCs\\DfE\\A2015a2016\\CfifDH\\20150821 LH seeking advice from Access at Transfer Manager reDfE_Ofsted - Files potentially relevant to The Independent Inquiry into Child Sexual Abuse  [OFFICIAL-SENSITIVE][A4212784.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/GA\\PRBn\\NPRBCs\\DfE\\P2015a2016\\IMA\\FW_ Letter from Jeff James, Chief Executive and Keeper of The National Archives, to Chris Wormald Permanent Secretary, Department for Education, re_ Information Management Assessment[A3917206.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/GA\\PRBn\\NPRBCs\\DfID\\TADTC20082014\\LMBDIDTNATNTC\\Listing checks DO 35 (10915-10921), DO 161 (494), DO 164 (137), DO 165 (175-178), CO 1017 (903-909), CO 822 (3294-3295), CO 1031 (5289), OD 115 (1-100) [UNCLASSIFIED][A3441869.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/GA\\PRBn\\NPRBCs\\DfID\\TADTC20082014\\LMBDIDTNATNTC\\RE_ Listing checks DO 35 (10915-10921), DO 161 (494), DO 164 (137), DO 165 (175-178), CO 1017 (903-909), CO 822 (3294-3295), CO 1031 (5289), OD 115 (1-100) [UNCLASSIFIED][A3441932.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/GA\\PRBn\\NPRBCs\\DfID\\TADTc2015a2016\\LMBDIDTNATNTCc\\Listing checks DO 35 (10915-10921), DO 161 (494), DO 164 (137), DO 165 (175-178), CO 1017 (903-909), CO 822 (3294-3295), CO 1031 (5289), OD 115 (1-100) [UNCLASSIFIED][A3906367.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/GA\\PRBn\\NPRBCs\\DfID\\TADTc2015a2016\\LMBDIDTNATNTCc\\RE_ Listing checks DO 35 (10915-10921), DO 161 (494), DO 164 (137), DO 165 (175-178), CO 1017 (903-909), CO 822 (3294-3295), CO 1031 (5289), OD 115 (1-100) [UNCLASSIFIED][A3906430.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/GA\\PRBn\\NPRBCs\\FC\\A2015a2016\\NYO\\20151119 FC initial response re NYO Action required by 24th November_ Guidance on records and information management on behalf of the Keeper of Public Records     [OFFICIAL-SENSITIVE][A4208631.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/GA\\PRBn\\NPRBCs\\FC\\A2015a2016\\NYO\\20151119 LH chasing FC re NYO_ Action required by 24th November_ Guidance on records and information management on behalf of the Keeper of Public Records     [OFFICIAL-SENSITIVE][A4208629.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/GA\\T\\TR\\DEFE\\DEFE20082016\\DEFE24\\Original E-transfer Form DEFE 24_1984-2093 various (transferring 1984_1-1985_1, 1987_1, 2005_1, 2043_1, 2061_1, 2073_1-2074_1, 2077_1-2080_1, 2083_1-2084_1, 2087_1-2090_1, 2093_1)[A2951999.3].txt\n",
      "C:/Users/svenkata/Documents/a_txt/GA\\T\\TR\\DEFE\\DEFE20082016\\DEFE24\\Original E-transfer Form DEFE 24_1986, 1997, 1999, 2018, 2021-2026, 2028, 2030, 2032, 2036- 2041, 2044, 2046, 2048-2050, 2052-2055, 2058-2059, 2062-2063, 2085, 2092 DIGITAL SURROGATES ONLY[A2405305.2].txt\n",
      "C:/Users/svenkata/Documents/a_txt/GA\\T\\TR\\PREM\\PREM20112018prtfnsatt\\PREM19\\Original E-transfer form PREM 19_1342, 1361, 1362, 1376, 1391, 1424, 1426, 1433, 1545, 1557, 1570-1572, 1581, 1587, 1588, 1646, 1660, 1647 extracts - previously retained[A4928313.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/GA\\T\\TR\\PREM\\PREM20112018prtfnsatt\\PREM19\\Original E-transfer Form PREM 19_691_1, 791_1, 916_1, 955_2, 1048_1, 1048_2, 1049_3, 1051_1, 1055_1, 1056_1, 1057_1,_2, 1059_1, 1087_2, 1097_1, 1105_1,_2, 1107_1, 1124_1 closed[A4928344.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/GA\\T\\TR\\T_3\\T20112018prtfnsatt\\T437_1\\Original e-Transfer Form T 437_383 384, 424, 425, 448-451, 454-458, 462, 463, 522-524, 547, 549, 553, 556, 558-560, 563, 565, 567, 569, 574, 576, 578-580 previously retained[A4927485.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/PPDaCM\\PD\\PC_1\\20YRaCS\\GE_1\\2012RTR\\_IGNORERTRDSfAS2012MP\\BISDECC The TNA Combined Statistical Return for 2012 for the Department for Business Innovation and Skills and the Department of Energy and Climate Chance[A3088320.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/WA\\HA\\201113Fo20MB\\20112013Fo20MB\\Fo20MB\\tg07-detailed-urban-characeter-analysis (http___webarchive.nationalarchives.gov.uk_20110118095356_http___www.cabe.org.uk_files_tg07-detailed-urban-characeter-analysis.txt)[A2623457.1].txt\n"
     ]
    }
   ],
   "source": [
    "docs_a = load_doc(a_txt)\n",
    "docs_a.to_pickle('H:/AI_for_Selection/a_contents_NLP.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read Preprocessed Data into Data frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_a_df = pd.read_pickle('H:/AI_for_Selection/a_contents_NLP.pkl')# This file consists of the contents of the files (structured data from Objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_meta_df = pd.read_pickle('H:/AI_for_Selection/metadata_with_filesize_lastmodified.pkl') # This file consists of processed metadata of structured data\n",
    "\n",
    "txt_extensions = ['pdf','rtf','txt','msg','doc','docx','xls', 'xlsx','mbx']\n",
    "\n",
    "a_meta_df=a_meta_df[a_meta_df.fileextension.isin(txt_extensions)]\n",
    "a_meta_df = a_meta_df[['fileextension', 'disposal_schedule', 'repository','file_path' ,'ret_schedule','file_size']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites_meta_df = pd.read_pickle('H:/AI_for_Selection/Websites_processed.pkl')# This file consists of metadata of processed websites data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_websites = pd.read_pickle('H:/AI_for_Selection/websites_contents_NLP.pkl') # This file consists of the contents of the files (unstructred X-drive data)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documentname</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:/Users/svenkata/Documents/websites_txt/Angel...</td>\n",
       "      <td>sheet archives sector pages broken links octob...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        documentname  \\\n",
       "0  C:/Users/svenkata/Documents/websites_txt/Angel...   \n",
       "\n",
       "                                              tokens  \n",
       "0  sheet archives sector pages broken links octob...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_websites.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge contents file with metadata file on the target column (retention schedule)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    C:/Users/svenkata/Documents/a_txt/20032008fp/Am/Cs/Es/L_5/LNAS/MDR 1 How to use the paper MDR[A905701.3].txt\n",
      "Name: documentname, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(docs_a_df[:1]['documentname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##function to extract file's absolute path and filename with no extension\n",
    "def extract_absolutePath_removeExtension(topfolderpath, filename):\n",
    "    filename = filename.replace(topfolderpath,'')\n",
    "    words = filename.split('.')\n",
    "    return '.'.join(words[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_a_df['abs_filepath'] = docs_a_df['documentname'].apply(lambda x: extract_absolutePath_removeExtension('C:/Users/svenkata/Documents/a_txt/',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                   documentname  \\\n",
      "0  C:/Users/svenkata/Documents/a_txt/20032008fp/Am/Cs/Es/L_5/LNAS/MDR 1 How to use the paper MDR[A905701.3].txt   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   tokens  \\\n",
      "0  mdr use manorial documents register mdr manorial documents register mdr mdr index manorial records provides brief descriptions documents details locations mdr mainly paper index details counties wales three ridings yorkshire hampshire isle wight norfolk surrey middlesex revised updated computerised searched online www mdr nationalarchives gov mdr see nra non computerised counties use paper indexes mdr paper indexes mdr held research enquiries room divided two parts parish index manor index parish index parish index identifies names manors associated parishes two always identical arranged county alphabetically parish name within county information contained slip parish name middle slip manor name located top right hand corner manors may known variety names records listed one name manor index slip two manor names top right hand corner name preceded see standard manor name may one slip associated parish giving several manor names research possible parish index may include reference manor records indexed manor index indicates manor existed records survived location remains unknown please aware parish include one manors manors cross parish boundaries parts manors exist separately main body manor different parishes counties manor name known manor index consulted manor index manor index arranged county alphabetically manor name within county manor index contain reference manors known existed records extant manor may mentioned index information contained slip manor name top right hand corner document description date middle slip details location document reference found bottom please aware details locations may date repository names may changed pro tna kent archive office centre kentish studies information contained reverse slip reverse slip often provides source details document overleaf see reference number beginning nra catalogue collection held research enquiries room users mdr researching particular topographical area may find helpful consult nra list may contain references records although strictly manorial nature therefore included mdr may nevertheless prove useful research information nra see nra reference annual return means information came tna accessions repositories survey see nra information accessions pages reference means mdr team information file regarding location records information please contact mdr team every document noted mdr available research always check relevant repository first making visit view document slip refers private owner please apply writing information terms conditions access   \n",
      "\n",
      "                                                             abs_filepath  \n",
      "0  20032008fp/Am/Cs/Es/L_5/LNAS/MDR 1 How to use the paper MDR[A905701.3]  \n"
     ]
    }
   ],
   "source": [
    "print(docs_a_df[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  fileextension                                           disposal_schedule  \\\n",
      "0           xls  24 Projects - Full Projects (Close file when Project ends)   \n",
      "\n",
      "           repository  \\\n",
      "0  Strategic Projects   \n",
      "\n",
      "                                                                                          file_path  \\\n",
      "0  PPDaCM/PD/PC_1/20YRaCS/GE_1/2012RTR/RTRS2012DS/The National Archives RTR 09-2012[A3109716.2].xls   \n",
      "\n",
      "  ret_schedule  file_size  \n",
      "0           24     225280  \n"
     ]
    }
   ],
   "source": [
    "# check how the file path is represented in structured file\n",
    "print(a_meta_df[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_meta_df['abs_filepath']  = a_meta_df['file_path'].apply(lambda x: extract_absolutePath_removeExtension('', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  fileextension                                           disposal_schedule  \\\n",
      "0           xls  24 Projects - Full Projects (Close file when Project ends)   \n",
      "\n",
      "           repository  \\\n",
      "0  Strategic Projects   \n",
      "\n",
      "                                                                                          file_path  \\\n",
      "0  PPDaCM/PD/PC_1/20YRaCS/GE_1/2012RTR/RTRS2012DS/The National Archives RTR 09-2012[A3109716.2].xls   \n",
      "\n",
      "  ret_schedule  file_size  \\\n",
      "0           24     225280   \n",
      "\n",
      "                                                                                   abs_filepath  \n",
      "0  PPDaCM/PD/PC_1/20YRaCS/GE_1/2012RTR/RTRS2012DS/The National Archives RTR 09-2012[A3109716.2]  \n"
     ]
    }
   ],
   "source": [
    "print(a_meta_df[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge doc contents with metadata on abs_filepath**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.merge(docs_a_df, a_meta_df, on='abs_filepath')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['documentname', 'tokens', 'abs_filepath', 'fileextension',\n",
      "       'disposal_schedule', 'repository', 'file_path', 'ret_schedule',\n",
      "       'file_size'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_pickle('H:/AI_for_Selection/a_cleaned_ready_for_NLP.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**add a numerical category_id to avoid string type ret_schedule**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documentname</th>\n",
       "      <th>tokens</th>\n",
       "      <th>abs_filepath</th>\n",
       "      <th>fileextension</th>\n",
       "      <th>disposal_schedule</th>\n",
       "      <th>repository</th>\n",
       "      <th>file_path</th>\n",
       "      <th>ret_schedule</th>\n",
       "      <th>file_size</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:/Users/svenkata/Documents/a_txt/20032008fp/Am/Cs/Es/L_5/LNAS/MDR 1 How to use the paper MDR[A905701.3].txt</td>\n",
       "      <td>mdr use manorial documents register mdr manorial documents register mdr mdr index manorial records provides brief descriptions documents details locations mdr mainly paper index details counties wales three ridings yorkshire hampshire isle wight norfolk surrey middlesex revised updated computerised searched online www mdr nationalarchives gov mdr see nra non computerised counties use paper indexes mdr paper indexes mdr held research enquiries room divided two parts parish index manor index parish index parish index identifies names manors associated parishes two always identical arranged county alphabetically parish name within county information contained slip parish name middle slip manor name located top right hand corner manors may known variety names records listed one name manor index slip two manor names top right hand corner name preceded see standard manor name may one slip associated parish giving several manor names research possible parish index may include reference manor records indexed manor index indicates manor existed records survived location remains unknown please aware parish include one manors manors cross parish boundaries parts manors exist separately main body manor different parishes counties manor name known manor index consulted manor index manor index arranged county alphabetically manor name within county manor index contain reference manors known existed records extant manor may mentioned index information contained slip manor name top right hand corner document description date middle slip details location document reference found bottom please aware details locations may date repository names may changed pro tna kent archive office centre kentish studies information contained reverse slip reverse slip often provides source details document overleaf see reference number beginning nra catalogue collection held research enquiries room users mdr researching particular topographical area may find helpful consult nra list may contain references records although strictly manorial nature therefore included mdr may nevertheless prove useful research information nra see nra reference annual return means information came tna accessions repositories survey see nra information accessions pages reference means mdr team information file regarding location records information please contact mdr team every document noted mdr available research always check relevant repository first making visit view document slip refers private owner please apply writing information terms conditions access</td>\n",
       "      <td>20032008fp/Am/Cs/Es/L_5/LNAS/MDR 1 How to use the paper MDR[A905701.3]</td>\n",
       "      <td>doc</td>\n",
       "      <td>33 Time Category Permanent</td>\n",
       "      <td>Historical Manuscripts Commission</td>\n",
       "      <td>20032008fp/Am/Cs/Es/L_5/LNAS/MDR 1 How to use the paper MDR[A905701.3].doc</td>\n",
       "      <td>33</td>\n",
       "      <td>225792</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                   documentname  \\\n",
       "0  C:/Users/svenkata/Documents/a_txt/20032008fp/Am/Cs/Es/L_5/LNAS/MDR 1 How to use the paper MDR[A905701.3].txt   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   tokens  \\\n",
       "0  mdr use manorial documents register mdr manorial documents register mdr mdr index manorial records provides brief descriptions documents details locations mdr mainly paper index details counties wales three ridings yorkshire hampshire isle wight norfolk surrey middlesex revised updated computerised searched online www mdr nationalarchives gov mdr see nra non computerised counties use paper indexes mdr paper indexes mdr held research enquiries room divided two parts parish index manor index parish index parish index identifies names manors associated parishes two always identical arranged county alphabetically parish name within county information contained slip parish name middle slip manor name located top right hand corner manors may known variety names records listed one name manor index slip two manor names top right hand corner name preceded see standard manor name may one slip associated parish giving several manor names research possible parish index may include reference manor records indexed manor index indicates manor existed records survived location remains unknown please aware parish include one manors manors cross parish boundaries parts manors exist separately main body manor different parishes counties manor name known manor index consulted manor index manor index arranged county alphabetically manor name within county manor index contain reference manors known existed records extant manor may mentioned index information contained slip manor name top right hand corner document description date middle slip details location document reference found bottom please aware details locations may date repository names may changed pro tna kent archive office centre kentish studies information contained reverse slip reverse slip often provides source details document overleaf see reference number beginning nra catalogue collection held research enquiries room users mdr researching particular topographical area may find helpful consult nra list may contain references records although strictly manorial nature therefore included mdr may nevertheless prove useful research information nra see nra reference annual return means information came tna accessions repositories survey see nra information accessions pages reference means mdr team information file regarding location records information please contact mdr team every document noted mdr available research always check relevant repository first making visit view document slip refers private owner please apply writing information terms conditions access   \n",
       "\n",
       "                                                             abs_filepath  \\\n",
       "0  20032008fp/Am/Cs/Es/L_5/LNAS/MDR 1 How to use the paper MDR[A905701.3]   \n",
       "\n",
       "  fileextension           disposal_schedule  \\\n",
       "0           doc  33 Time Category Permanent   \n",
       "\n",
       "                          repository  \\\n",
       "0  Historical Manuscripts Commission   \n",
       "\n",
       "                                                                    file_path  \\\n",
       "0  20032008fp/Am/Cs/Es/L_5/LNAS/MDR 1 How to use the paper MDR[A905701.3].doc   \n",
       "\n",
       "  ret_schedule  file_size  category_id  \n",
       "0           33     225792            0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['category_id'] = X['ret_schedule'].factorize()[0]\n",
    "category_id_df = X[['disposal_schedule','category_id']].drop_duplicates().sort_values('category_id')\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['category_id','disposal_schedule']].values)\n",
    "X[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X: 93824\n",
      "7     18741\n",
      "11    13110\n",
      "0     10786\n",
      "6      8973\n",
      "12     8047\n",
      "1      7100\n",
      "10     5115\n",
      "4      2762\n",
      "14     2713\n",
      "3      2686\n",
      "13     2679\n",
      "9      2573\n",
      "5      2298\n",
      "8      1909\n",
      "2      1612\n",
      "18     1486\n",
      "15      925\n",
      "16      250\n",
      "17       58\n",
      "19        1\n",
      "Name: category_id, dtype: int64\n",
      "24     18741\n",
      "02     13110\n",
      "33     10786\n",
      "05      8973\n",
      "04      8047\n",
      "23      7100\n",
      "03      5115\n",
      "20      2762\n",
      "11      2713\n",
      "21      2686\n",
      "07      2679\n",
      "27      2573\n",
      "32      2298\n",
      "28      1909\n",
      "16      1612\n",
      "24b     1486\n",
      "10       925\n",
      "25       250\n",
      "06        58\n",
      "24a        1\n",
      "Name: ret_schedule, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Length of X:',len(X))\n",
    "# just checking\n",
    "print(X.category_id.value_counts())\n",
    "print(X.ret_schedule.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8,6))\n",
    "X.groupby('ret_schedule').tokens.count().plot.bar(ylim=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Text docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just finding all those classes less than the average\n",
    "zz = pd.DataFrame(list(zip(X.ret_schedule.value_counts().index.tolist(),X.ret_schedule.value_counts()) ),columns=['ret_sc', 'value'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ret_sc</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24</td>\n",
       "      <td>18741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>02</td>\n",
       "      <td>13110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>10786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>05</td>\n",
       "      <td>8973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>04</td>\n",
       "      <td>8047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23</td>\n",
       "      <td>7100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>03</td>\n",
       "      <td>5115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20</td>\n",
       "      <td>2762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11</td>\n",
       "      <td>2713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>21</td>\n",
       "      <td>2686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>07</td>\n",
       "      <td>2679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>27</td>\n",
       "      <td>2573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>32</td>\n",
       "      <td>2298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>28</td>\n",
       "      <td>1909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>16</td>\n",
       "      <td>1612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>24b</td>\n",
       "      <td>1486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10</td>\n",
       "      <td>925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>25</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>06</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>24a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ret_sc  value\n",
       "0      24  18741\n",
       "1      02  13110\n",
       "2      33  10786\n",
       "3      05   8973\n",
       "4      04   8047\n",
       "5      23   7100\n",
       "6      03   5115\n",
       "7      20   2762\n",
       "8      11   2713\n",
       "9      21   2686\n",
       "10     07   2679\n",
       "11     27   2573\n",
       "12     32   2298\n",
       "13     28   1909\n",
       "14     16   1612\n",
       "15    24b   1486\n",
       "16     10    925\n",
       "17     25    250\n",
       "18     06     58\n",
       "19    24a      1"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zz ypred_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The classes are highly unbalanced. Upsampling classes to an average**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = round(sum(X.ret_schedule.value_counts())/X.ret_schedule.nunique()) #4691\n",
    "x_less_than_avg = zz[zz['value']<avg].ret_sc # list them\n",
    "X_more_than_avg = zz[zz['value']>=avg].ret_sc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_balanced = pd.DataFrame(columns=X.columns)\n",
    "#Up sampling minority classes\n",
    "for ret_sc in x_less_than_avg:\n",
    "    df = X[X.ret_schedule == ret_sc]\n",
    "    df = resample(df, \n",
    "                  replace=True,     # sample with replacement\n",
    "                  n_samples=avg,    # to match majority class\n",
    "                  random_state=123) # reproducible results\n",
    " \n",
    "    #Combine majority class with upsampled minority class\n",
    "    X_balanced = pd.concat([X_balanced, df])\n",
    "\n",
    "#down sampling majority classes\n",
    "for ret_sc in X_more_than_avg:\n",
    "    df = X[X.ret_schedule == ret_sc]\n",
    "    df = resample(df, \n",
    "                  #replace=True,     # sample with replacement\n",
    "                  n_samples=avg,    # to match majority class\n",
    "                  random_state=123) # reproducible results\n",
    " \n",
    "    #Combine majority class with upsampled minority class\n",
    "    X_balanced = pd.concat([X_balanced, df])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "05     4691\n",
       "16     4691\n",
       "24     4691\n",
       "07     4691\n",
       "10     4691\n",
       "32     4691\n",
       "23     4691\n",
       "06     4691\n",
       "21     4691\n",
       "27     4691\n",
       "25     4691\n",
       "03     4691\n",
       "04     4691\n",
       "02     4691\n",
       "11     4691\n",
       "33     4691\n",
       "20     4691\n",
       "28     4691\n",
       "24b    4691\n",
       "24a    4691\n",
       "Name: ret_schedule, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_balanced.ret_schedule.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X_balanced.tokens\n",
    "y = X_balanced.ret_schedule\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1,y,test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "accuracy 0.8174163291409081\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          02       0.70      0.66      0.68      1427\n",
      "          03       0.88      0.89      0.88      1390\n",
      "          04       0.92      0.81      0.86      1386\n",
      "          05       0.91      0.66      0.77      1400\n",
      "          06       0.91      1.00      0.95      1414\n",
      "          07       0.64      0.86      0.73      1367\n",
      "          10       0.85      0.78      0.81      1431\n",
      "          11       0.63      0.74      0.68      1379\n",
      "          16       0.99      0.97      0.98      1506\n",
      "          20       0.85      0.73      0.79      1402\n",
      "          21       0.64      0.86      0.74      1382\n",
      "          23       0.94      0.68      0.79      1400\n",
      "          24       0.97      0.61      0.75      1388\n",
      "         24a       0.99      1.00      0.99      1399\n",
      "         24b       0.90      0.98      0.94      1439\n",
      "          25       0.92      0.92      0.92      1397\n",
      "          27       0.94      0.79      0.86      1398\n",
      "          28       0.93      0.68      0.79      1401\n",
      "          32       0.57      0.88      0.69      1393\n",
      "          33       0.72      0.82      0.77      1447\n",
      "\n",
      "    accuracy                           0.82     28146\n",
      "   macro avg       0.84      0.82      0.82     28146\n",
      "weighted avg       0.84      0.82      0.82     28146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(X_test)\n",
    "print('accuracy %s' % accuracy_score(y_pred,y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      02    03    04   05    06    07    10    11    16    20    21   23   24   24a   24b    25    27   28    32    33\n",
      "02   942    80    11    0     4    64     3    16    10     2    98    2    0     0     2     8     6    0   105    74\n",
      "03    12  1235    31    4     0     0     0     5     0     0     8    8    0     0     2     1     0    0    82     2\n",
      "04    26    78  1128    0     0     0     0     0     0     1    27   12    0     0     1     1     0    4   103     5\n",
      "05    46     3    20  924    64    55    17    26     3     5    65    2    3     0    11    19     9    1    88    39\n",
      "06     0     0     0    0  1414     0     0     0     0     0     0    0    0     0     0     0     0    0     0     0\n",
      "07    11     0     1   23     1  1181     8    14     0     9    35    0    1     0     4     5     4    1    55    14\n",
      "10     6     0     1    9     0   106  1120    49     4    28    36    3    5     0     0     2     9    2    15    36\n",
      "11    51     0     2    9     4    36    24  1020     0    13    57   13    3     0     5     4     4   46    15    73\n",
      "16    19     0     0    0     0     0     1    11  1461     5     5    1    0     0     2     0     0    0     1     0\n",
      "20    29     0     0    4     3    11    16    21     0  1024    91    0    2     0     3    13     3    2   136    44\n",
      "21    41     2     0    0     1    86     1     8     0     0  1190    5    2     0     2     3     3    0    12    26\n",
      "23    25     3    15    1     3   177     5    36     0    18   112  949    2     0    12     7     4    2     9    20\n",
      "24    39     3     1    9    25    26    42    39     1    19    32    6  851     0    15    17    18    4   176    65\n",
      "24a    0     0     0    0     0     0     0     0     0     0     0    0    0  1399     0     0     0    0     0     0\n",
      "24b    0     0     0    0     8     0     1     5     0     0     2    0    0     8  1409     1     0    0     4     1\n",
      "25     0     0     0    0     0     0     0   116     0     0     0    0    0     0     0  1281     0    0     0     0\n",
      "27     4     3     2    6    13    17    64    44     2     7    26    4    9     0    42     1  1110    4    33     7\n",
      "28    48     0     6    4     3    70    15   171     0     0    17    0    0     0    11    13     3  956    45    39\n",
      "32     9     3    13   13     3     2     2    15     0    70    11    0    1     0     6     0     4    0  1230    11\n",
      "33    47     0     0    7     2    28     0    33     1     2    38    1    0     8    31    10     2    1    53  1183\n"
     ]
    }
   ],
   "source": [
    "cm =confusion_matrix(y_test, y_pred)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "labels = np.unique(y_test)\n",
    "df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Support Vector Machine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "accuracy 0.8379521068713138\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          02       0.77      0.62      0.69      1427\n",
      "          03       0.81      0.93      0.87      1390\n",
      "          04       0.91      0.85      0.88      1386\n",
      "          05       0.84      0.71      0.77      1400\n",
      "          06       0.88      1.00      0.93      1414\n",
      "          07       0.78      0.81      0.80      1367\n",
      "          10       0.88      0.83      0.85      1431\n",
      "          11       0.64      0.83      0.72      1379\n",
      "          16       0.94      0.98      0.96      1506\n",
      "          20       0.84      0.84      0.84      1402\n",
      "          21       0.84      0.77      0.81      1382\n",
      "          23       0.88      0.81      0.84      1400\n",
      "          24       0.90      0.70      0.79      1388\n",
      "         24a       0.99      1.00      1.00      1399\n",
      "         24b       0.91      0.99      0.95      1439\n",
      "          25       0.81      0.92      0.86      1397\n",
      "          27       0.82      0.85      0.83      1398\n",
      "          28       0.80      0.77      0.78      1401\n",
      "          32       0.71      0.80      0.76      1393\n",
      "          33       0.86      0.74      0.79      1447\n",
      "\n",
      "    accuracy                           0.84     28146\n",
      "   macro avg       0.84      0.84      0.84     28146\n",
      "weighted avg       0.84      0.84      0.84     28146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', SGDClassifier(loss= 'hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "              ])\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "%time\n",
    "\n",
    "y_pred = sgd.predict(X_test)\n",
    "print('accuracy %s' % accuracy_score(y_pred,y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      02    03    04   05    06    07    10    11    16    20    21    23   24   24a   24b    25    27    28    32    33\n",
      "02   890   102    11   14    15    40    13    21    53    11    54     4   10     0     5    37    16    13    71    47\n",
      "03     0  1295    37    3     0     0     0     3     0     1     1     7    0     1     2     1     3     2    31     3\n",
      "04     3    80  1176    1     3     1     0     3     0     3    15     6    1     0     4    10     3     6    67     4\n",
      "05    15    22    13  998    71    33    13    36     2    11    14    11    9     1    12    25    27    29    44    14\n",
      "06     0     0     0    0  1414     0     0     0     0     0     0     0    0     0     0     0     0     0     0     0\n",
      "07     9     8     2   40    11  1108    13    18     2     6    12    11    2     0     3    45    13    19    28    17\n",
      "10     3     2     4   29     2    31  1186    51     0    22     4     0   18     0     0    14    35    13     6    11\n",
      "11    13     3     2   13     8    14     7  1143     1    20    10    23    5     0     8    11    19    66     5     8\n",
      "16     9     0     0    2     0     0     3     8  1473     1     0     7    0     0     0     0     0     3     0     0\n",
      "20    16    11     0   11     1     5     7    24    11  1181    22     1   11     0     1    18    14    12    43    13\n",
      "21    73     6     0   21     6    43     4    10     7     4  1068    32    2     0     1    39    26     5    16    19\n",
      "23     6    20    16    6     2    54     2    37     2    17    27  1134   13     0     8    16    15    11     6     8\n",
      "24     8    17     3   23    33     7    33    38     4    14     8    17  968     0    21    15    64    37    68    10\n",
      "24a    0     0     0    0     0     0     0     0     0     0     0     0    0  1399     0     0     0     0     0     0\n",
      "24b    0     1     0    0     0     0     4     5     0     0     1     0    0     8  1418     0     2     0     0     0\n",
      "25     0     0     0    0     0     0     0   116     0     0     0     0    0     0     0  1281     0     0     0     0\n",
      "27     5     4     0    5    16     5    44    42     2     4     2    11   14     0    16     6  1194    23     4     1\n",
      "28    47     2     2    5     2    28    15   171     0    13     0     3    2     0     1     8     8  1075    16     3\n",
      "32    11    24    23   10    13     1     2    15     2    77     9     8   12     2    12     7    15    12  1119    19\n",
      "33    48     6     2    9    15    42     4    34     4    19    20    10   10     0    38    44    10    23    44  1065\n"
     ]
    }
   ],
   "source": [
    "cm =confusion_matrix(y_test, y_pred)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "labels = np.unique(y_test)\n",
    "df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "accuracy 0.9399559440062532\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          02       0.88      0.82      0.85      1427\n",
      "          03       0.92      0.95      0.93      1390\n",
      "          04       0.97      0.97      0.97      1386\n",
      "          05       0.95      0.90      0.92      1400\n",
      "          06       1.00      1.00      1.00      1414\n",
      "          07       0.96      0.95      0.96      1367\n",
      "          10       0.98      0.97      0.97      1431\n",
      "          11       0.71      0.96      0.81      1379\n",
      "          16       1.00      1.00      1.00      1506\n",
      "          20       0.96      0.94      0.95      1402\n",
      "          21       0.95      0.94      0.94      1382\n",
      "          23       0.93      0.95      0.94      1400\n",
      "          24       0.94      0.89      0.91      1388\n",
      "         24a       1.00      1.00      1.00      1399\n",
      "         24b       0.99      1.00      0.99      1439\n",
      "          25       1.00      0.92      0.95      1397\n",
      "          27       0.96      0.94      0.95      1398\n",
      "          28       0.95      0.87      0.91      1401\n",
      "          32       0.92      0.94      0.93      1393\n",
      "          33       0.92      0.91      0.92      1447\n",
      "\n",
      "    accuracy                           0.94     28146\n",
      "   macro avg       0.94      0.94      0.94     28146\n",
      "weighted avg       0.94      0.94      0.94     28146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "%time\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       02    03    04    05    06    07    10    11    16    20    21    23    24   24a   24b    25    27    28    32    33\n",
      "02   1169    86     7    11     0    14     0    21     2     4    17    10     8     0     2     0     2     8    26    40\n",
      "03     43  1317    16     2     0     0     0     1     0     0     0     1     1     0     0     0     2     0     7     0\n",
      "04      7    10  1340     0     0     0     0     1     0     1     5     0     2     0     0     0     0     0    19     1\n",
      "05     16     8     2  1260     0     9     6    21     1     8     4     8    17     0     0     1    11     3    14    11\n",
      "06      0     0     0     0  1414     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "07     11     0     3     8     0  1301     2    13     0     3     4    12     1     0     0     1     1     3     3     1\n",
      "10      0     0     0     0     0     1  1383    41     0     0     0     1     0     0     0     0     1     0     0     4\n",
      "11      3     0     0     3     0     3     1  1320     0     4     1     5     9     0     0     0     5    17     1     7\n",
      "16      0     0     0     0     0     0     0     5  1501     0     0     0     0     0     0     0     0     0     0     0\n",
      "20      8     0     0     4     0     1     0    23     0  1315     3     7     8     0     1     1     4     4    17     6\n",
      "21     16     3     0     5     0     6     0     8     0     0  1297    25     0     0     0     2     3     4     2    11\n",
      "23      5     1     1     6     0     5     0    11     0     6    20  1324    10     0     0     0     2     3     1     5\n",
      "24     11     1     3    16     0     7    16    28     0     9     2     9  1233     0     3     0    18     3    13    16\n",
      "24a     0     0     0     0     0     0     0     0     0     0     0     0     0  1399     0     0     0     0     0     0\n",
      "24b     0     0     0     0     0     0     0     5     0     0     0     0     0     0  1432     0     2     0     0     0\n",
      "25      0     0     0     0     0     0     0   116     0     0     0     0     0     0     0  1281     0     0     0     0\n",
      "27      5     1     0     4     0     0     2    41     0     1     0    10     4     0     0     0  1321     7     2     0\n",
      "28      2     1     0     0     0     1     3   165     0     1     0     3     0     0     0     0     1  1223     0     1\n",
      "32      8     6     8     4     0     1     1    14     0    16     1     0    10     0     0     0     3     2  1314     5\n",
      "33     29     4     2     9     0     8     1    31     0     5     9     9     6     0     5     0     5     4     8  1312\n"
     ]
    }
   ],
   "source": [
    "cm =confusion_matrix(y_test, y_pred)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "labels = np.unique(y_test)\n",
    "df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RandomForest Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "accuracy 0.9252824557663611\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "multi_class must be in ('ovo', 'ovr')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-85-b180bda17765>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mypred_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mypred_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mypred_2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m'auc_roc_score %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mypred_2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Classification Report'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\Python37\\lib\\site-packages\\sklearn\\metrics\\_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    379\u001b[0m                              \"instead\".format(max_fpr))\n\u001b[0;32m    380\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmulti_class\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raise'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 381\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"multi_class must be in ('ovo', 'ovr')\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    382\u001b[0m         return _multiclass_roc_auc_score(y_true, y_score, labels,\n\u001b[0;32m    383\u001b[0m                                          multi_class, average, sample_weight)\n",
      "\u001b[1;31mValueError\u001b[0m: multi_class must be in ('ovo', 'ovr')"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', RandomForestClassifier()),\n",
    "                 ])\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "%time\n",
    "\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "ypred_2 = rf.predict_proba(X)\n",
    "ypred_2 = [p[1] for p in ypred_2]\n",
    "print( 'auc_roc_score %s' % roc_auc_score(y_test, ypred_2) )\n",
    "print('Classification Report')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          02       0.79      0.78      0.78      1427\n",
      "          03       0.91      0.95      0.93      1390\n",
      "          04       0.99      0.94      0.96      1386\n",
      "          05       0.92      0.89      0.91      1400\n",
      "          06       1.00      1.00      1.00      1414\n",
      "          07       0.93      0.95      0.94      1367\n",
      "          10       0.98      0.97      0.97      1431\n",
      "          11       0.71      0.96      0.82      1379\n",
      "          16       0.99      0.99      0.99      1506\n",
      "          20       0.96      0.94      0.95      1402\n",
      "          21       0.94      0.92      0.93      1382\n",
      "          23       0.91      0.90      0.91      1400\n",
      "          24       0.95      0.80      0.87      1388\n",
      "         24a       1.00      1.00      1.00      1399\n",
      "         24b       0.99      0.99      0.99      1439\n",
      "          25       1.00      0.92      0.96      1397\n",
      "          27       0.94      0.94      0.94      1398\n",
      "          28       0.96      0.88      0.91      1401\n",
      "          32       0.90      0.94      0.92      1393\n",
      "          33       0.83      0.85      0.84      1447\n",
      "\n",
      "    accuracy                           0.93     28146\n",
      "   macro avg       0.93      0.93      0.93     28146\n",
      "weighted avg       0.93      0.93      0.93     28146\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification Report')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       02    03    04    05    06    07    10    11    16    20    21    23    24   24a   24b    25    27    28    32    33\n",
      "02   1108    87     3    18     0    22     0    16     1     4    19    11     7     0     1     0     7     6    29    88\n",
      "03     35  1325     8     2     0     0     0     1     0     0     0     2     4     0     0     0     1     0     9     3\n",
      "04     13    27  1297     1     0     0     0     0     0     2     2     4     0     0     0     0     1     0    29    10\n",
      "05     20     0     0  1249     0    14     4    18     4     2     6    10     5     0     2     1    15     6    15    29\n",
      "06      0     0     0     0  1414     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "07     10     0     1     4     0  1292     0    14     1     3     5    12     4     0     0     0     5     3     6     7\n",
      "10      5     0     0     0     0     0  1384    41     0     1     0     0     0     0     0     0     0     0     0     0\n",
      "11      3     0     0     5     0     2     0  1324     0     6     2     3     5     0     0     0     2    15     1    11\n",
      "16      0     0     0     0     0     0     0     5  1498     0     0     3     0     0     0     0     0     0     0     0\n",
      "20     26     0     0     6     0     2     0    20     2  1315     3     3     5     0     1     1     3     2     8     5\n",
      "21     32     0     0     5     0     9     0     8     2     4  1272    18     5     0     2     0     2     4     2    17\n",
      "23     18     0     0     9     0    23     0    18     2     2    16  1264     2     0     0     0    13     0     0    33\n",
      "24     40     3     0    26     0    11    15    27     6     6     8    40  1108     0     4     0    21     5    31    37\n",
      "24a     0     0     0     0     0     0     0     0     0     0     0     0     0  1399     0     0     0     0     0     0\n",
      "24b     4     0     0     0     0     0     0     5     0     0     0     0     0     0  1426     0     2     0     0     2\n",
      "25      0     0     0     0     0     0     0   116     0     0     0     0     0     0     0  1281     0     0     0     0\n",
      "27      3     0     0     4     0     1     7    41     4     1     1     0     4     0     1     0  1316     7     5     3\n",
      "28      8     0     0     0     0     0     0   155     0     0     2     2     3     0     0     0     5  1226     0     0\n",
      "32     13     7     3     5     0     3     0    14     0    17     5     2     3     0     2     0     2     0  1309     8\n",
      "33     73     2     0    20     0    11     1    32     0     7    14    11    12     0     2     0     7     8    11  1236\n"
     ]
    }
   ],
   "source": [
    "cm =confusion_matrix(y_test, y_pred)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "labels = np.unique(y_test)\n",
    "df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "print(df.to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
