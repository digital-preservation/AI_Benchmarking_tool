{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract Topics from files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\svenkata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\svenkata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import spacy\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites_txt = 'C:/Users/svenkata/Documents/websites_txt/'\n",
    "a_txt = 'C:/Users/svenkata/Documents/a_txt/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Conventional algorithms are often biased towards the majority class, not taking the data distribution into consideration*\n",
    "\n",
    "*Many times, the minority classes are treated as outliers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    stop_free = ' '.join([i for i in doc if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalised = ''.join(lemma.lemmatize(word) for word in punc_free)\n",
    "    \n",
    "    return normalised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['easy', 'ways', 'say', 'id', 'love', 'but', 'floss']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "doc = ['easy',' ways', 'say', 'i\\'d',' love',' but', 'have', 'floss']\n",
    "clean(doc)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "##I know this is very unprofessional but, string manipulation is very difficult otherwise!!\n",
    "def replace_chars(path):\n",
    "    to_replace = ['\\\\\\\\','\\\\']\n",
    "    string=''\n",
    "    for ch in path:\n",
    "        if ch in to_replace:\n",
    "            ch = '/'\n",
    "        elif ch == '\\v':\n",
    "            ch = '/v'\n",
    "        elif ch == '\\b':\n",
    "            ch = '/b'\n",
    "        elif ch == '\\t':\n",
    "            ch = '/t'\n",
    "        elif ch == '\\a':\n",
    "            ch = '/a'\n",
    "        elif ch == '\\f':\n",
    "            ch = '/f'\n",
    "        elif ch == '\\n':\n",
    "            ch = '/n'\n",
    "        elif ch == '\\r':\n",
    "            ch = '\\r'\n",
    "        elif ch == '\\h':\n",
    "            ch = '/h'\n",
    "        elif ch == '\\o':\n",
    "            ch = '/o'\n",
    "        string = string+ch\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import funcy as fp\n",
    "## Something quick and dirty......\n",
    "EMAIL_REGEX = re.compile(r'[a-z0-9\\.\\+_-]+@[a-z0-9\\.\\+_-]+\\.[a-z]*')\n",
    "FILTER_REGEX = re.compile(r\"[^a-z '#]\")\n",
    "TOKEN_MAPPINGS = [(EMAIL_REGEX,\"#email\"),(FILTER_REGEX,' ')]\n",
    "\n",
    "def tokenize_line(line):\n",
    "    res = line.lower()\n",
    "    for regexp, replacement in TOKEN_MAPPINGS:\n",
    "        res = regexp.sub(replacement,res)\n",
    "    return res.split()\n",
    "\n",
    "def tokenize(lines, token_size_filter = 2):\n",
    "    tokens = fp.mapcat(tokenize_line,lines)\n",
    "    return [t for t in tokens if len(t) > token_size_filter]\n",
    "   \n",
    "\n",
    "def load_doc(folder_path):\n",
    "    doc_list =[]\n",
    "    for subdir, dirnames, files  in os.walk(folder_path):         \n",
    "        for filename in files:             \n",
    "            if (filename.split('.')[-1] =='txt'):\n",
    "                #print(filename)\n",
    "                filepath = subdir+os.sep+filename\n",
    "                documentname = replace_chars(filepath)\n",
    "                try:\n",
    "                    with open(filepath, encoding='UTF-8', errors='ignore') as f:\n",
    "                        doc = f.readlines()                          \n",
    "                        tokens = tokenize(doc)\n",
    "                        tokens = clean(tokens)\n",
    "                        \n",
    "                        \n",
    "                    doc_list.append({'documentname': documentname, 'tokens':tokens  })\n",
    "                except :\n",
    "                    print(filepath)\n",
    "            else:\n",
    "                continue     \n",
    "             \n",
    "    doc_df = pd.DataFrame(doc_list)\n",
    "    \n",
    "    return doc_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/svenkata/Documents/websites_txt/Webteam\\blogs\\images from bloggers\\Martin Willis blog pics\\In reviewing the correspondence it is clear that the substance of your complaint regarding the wall boards is a continuation of the issues raised in your previous complaints.txt\n",
      "C:/Users/svenkata/Documents/websites_txt/Webteam\\Editorial\\FOI request files 2016-12-12\\Payments to he Advisory Panel on Public Sector Information [APPSI] and the Advisory Council on National Records and Archives [ACNRA] to the Confederation of British Industry and its subsidiaries.txt\n"
     ]
    }
   ],
   "source": [
    "docs_websites = load_doc(websites_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documentname</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:/Users/svenkata/Documents/websites_txt/Angel...</td>\n",
       "      <td>sheet archives sector pages broken links octob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:/Users/svenkata/Documents/websites_txt/Angel...</td>\n",
       "      <td>sheet cake contributors yes kind cake vegan gl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:/Users/svenkata/Documents/websites_txt/Angel...</td>\n",
       "      <td>https media nationalarchives gov index php yea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:/Users/svenkata/Documents/websites_txt/Angel...</td>\n",
       "      <td>blog front end https blog nationalarchives gov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:/Users/svenkata/Documents/websites_txt/Angel...</td>\n",
       "      <td>https media nationalarchives gov index php col...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        documentname  \\\n",
       "0  C:/Users/svenkata/Documents/websites_txt/Angel...   \n",
       "1  C:/Users/svenkata/Documents/websites_txt/Angel...   \n",
       "2  C:/Users/svenkata/Documents/websites_txt/Angel...   \n",
       "3  C:/Users/svenkata/Documents/websites_txt/Angel...   \n",
       "4  C:/Users/svenkata/Documents/websites_txt/Angel...   \n",
       "\n",
       "                                              tokens  \n",
       "0  sheet archives sector pages broken links octob...  \n",
       "1  sheet cake contributors yes kind cake vegan gl...  \n",
       "2  https media nationalarchives gov index php yea...  \n",
       "3  blog front end https blog nationalarchives gov...  \n",
       "4  https media nationalarchives gov index php col...  "
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_websites.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consists of contents of websites files as tokens for developing NLP models\n",
    "docs_websites.to_pickle('H:/AI_for_Selection/websites_contents_NLP.pkl') # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/svenkata/Documents/a_txt/20032008fp\\RMDaILUa\\C_3\\DfID\\A_3\\PRAaD\\AISU note of decisions from meeting with Middle East Centre and British Empire and Commonwealth Museum forwarded to DFID for confirmation that proposed solution is acceptable[A121439.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/ASD\\Cio\\RSE\\PHC0042\\PHCIoSLO2015\\Portsmouth Library and Archive Service - Inspection of Southsea Outstore 24th February 2015  - Advisory Letter   25th January 2016 - Postponing appointment pending accreditation[A4185929.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/ASD\\CS\\AtA\\AtApm\\CaIPODPaICO\\DIGITAL MEETS CULTURE - source for digital copyright contacts_activities_ RE_ published Fwd_ _AN INVITATION_ UKAD Forum - Thursday 27 March 2014, The National Archives [UNCLASSIFIED][A3529985.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/ASD\\Rwgan\\Gb\\NLHFHLF\\CC_7\\Collecting Cultures - response to request for second meeting - separate strand for discussing archive purchasing behaviour_ RE_ Next Collecting Cultures working group meeting - July [UNCLASSIFIED][A3348051.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/CC\\CCDPC\\CCDPCM\\CCDPCMMTP\\CCDMDCFFrc201718\\RE_ TNA170224827_ Document condition feedback enquiry (Document condition feedback) - E 122_208 and TNA170224828_ Document condition feedback enquiry (Document condition feedback)[A4647752.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/CC\\CCDPC\\CCDPCM\\CCDPCMMTP\\CCDMDCFFrc201819\\Re_ Document condition feedback - Ref_ TNA1528103176O79 - BT 43_117,  Ref_ TNA1528102977G49 - BT 43_129, Ref_ TNA1528103118E12 - BT 43_141, Ref_ TNA1528103245V68 - BT 43_153[A4996157.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/CC\\CCDPC\\CCDPCM\\CCDPCMMTP\\CCDMDCFFrc201920\\FW_ Document condition feedback - Ref_ TNA1528103176O79 - BT 43_117,  Ref_ TNA1528102977G49 - BT 43_129, Ref_ TNA1528103118E12 - BT 43_141, Ref_ TNA1528103245V68 - BT 43_153.1[A5165691.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/CC\\CCDPC\\CCDPCM\\CCDPCMMTP\\CCDMDCFFrc201920\\FW_ Document condition feedback - Ref_ TNA1528103176O79 - BT 43_117,  Ref_ TNA1528102977G49 - BT 43_129, Ref_ TNA1528103118E12 - BT 43_141, Ref_ TNA1528103245V68 - BT 43_153.2[A5165693.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/GA\\PRBn\\NPRBCs\\DfE\\A2015a2016\\CfifDH\\20150821 LH seeking advice from Access at Transfer Manager reDfE_Ofsted - Files potentially relevant to The Independent Inquiry into Child Sexual Abuse  [OFFICIAL-SENSITIVE][A4212784.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/GA\\PRBn\\NPRBCs\\DfE\\P2015a2016\\IMA\\FW_ Letter from Jeff James, Chief Executive and Keeper of The National Archives, to Chris Wormald Permanent Secretary, Department for Education, re_ Information Management Assessment[A3917206.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/GA\\PRBn\\NPRBCs\\DfID\\TADTC20082014\\LMBDIDTNATNTC\\Listing checks DO 35 (10915-10921), DO 161 (494), DO 164 (137), DO 165 (175-178), CO 1017 (903-909), CO 822 (3294-3295), CO 1031 (5289), OD 115 (1-100) [UNCLASSIFIED][A3441869.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/GA\\PRBn\\NPRBCs\\DfID\\TADTC20082014\\LMBDIDTNATNTC\\RE_ Listing checks DO 35 (10915-10921), DO 161 (494), DO 164 (137), DO 165 (175-178), CO 1017 (903-909), CO 822 (3294-3295), CO 1031 (5289), OD 115 (1-100) [UNCLASSIFIED][A3441932.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/GA\\PRBn\\NPRBCs\\DfID\\TADTc2015a2016\\LMBDIDTNATNTCc\\Listing checks DO 35 (10915-10921), DO 161 (494), DO 164 (137), DO 165 (175-178), CO 1017 (903-909), CO 822 (3294-3295), CO 1031 (5289), OD 115 (1-100) [UNCLASSIFIED][A3906367.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/GA\\PRBn\\NPRBCs\\DfID\\TADTc2015a2016\\LMBDIDTNATNTCc\\RE_ Listing checks DO 35 (10915-10921), DO 161 (494), DO 164 (137), DO 165 (175-178), CO 1017 (903-909), CO 822 (3294-3295), CO 1031 (5289), OD 115 (1-100) [UNCLASSIFIED][A3906430.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/GA\\PRBn\\NPRBCs\\FC\\A2015a2016\\NYO\\20151119 FC initial response re NYO Action required by 24th November_ Guidance on records and information management on behalf of the Keeper of Public Records     [OFFICIAL-SENSITIVE][A4208631.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/GA\\PRBn\\NPRBCs\\FC\\A2015a2016\\NYO\\20151119 LH chasing FC re NYO_ Action required by 24th November_ Guidance on records and information management on behalf of the Keeper of Public Records     [OFFICIAL-SENSITIVE][A4208629.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/GA\\T\\TR\\DEFE\\DEFE20082016\\DEFE24\\Original E-transfer Form DEFE 24_1984-2093 various (transferring 1984_1-1985_1, 1987_1, 2005_1, 2043_1, 2061_1, 2073_1-2074_1, 2077_1-2080_1, 2083_1-2084_1, 2087_1-2090_1, 2093_1)[A2951999.3].txt\n",
      "C:/Users/svenkata/Documents/a_txt/GA\\T\\TR\\DEFE\\DEFE20082016\\DEFE24\\Original E-transfer Form DEFE 24_1986, 1997, 1999, 2018, 2021-2026, 2028, 2030, 2032, 2036- 2041, 2044, 2046, 2048-2050, 2052-2055, 2058-2059, 2062-2063, 2085, 2092 DIGITAL SURROGATES ONLY[A2405305.2].txt\n",
      "C:/Users/svenkata/Documents/a_txt/GA\\T\\TR\\PREM\\PREM20112018prtfnsatt\\PREM19\\Original E-transfer form PREM 19_1342, 1361, 1362, 1376, 1391, 1424, 1426, 1433, 1545, 1557, 1570-1572, 1581, 1587, 1588, 1646, 1660, 1647 extracts - previously retained[A4928313.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/GA\\T\\TR\\PREM\\PREM20112018prtfnsatt\\PREM19\\Original E-transfer Form PREM 19_691_1, 791_1, 916_1, 955_2, 1048_1, 1048_2, 1049_3, 1051_1, 1055_1, 1056_1, 1057_1,_2, 1059_1, 1087_2, 1097_1, 1105_1,_2, 1107_1, 1124_1 closed[A4928344.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/GA\\T\\TR\\T_3\\T20112018prtfnsatt\\T437_1\\Original e-Transfer Form T 437_383 384, 424, 425, 448-451, 454-458, 462, 463, 522-524, 547, 549, 553, 556, 558-560, 563, 565, 567, 569, 574, 576, 578-580 previously retained[A4927485.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/PPDaCM\\PD\\PC_1\\20YRaCS\\GE_1\\2012RTR\\_IGNORERTRDSfAS2012MP\\BISDECC The TNA Combined Statistical Return for 2012 for the Department for Business Innovation and Skills and the Department of Energy and Climate Chance[A3088320.1].txt\n",
      "C:/Users/svenkata/Documents/a_txt/WA\\HA\\201113Fo20MB\\20112013Fo20MB\\Fo20MB\\tg07-detailed-urban-characeter-analysis (http___webarchive.nationalarchives.gov.uk_20110118095356_http___www.cabe.org.uk_files_tg07-detailed-urban-characeter-analysis.txt)[A2623457.1].txt\n"
     ]
    }
   ],
   "source": [
    "docs_a = load_doc(a_txt)\n",
    "docs_a.to_pickle('H:/AI_for_Selection/a_contents_NLP.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read Preprocessed Data into Data frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_a_df = pd.read_pickle('H:/AI_for_Selection/a_contents_NLP.pkl')# This file consists of the contents of the files (structured data from Objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_meta_df = pd.read_pickle('H:/AI_for_Selection/metadata_with_filesize_lastmodified.pkl') # This file consists of processed metadata of structured data\n",
    "\n",
    "txt_extensions = ['pdf','rtf','txt','msg','doc','docx','xls', 'xlsx','mbx']\n",
    "\n",
    "a_meta_df=a_meta_df[a_meta_df.fileextension.isin(txt_extensions)]\n",
    "a_meta_df = a_meta_df[['fileextension', 'disposal_schedule', 'repository','file_path' ,'ret_schedule','file_size']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites_meta_df = pd.read_pickle('H:/AI_for_Selection/Websites_processed.pkl')# This file consists of metadata of processed websites data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_websites = pd.read_pickle('H:/AI_for_Selection/websites_contents_NLP.pkl') # This file consists of the contents of the files (unstructred X-drive data)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documentname</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:/Users/svenkata/Documents/websites_txt/Angela-Hill/archives-sector-dec-17.txt</td>\n",
       "      <td>sheet archives sector pages broken links october url http www nationalarchives gov archives sector finding funding funding programmes government statutory funding http www nationalarchives gov archives sector archives sector role working partnership http www artscouncil org funding apply funding grants arts url http www nationalarchives gov archives sector finding funding private giving individual http www nationalarchives gov archives sector finding funding private giving corporate http artsandbusiness org url http www nationalarchives gov archives sector finding funding lottery funding http www nationalarchives gov archives sector finding funding funding programmes trusts foundations http www nhmf org pages default aspx url http www nationalarchives gov archives sector finding funding funding programmes trusts foundations http www nationalarchives gov archives sector finding funding funding programmes government statutory funding http www artscouncil org funding apply funding prism url http www nationalarchives gov archives sector finding funding working volunteers http www nationalarchives gov archives sector projects programmes volunteering research project http www archives org campaigns volunteering html url http www nationalarchives gov archives sector keeping touch share expertise http www britishrecordsassociation org url http www nationalarchives gov archives sector case studies research reports research reports http almauk org working together activity economic impacts url http www nationalarchives gov archives sector case studies research reports research reports http www willpowerinfovm webspace virginmedia com jisc url http www nationalarchives gov archives sector archives sector role legislation legislation title tbc http www justice gov information access rights foi guidance practitioners code practice url http www nationalarchives gov archives sector finding funding funding programmes government statutory funding http www ahrc pages default aspx url http www nationalarchives gov archives sector archives sector role working partnership http www local gov culture tourism sport jsessionid ecf wlb url http www nationalarchives gov archives sector finding funding fundraising strategy developing fundraising strategy resources http www missionmodelsmoney org url http www nationalarchives gov archives sector finding funding generating income http www ica org institutions conferences congress commercial activity archives seminar london may html url http www nationalarchives gov archives sector advice guidance running organisation capital developments http www cultureandsportplanningtoolkit org url http www nationalarchives gov archives sector advice guidance running organisation capital developments http www artscouncil org media uploads pdf cil guidance final pdf url http www nationalarchives gov archives sector advice guidance running organisation capital developments http www architecture com files ribaprofessionalservices clientservices commissioningarchitecture pdf url http www nationalarchives gov archives sector advice guidance running organisation assessing environmental impact http www hlf org howtoapply furtherresources pages greenerheritageprojects aspx url http www nationalarchives gov archives sector advice guidance running organisation managing performance http www cipfasocialresearch net libraries plus url http www nationalarchives gov archives sector advice guidance running organisation managing performance http www archives org psqg public services quality group psqg html url http www nationalarchives gov archives sector advice guidance managing collection designation http www unesco org webworld mdm index mdm html url http www nationalarchives gov archives sector finding funding private giving individual http www legacy com downloads url http www nationalarchives gov archives sector finding funding private giving individual http legacy com url http www nationalarchives gov archives sector finding funding private giving individual http www philanthropyuk org url http www nationalarchives gov archives sector finding funding private giving individual http www philanthropyuk org publications rich people give url http www nationalarchives gov archives sector finding funding lottery funding http www carnivalarts org news tabid articletype articleview articleid ukcca launches first regional digital carnival archive aspx url http www nationalarchives gov archives sector finding funding lottery funding http www gloucestershire gov archives article fielding platt community archive project gains heritage lottery fund support url http www nationalarchives gov archives sector finding funding working volunteers http www volunteering org goodpractice url http www nationalarchives gov archives sector advice guidance talking community developing formal education http www inspiringlearningforall gov url http www nationalarchives gov archives sector advice guidance talking community community engagement http www participatorybudgeting org url http www nationalarchives gov archives sector advice guidance talking community informal learning http shop niace org family learning mla html url http www nationalarchives gov archives sector advice guidance talking community informal learning http www alw org url http www nationalarchives gov archives sector advice guidance talking community informal learning http www local gov web guest publications journal content publication template url http www nationalarchives gov archives sector advice guidance managing collection documenting collections cataloguing archives networks http www ica org standards standards list html url http www nationalarchives gov archives sector case studies research reports case studies sustainable services staffordshire archive service http www staffordshire gov leisure archives homepage aspx url http www nationalarchives gov archives sector finding funding funding programmes trusts foundations http www nms making connections support museums national fund acquisitions aspx url http www nationalarchives gov archives sector finding funding fundraising strategy developing fundraising strategy resources http www ncvo org practical support funding grants url http www nationalarchives gov archives sector case studies research reports case studies audience development angus library archive http intouniversity org content graduate trainee education worker scheme url http www nationalarchives gov archives sector advice guidance managing collection documenting collections cataloguing archives networks http www collectionstrust org choose cms url http www nationalarchives gov archives sector advice guidance managing collection preserving digital collections get started https app box com qwawctlwqcz url http www nationalarchives gov archives sector advice guidance running organisation capital developments http www archiveswales org find archive gwent archives url http www nationalarchives gov archives sector finding funding funding programmes trusts foundations http www wellcome stellent groups corporatesite policy communications documents web document wtvm pdf url http www nationalarchives gov archives sector finding funding lottery funding http www collectionstrust org collections link url http www nationalarchives gov archives sector advice guidance managing collection developing collections collection development http www collectionstrust org item revisiting collections url http www nationalarchives gov archives sector advice guidance managing collection developing collections collection development http www collectionstrust org item revisiting museum collections toolkit url http www nationalarchives gov archives sector projects programmes first world war archives http nationalarchives gov archives sector working partnership htm url http www nationalarchives gov archives sector finding funding lottery funding http www york gov info archives archives eyork archives local history cli href url http www nationalarchives gov archives sector keeping touch blog http www kew org discover blogs library art archives url http www nationalarchives gov archives sector case studies research reports case studies collections development care hudd music hall archive https www suffolk hudd music hall archive http www nationalarchives gov archives sector projects programmes first world war archives http nationalarchives gov archives sector working partnership htm sheet sheet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                      documentname  \\\n",
       "0  C:/Users/svenkata/Documents/websites_txt/Angela-Hill/archives-sector-dec-17.txt   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               tokens  \n",
       "0  sheet archives sector pages broken links october url http www nationalarchives gov archives sector finding funding funding programmes government statutory funding http www nationalarchives gov archives sector archives sector role working partnership http www artscouncil org funding apply funding grants arts url http www nationalarchives gov archives sector finding funding private giving individual http www nationalarchives gov archives sector finding funding private giving corporate http artsandbusiness org url http www nationalarchives gov archives sector finding funding lottery funding http www nationalarchives gov archives sector finding funding funding programmes trusts foundations http www nhmf org pages default aspx url http www nationalarchives gov archives sector finding funding funding programmes trusts foundations http www nationalarchives gov archives sector finding funding funding programmes government statutory funding http www artscouncil org funding apply funding prism url http www nationalarchives gov archives sector finding funding working volunteers http www nationalarchives gov archives sector projects programmes volunteering research project http www archives org campaigns volunteering html url http www nationalarchives gov archives sector keeping touch share expertise http www britishrecordsassociation org url http www nationalarchives gov archives sector case studies research reports research reports http almauk org working together activity economic impacts url http www nationalarchives gov archives sector case studies research reports research reports http www willpowerinfovm webspace virginmedia com jisc url http www nationalarchives gov archives sector archives sector role legislation legislation title tbc http www justice gov information access rights foi guidance practitioners code practice url http www nationalarchives gov archives sector finding funding funding programmes government statutory funding http www ahrc pages default aspx url http www nationalarchives gov archives sector archives sector role working partnership http www local gov culture tourism sport jsessionid ecf wlb url http www nationalarchives gov archives sector finding funding fundraising strategy developing fundraising strategy resources http www missionmodelsmoney org url http www nationalarchives gov archives sector finding funding generating income http www ica org institutions conferences congress commercial activity archives seminar london may html url http www nationalarchives gov archives sector advice guidance running organisation capital developments http www cultureandsportplanningtoolkit org url http www nationalarchives gov archives sector advice guidance running organisation capital developments http www artscouncil org media uploads pdf cil guidance final pdf url http www nationalarchives gov archives sector advice guidance running organisation capital developments http www architecture com files ribaprofessionalservices clientservices commissioningarchitecture pdf url http www nationalarchives gov archives sector advice guidance running organisation assessing environmental impact http www hlf org howtoapply furtherresources pages greenerheritageprojects aspx url http www nationalarchives gov archives sector advice guidance running organisation managing performance http www cipfasocialresearch net libraries plus url http www nationalarchives gov archives sector advice guidance running organisation managing performance http www archives org psqg public services quality group psqg html url http www nationalarchives gov archives sector advice guidance managing collection designation http www unesco org webworld mdm index mdm html url http www nationalarchives gov archives sector finding funding private giving individual http www legacy com downloads url http www nationalarchives gov archives sector finding funding private giving individual http legacy com url http www nationalarchives gov archives sector finding funding private giving individual http www philanthropyuk org url http www nationalarchives gov archives sector finding funding private giving individual http www philanthropyuk org publications rich people give url http www nationalarchives gov archives sector finding funding lottery funding http www carnivalarts org news tabid articletype articleview articleid ukcca launches first regional digital carnival archive aspx url http www nationalarchives gov archives sector finding funding lottery funding http www gloucestershire gov archives article fielding platt community archive project gains heritage lottery fund support url http www nationalarchives gov archives sector finding funding working volunteers http www volunteering org goodpractice url http www nationalarchives gov archives sector advice guidance talking community developing formal education http www inspiringlearningforall gov url http www nationalarchives gov archives sector advice guidance talking community community engagement http www participatorybudgeting org url http www nationalarchives gov archives sector advice guidance talking community informal learning http shop niace org family learning mla html url http www nationalarchives gov archives sector advice guidance talking community informal learning http www alw org url http www nationalarchives gov archives sector advice guidance talking community informal learning http www local gov web guest publications journal content publication template url http www nationalarchives gov archives sector advice guidance managing collection documenting collections cataloguing archives networks http www ica org standards standards list html url http www nationalarchives gov archives sector case studies research reports case studies sustainable services staffordshire archive service http www staffordshire gov leisure archives homepage aspx url http www nationalarchives gov archives sector finding funding funding programmes trusts foundations http www nms making connections support museums national fund acquisitions aspx url http www nationalarchives gov archives sector finding funding fundraising strategy developing fundraising strategy resources http www ncvo org practical support funding grants url http www nationalarchives gov archives sector case studies research reports case studies audience development angus library archive http intouniversity org content graduate trainee education worker scheme url http www nationalarchives gov archives sector advice guidance managing collection documenting collections cataloguing archives networks http www collectionstrust org choose cms url http www nationalarchives gov archives sector advice guidance managing collection preserving digital collections get started https app box com qwawctlwqcz url http www nationalarchives gov archives sector advice guidance running organisation capital developments http www archiveswales org find archive gwent archives url http www nationalarchives gov archives sector finding funding funding programmes trusts foundations http www wellcome stellent groups corporatesite policy communications documents web document wtvm pdf url http www nationalarchives gov archives sector finding funding lottery funding http www collectionstrust org collections link url http www nationalarchives gov archives sector advice guidance managing collection developing collections collection development http www collectionstrust org item revisiting collections url http www nationalarchives gov archives sector advice guidance managing collection developing collections collection development http www collectionstrust org item revisiting museum collections toolkit url http www nationalarchives gov archives sector projects programmes first world war archives http nationalarchives gov archives sector working partnership htm url http www nationalarchives gov archives sector finding funding lottery funding http www york gov info archives archives eyork archives local history cli href url http www nationalarchives gov archives sector keeping touch blog http www kew org discover blogs library art archives url http www nationalarchives gov archives sector case studies research reports case studies collections development care hudd music hall archive https www suffolk hudd music hall archive http www nationalarchives gov archives sector projects programmes first world war archives http nationalarchives gov archives sector working partnership htm sheet sheet  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_websites.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge contents file with metadata file on the target column (retention schedule)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth',None) # to view the whole width of the cells in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    C:/Users/svenkata/Documents/a_txt/20032008fp/Am/Cs/Es/L_5/LNAS/MDR 1 How to use the paper MDR[A905701.3].txt\n",
      "Name: documentname, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(docs_a_df[:1]['documentname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "##function to extract file's absolute path and filename with no extension\n",
    "def extract_absolutePath_removeExtension(topfolderpath, filename):\n",
    "    filename = filename.replace(topfolderpath,'')\n",
    "    words = filename.split('.')\n",
    "    return '.'.join(words[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_a_df['abs_filepath'] = docs_a_df['documentname'].apply(lambda x: extract_absolutePath_removeExtension('C:/Users/svenkata/Documents/a_txt/',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                   documentname  \\\n",
      "0  C:/Users/svenkata/Documents/a_txt/20032008fp/Am/Cs/Es/L_5/LNAS/MDR 1 How to use the paper MDR[A905701.3].txt   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   tokens  \\\n",
      "0  mdr use manorial documents register mdr manorial documents register mdr mdr index manorial records provides brief descriptions documents details locations mdr mainly paper index details counties wales three ridings yorkshire hampshire isle wight norfolk surrey middlesex revised updated computerised searched online www mdr nationalarchives gov mdr see nra non computerised counties use paper indexes mdr paper indexes mdr held research enquiries room divided two parts parish index manor index parish index parish index identifies names manors associated parishes two always identical arranged county alphabetically parish name within county information contained slip parish name middle slip manor name located top right hand corner manors may known variety names records listed one name manor index slip two manor names top right hand corner name preceded see standard manor name may one slip associated parish giving several manor names research possible parish index may include reference manor records indexed manor index indicates manor existed records survived location remains unknown please aware parish include one manors manors cross parish boundaries parts manors exist separately main body manor different parishes counties manor name known manor index consulted manor index manor index arranged county alphabetically manor name within county manor index contain reference manors known existed records extant manor may mentioned index information contained slip manor name top right hand corner document description date middle slip details location document reference found bottom please aware details locations may date repository names may changed pro tna kent archive office centre kentish studies information contained reverse slip reverse slip often provides source details document overleaf see reference number beginning nra catalogue collection held research enquiries room users mdr researching particular topographical area may find helpful consult nra list may contain references records although strictly manorial nature therefore included mdr may nevertheless prove useful research information nra see nra reference annual return means information came tna accessions repositories survey see nra information accessions pages reference means mdr team information file regarding location records information please contact mdr team every document noted mdr available research always check relevant repository first making visit view document slip refers private owner please apply writing information terms conditions access   \n",
      "\n",
      "                                                             abs_filepath  \n",
      "0  20032008fp/Am/Cs/Es/L_5/LNAS/MDR 1 How to use the paper MDR[A905701.3]  \n"
     ]
    }
   ],
   "source": [
    "print(docs_a_df[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  fileextension                                           disposal_schedule  \\\n",
      "0           xls  24 Projects - Full Projects (Close file when Project ends)   \n",
      "\n",
      "           repository  \\\n",
      "0  Strategic Projects   \n",
      "\n",
      "                                                                                          file_path  \\\n",
      "0  PPDaCM/PD/PC_1/20YRaCS/GE_1/2012RTR/RTRS2012DS/The National Archives RTR 09-2012[A3109716.2].xls   \n",
      "\n",
      "  ret_schedule  file_size  \\\n",
      "0           24     225280   \n",
      "\n",
      "                                                                                   abs_filepath  \n",
      "0  PPDaCM/PD/PC_1/20YRaCS/GE_1/2012RTR/RTRS2012DS/The National Archives RTR 09-2012[A3109716.2]  \n"
     ]
    }
   ],
   "source": [
    "# check how the file path is represented in structured file\n",
    "print(a_meta_df[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_meta_df['abs_filepath']  = a_meta_df['file_path'].apply(lambda x: extract_absolutePath_removeExtension('', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  fileextension                                           disposal_schedule  \\\n",
      "0           xls  24 Projects - Full Projects (Close file when Project ends)   \n",
      "\n",
      "           repository  \\\n",
      "0  Strategic Projects   \n",
      "\n",
      "                                                                                          file_path  \\\n",
      "0  PPDaCM/PD/PC_1/20YRaCS/GE_1/2012RTR/RTRS2012DS/The National Archives RTR 09-2012[A3109716.2].xls   \n",
      "\n",
      "  ret_schedule  file_size  \\\n",
      "0           24     225280   \n",
      "\n",
      "                                                                                   abs_filepath  \n",
      "0  PPDaCM/PD/PC_1/20YRaCS/GE_1/2012RTR/RTRS2012DS/The National Archives RTR 09-2012[A3109716.2]  \n"
     ]
    }
   ],
   "source": [
    "print(a_meta_df[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge doc contents with metadata on abs_filepath**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.merge(docs_a_df, a_meta_df, on='abs_filepath')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['documentname', 'tokens', 'abs_filepath', 'fileextension',\n",
      "       'disposal_schedule', 'repository', 'file_path', 'ret_schedule',\n",
      "       'file_size'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**add a numerical category_id to avoid string type ret_schedule**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>documentname</th>\n",
       "      <th>tokens</th>\n",
       "      <th>abs_filepath</th>\n",
       "      <th>fileextension</th>\n",
       "      <th>disposal_schedule</th>\n",
       "      <th>repository</th>\n",
       "      <th>file_path</th>\n",
       "      <th>ret_schedule</th>\n",
       "      <th>file_size</th>\n",
       "      <th>category_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:/Users/svenkata/Documents/a_txt/20032008fp/Am/Cs/Es/L_5/LNAS/MDR 1 How to use the paper MDR[A905701.3].txt</td>\n",
       "      <td>mdr use manorial documents register mdr manorial documents register mdr mdr index manorial records provides brief descriptions documents details locations mdr mainly paper index details counties wales three ridings yorkshire hampshire isle wight norfolk surrey middlesex revised updated computerised searched online www mdr nationalarchives gov mdr see nra non computerised counties use paper indexes mdr paper indexes mdr held research enquiries room divided two parts parish index manor index parish index parish index identifies names manors associated parishes two always identical arranged county alphabetically parish name within county information contained slip parish name middle slip manor name located top right hand corner manors may known variety names records listed one name manor index slip two manor names top right hand corner name preceded see standard manor name may one slip associated parish giving several manor names research possible parish index may include reference manor records indexed manor index indicates manor existed records survived location remains unknown please aware parish include one manors manors cross parish boundaries parts manors exist separately main body manor different parishes counties manor name known manor index consulted manor index manor index arranged county alphabetically manor name within county manor index contain reference manors known existed records extant manor may mentioned index information contained slip manor name top right hand corner document description date middle slip details location document reference found bottom please aware details locations may date repository names may changed pro tna kent archive office centre kentish studies information contained reverse slip reverse slip often provides source details document overleaf see reference number beginning nra catalogue collection held research enquiries room users mdr researching particular topographical area may find helpful consult nra list may contain references records although strictly manorial nature therefore included mdr may nevertheless prove useful research information nra see nra reference annual return means information came tna accessions repositories survey see nra information accessions pages reference means mdr team information file regarding location records information please contact mdr team every document noted mdr available research always check relevant repository first making visit view document slip refers private owner please apply writing information terms conditions access</td>\n",
       "      <td>20032008fp/Am/Cs/Es/L_5/LNAS/MDR 1 How to use the paper MDR[A905701.3]</td>\n",
       "      <td>doc</td>\n",
       "      <td>33 Time Category Permanent</td>\n",
       "      <td>Historical Manuscripts Commission</td>\n",
       "      <td>20032008fp/Am/Cs/Es/L_5/LNAS/MDR 1 How to use the paper MDR[A905701.3].doc</td>\n",
       "      <td>33</td>\n",
       "      <td>225792</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                   documentname  \\\n",
       "0  C:/Users/svenkata/Documents/a_txt/20032008fp/Am/Cs/Es/L_5/LNAS/MDR 1 How to use the paper MDR[A905701.3].txt   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   tokens  \\\n",
       "0  mdr use manorial documents register mdr manorial documents register mdr mdr index manorial records provides brief descriptions documents details locations mdr mainly paper index details counties wales three ridings yorkshire hampshire isle wight norfolk surrey middlesex revised updated computerised searched online www mdr nationalarchives gov mdr see nra non computerised counties use paper indexes mdr paper indexes mdr held research enquiries room divided two parts parish index manor index parish index parish index identifies names manors associated parishes two always identical arranged county alphabetically parish name within county information contained slip parish name middle slip manor name located top right hand corner manors may known variety names records listed one name manor index slip two manor names top right hand corner name preceded see standard manor name may one slip associated parish giving several manor names research possible parish index may include reference manor records indexed manor index indicates manor existed records survived location remains unknown please aware parish include one manors manors cross parish boundaries parts manors exist separately main body manor different parishes counties manor name known manor index consulted manor index manor index arranged county alphabetically manor name within county manor index contain reference manors known existed records extant manor may mentioned index information contained slip manor name top right hand corner document description date middle slip details location document reference found bottom please aware details locations may date repository names may changed pro tna kent archive office centre kentish studies information contained reverse slip reverse slip often provides source details document overleaf see reference number beginning nra catalogue collection held research enquiries room users mdr researching particular topographical area may find helpful consult nra list may contain references records although strictly manorial nature therefore included mdr may nevertheless prove useful research information nra see nra reference annual return means information came tna accessions repositories survey see nra information accessions pages reference means mdr team information file regarding location records information please contact mdr team every document noted mdr available research always check relevant repository first making visit view document slip refers private owner please apply writing information terms conditions access   \n",
       "\n",
       "                                                             abs_filepath  \\\n",
       "0  20032008fp/Am/Cs/Es/L_5/LNAS/MDR 1 How to use the paper MDR[A905701.3]   \n",
       "\n",
       "  fileextension           disposal_schedule  \\\n",
       "0           doc  33 Time Category Permanent   \n",
       "\n",
       "                          repository  \\\n",
       "0  Historical Manuscripts Commission   \n",
       "\n",
       "                                                                    file_path  \\\n",
       "0  20032008fp/Am/Cs/Es/L_5/LNAS/MDR 1 How to use the paper MDR[A905701.3].doc   \n",
       "\n",
       "  ret_schedule  file_size  category_id  \n",
       "0           33     225792            0  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X['category_id'] = X['ret_schedule'].factorize()[0]\n",
    "category_id_df = X[['disposal_schedule','category_id']].drop_duplicates().sort_values('category_id')\n",
    "category_to_id = dict(category_id_df.values)\n",
    "id_to_category = dict(category_id_df[['category_id','disposal_schedule']].values)\n",
    "X[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of X: 93824\n",
      "7     18741\n",
      "11    13110\n",
      "0     10786\n",
      "6      8973\n",
      "12     8047\n",
      "1      7100\n",
      "10     5115\n",
      "4      2762\n",
      "14     2713\n",
      "3      2686\n",
      "13     2679\n",
      "9      2573\n",
      "5      2298\n",
      "8      1909\n",
      "2      1612\n",
      "18     1486\n",
      "15      925\n",
      "16      250\n",
      "17       58\n",
      "19        1\n",
      "Name: category_id, dtype: int64\n",
      "24     18741\n",
      "02     13110\n",
      "33     10786\n",
      "05      8973\n",
      "04      8047\n",
      "23      7100\n",
      "03      5115\n",
      "20      2762\n",
      "11      2713\n",
      "21      2686\n",
      "07      2679\n",
      "27      2573\n",
      "32      2298\n",
      "28      1909\n",
      "16      1612\n",
      "24b     1486\n",
      "10       925\n",
      "25       250\n",
      "06        58\n",
      "24a        1\n",
      "Name: ret_schedule, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Length of X:',len(X))\n",
    "# just checking\n",
    "print(X.category_id.value_counts())\n",
    "print(X.ret_schedule.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfMAAAF9CAYAAAAQtYHjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de7hddX3n8ffHIFTHCyABkUuDGm1R2ygZtOPY4uAFtS3YxwtMi2htoz6iYzvTEqvz4GjppB2tldZSsUago1AKKnlKFCNj1Xa4JCDl4o0IqJEIKXjB0WrB7/yxfkc34Zzk5OyzSX4n79fz7Gev/Vtrfffv7JPsz1m/9dtrp6qQJEn9esDO7oAkSRqPYS5JUucMc0mSOmeYS5LUOcNckqTOGeaSJHVuj53dgbnab7/9asmSJTu7G5Ik3S+uuuqqf6mqxdOt6zbMlyxZwoYNG3Z2NyRJul8k+cpM6xxmlySpc4a5JEmdM8wlSeqcYS5JUucMc0mSOmeYS5LUOcNckqTOGeaSJHXOMJckqXOGuSRJnTPMJUnqnGEuSVLnDHNJkjrX7bemSdI4lqy8eNbb3rLqBRPsiTQ+j8wlSeqcYS5JUucMc0mSOmeYS5LUOcNckqTOGeaSJHXOMJckqXOGuSRJnTPMJUnqnGEuSVLnDHNJkjpnmEuS1DnDXJKkzhnmkiR1zjCXJKlzhrkkSZ3bbpgnWZ3k9iTXj7T9bZJr2u2WJNe09iVJvj+y7q9G9jkiyXVJNiY5PUla+75J1iW5sd3vM4kfVJKkhWo2R+ZnAceMNlTVS6tqWVUtAy4EPjSy+stT66rq1SPtZwArgKXtNlVzJXBpVS0FLm2PJUnSLG03zKvq08Cd061rR9cvAc7dVo0kBwIPq6rLqqqAc4Dj2upjgbPb8tkj7ZIkaRbGPWf+DOC2qrpxpO2wJJ9N8qkkz2htBwGbRrbZ1NoADqiqzQDtfv8x+yRJ0m5ljzH3P4F7H5VvBg6tqjuSHAF8JMkTgEyzb+3okyVZwTBUz6GHHjqH7kqStPDM+cg8yR7ArwF/O9VWVT+oqjva8lXAl4HHMRyJHzyy+8HArW35tjYMPzUcf/tMz1lVZ1bV8qpavnjx4rl2XZKkBWWcYfZnAV+oqh8PnydZnGRRW340w0S3m9rw+V1JntbOs78MuKjttgY4qS2fNNIuSZJmYTYfTTsXuAx4fJJNSV7ZVh3PfSe+/SJwbZJ/Bi4AXl1VU5PnXgP8NbCR4Yj9o619FfDsJDcCz26PJUnSLG33nHlVnTBD+8unabuQ4aNq022/AXjiNO13AEdvrx+SJGl6XgFOkqTOGeaSJHXOMJckqXOGuSRJnTPMJUnqnGEuSVLnDHNJkjpnmEuS1DnDXJKkzhnmkiR1zjCXJKlzhrkkSZ0zzCVJ6pxhLklS5wxzSZI6Z5hLktQ5w1ySpM4Z5pIkdc4wlySpc4a5JEmdM8wlSeqcYS5JUucMc0mSOmeYS5LUOcNckqTOGeaSJHXOMJckqXOGuSRJnTPMJUnqnGEuSVLnDHNJkjpnmEuS1DnDXJKkzm03zJOsTnJ7kutH2t6S5OtJrmm354+se2OSjUm+mOS5I+3HtLaNSVaOtB+W5IokNyb52yR7zucPKEnSQjebI/OzgGOmaX9nVS1rt7UASQ4Hjgee0Pb5yySLkiwC3g08DzgcOKFtC/DHrdZS4JvAK8f5gSRJ2t1sN8yr6tPAnbOsdyxwXlX9oKpuBjYCR7bbxqq6qap+CJwHHJskwH8CLmj7nw0ct4M/gyRJu7VxzpmfnOTaNgy/T2s7CPjayDabWttM7Y8AvlVVd2/VPq0kK5JsSLJhy5YtY3RdkqSFY65hfgbwGGAZsBl4R2vPNNvWHNqnVVVnVtXyqlq+ePHiHeuxJEkL1B5z2amqbptaTvJe4O/bw03AISObHgzc2pana/8XYO8ke7Sj89HtJUnSLMzpyDzJgSMPXwhMzXRfAxyfZK8khwFLgSuB9cDSNnN9T4ZJcmuqqoBPAi9q+58EXDSXPkmStLva7pF5knOBo4D9kmwCTgWOSrKMYUj8FuBVAFV1Q5Lzgc8BdwOvrap7Wp2TgUuARcDqqrqhPcUpwHlJ/hD4LPC+efvpJEnaDWw3zKvqhGmaZwzcqjoNOG2a9rXA2mnab2KY7S5JkubAK8BJktQ5w1ySpM4Z5pIkdc4wlySpc4a5JEmdM8wlSeqcYS5JUucMc0mSOmeYS5LUOcNckqTOGeaSJHXOMJckqXOGuSRJnTPMJUnqnGEuSVLnDHNJkjpnmEuS1DnDXJKkzhnmkiR1zjCXJKlzhrkkSZ0zzCVJ6pxhLklS5wxzSZI6Z5hLktQ5w1ySpM7tsbM7MJ+WrLx4h7a/ZdULJtQTSZLuPx6ZS5LUOcNckqTOGeaSJHXOMJckqXOGuSRJndtumCdZneT2JNePtP2vJF9Icm2SDyfZu7UvSfL9JNe021+N7HNEkuuSbExyepK09n2TrEtyY7vfZxI/qCRJC9VsjszPAo7Zqm0d8MSq+jngS8AbR9Z9uaqWtdurR9rPAFYAS9ttquZK4NKqWgpc2h5LkqRZ2m6YV9WngTu3avt4Vd3dHl4OHLytGkkOBB5WVZdVVQHnAMe11ccCZ7fls0faJUnSLMzHOfPfBD468viwJJ9N8qkkz2htBwGbRrbZ1NoADqiqzQDtfv956JMkSbuNsa4Al+RNwN3AB1rTZuDQqrojyRHAR5I8Acg0u9ccnm8Fw1A9hx566Nw6LUnSAjPnI/MkJwG/DPx6Gzqnqn5QVXe05auALwOPYzgSHx2KPxi4tS3f1obhp4bjb5/pOavqzKpaXlXLFy9ePNeuS5K0oMwpzJMcA5wC/GpVfW+kfXGSRW350QwT3W5qw+d3JXlam8X+MuCittsa4KS2fNJIuyRJmoXtDrMnORc4CtgvySbgVIbZ63sB69onzC5vM9d/EXhrkruBe4BXV9XU5LnXMMyMfxDDOfap8+yrgPOTvBL4KvDiefnJJEnaTWw3zKvqhGma3zfDthcCF86wbgPwxGna7wCO3l4/JEnS9LwCnCRJnTPMJUnqnGEuSVLnDHNJkjpnmEuS1DnDXJKkzhnmkiR1zjCXJKlzhrkkSZ0zzCVJ6pxhLklS5wxzSZI6Z5hLktQ5w1ySpM4Z5pIkdc4wlySpc4a5JEmdM8wlSeqcYS5JUucMc0mSOmeYS5LUOcNckqTOGeaSJHVuj53dAUmSFrolKy/eoe1vWfWCHdreI3NJkjpnmEuS1DnDXJKkzhnmkiR1zjCXJKlzhrkkSZ0zzCVJ6pxhLklS5wxzSZI6N6swT7I6ye1Jrh9p2zfJuiQ3tvt9WnuSnJ5kY5JrkzxlZJ+T2vY3JjlppP2IJNe1fU5Pkvn8ISVJWshme2R+FnDMVm0rgUurailwaXsM8DxgabutAM6AIfyBU4GnAkcCp079AdC2WTGy39bPJUmSZjCrMK+qTwN3btV8LHB2Wz4bOG6k/ZwaXA7sneRA4LnAuqq6s6q+CawDjmnrHlZVl1VVAeeM1JIkSdsxzjnzA6pqM0C737+1HwR8bWS7Ta1tW+2bpmm/jyQrkmxIsmHLli1jdF2SpIVjEhPgpjvfXXNov29j1ZlVtbyqli9evHiMLkqStHCME+a3tSFy2v3trX0TcMjIdgcDt26n/eBp2iVJ0iyME+ZrgKkZ6ScBF420v6zNan8a8O02DH8J8Jwk+7SJb88BLmnr7krytDaL/WUjtSRJ0nbsMZuNkpwLHAXsl2QTw6z0VcD5SV4JfBV4cdt8LfB8YCPwPeAVAFV1Z5K3Aevbdm+tqqlJda9hmDH/IOCj7SZJkmZhVmFeVSfMsOroabYt4LUz1FkNrJ6mfQPwxNn0RZIk3ZtXgJMkqXOGuSRJnTPMJUnqnGEuSVLnDHNJkjpnmEuS1DnDXJKkzhnmkiR1zjCXJKlzhrkkSZ0zzCVJ6tysrs2u/ixZefEObX/LqhdMqCeSpEnzyFySpM4Z5pIkdc4wlySpc4a5JEmdM8wlSeqcYS5JUucMc0mSOmeYS5LUOcNckqTOGeaSJHXOMJckqXOGuSRJnTPMJUnqnGEuSVLnDHNJkjrn95nvZDvyveN+57gkaToemUuS1DnDXJKkzhnmkiR1zjCXJKlzcw7zJI9Pcs3I7TtJ3pDkLUm+PtL+/JF93phkY5IvJnnuSPsxrW1jkpXj/lCSJO1O5jybvaq+CCwDSLII+DrwYeAVwDur6u2j2yc5HDgeeALwKOATSR7XVr8beDawCVifZE1VfW6ufZMkaXcyXx9NOxr4clV9JclM2xwLnFdVPwBuTrIROLKt21hVNwEkOa9ta5hLkjQL83XO/Hjg3JHHJye5NsnqJPu0toOAr41ss6m1zdQuSZJmYewwT7In8KvA37WmM4DHMAzBbwbeMbXpNLvXNtqne64VSTYk2bBly5ax+i1J0kIxH0fmzwOurqrbAKrqtqq6p6p+BLyXnwylbwIOGdnvYODWbbTfR1WdWVXLq2r54sWL56HrkiT1bz7C/ARGhtiTHDiy7oXA9W15DXB8kr2SHAYsBa4E1gNLkxzWjvKPb9tKkqRZGGsCXJIHM8xCf9VI858kWcYwVH7L1LqquiHJ+QwT2+4GXltV97Q6JwOXAIuA1VV1wzj9kiRpdzJWmFfV94BHbNV24ja2Pw04bZr2tcDacfoiSdLuyivASZLUOb8CVdIuza8JlrbPI3NJkjpnmEuS1DnDXJKkzhnmkiR1zjCXJKlzhrkkSZ0zzCVJ6pxhLklS5wxzSZI6Z5hLktQ5w1ySpM4Z5pIkdc4wlySpc4a5JEmdM8wlSeqcYS5JUucMc0mSOmeYS5LUOcNckqTOGeaSJHXOMJckqXOGuSRJnTPMJUnqnGEuSVLnDHNJkjpnmEuS1DnDXJKkzhnmkiR1zjCXJKlzhrkkSZ0bO8yT3JLkuiTXJNnQ2vZNsi7Jje1+n9aeJKcn2Zjk2iRPGalzUtv+xiQnjdsvSZJ2F/N1ZP7MqlpWVcvb45XApVW1FLi0PQZ4HrC03VYAZ8AQ/sCpwFOBI4FTp/4AkCRJ27bHhOoeCxzVls8G/gE4pbWfU1UFXJ5k7yQHtm3XVdWdAEnWAccA506ofztsycqLZ73tLateMMGeSJJ0b/NxZF7Ax5NclWRFazugqjYDtPv9W/tBwNdG9t3U2mZqlyRJ2zEfR+ZPr6pbk+wPrEvyhW1sm2naahvt9955+GNhBcChhx46l75KkrTgjH1kXlW3tvvbgQ8znPO+rQ2f0+5vb5tvAg4Z2f1g4NZttG/9XGdW1fKqWr548eJxuy5J0oIwVpgn+XdJHjq1DDwHuB5YA0zNSD8JuKgtrwFe1ma1Pw34dhuGvwR4TpJ92sS357Q2SZK0HeMOsx8AfDjJVK0PVtXHkqwHzk/ySuCrwIvb9muB5wMbge8BrwCoqjuTvA1Y37Z769RkOEmStG1jhXlV3QT8/DTtdwBHT9NewGtnqLUaWD1OfyRJ2h15BThJkjpnmEuS1DnDXJKkzhnmkiR1zjCXJKlzhrkkSZ0zzCVJ6pxhLklS5wxzSZI6Z5hLktQ5w1ySpM4Z5pIkdc4wlySpc4a5JEmdM8wlSeqcYS5JUucMc0mSOmeYS5LUOcNckqTOGeaSJHXOMJckqXOGuSRJnTPMJUnqnGEuSVLnDHNJkjpnmEuS1DnDXJKkzhnmkiR1zjCXJKlzhrkkSZ0zzCVJ6pxhLklS5/bY2R2QdP9ZsvLiWW97y6oXTLAnkubTnMM8ySHAOcAjgR8BZ1bVu5K8BfhtYEvb9A+qam3b543AK4F7gNdX1SWt/RjgXcAi4K+ratVc+yXdnwzHn/C1kHaecY7M7wb+a1VdneShwFVJ1rV176yqt49unORw4HjgCcCjgE8keVxb/W7g2cAmYH2SNVX1uTH6pg4ZBpI0N3MO86raDGxuy3cl+Txw0DZ2ORY4r6p+ANycZCNwZFu3sapuAkhyXtvWMJekzvlH+v1jXibAJVkCPBm4ojWdnOTaJKuT7NPaDgK+NrLbptY2U7skSZqFscM8yUOAC4E3VNV3gDOAxwDLGI7c3zG16TS71zbap3uuFUk2JNmwZcuW6TaRJGm3M1aYJ3kgQ5B/oKo+BFBVt1XVPVX1I+C9/GQofRNwyMjuBwO3bqP9PqrqzKpaXlXLFy9ePE7XJUlaMOYc5kkCvA/4fFX96Uj7gSObvRC4vi2vAY5PsleSw4ClwJXAemBpksOS7MkwSW7NXPslSdLuZpzZ7E8HTgSuS3JNa/sD4IQkyxiGym8BXgVQVTckOZ9hYtvdwGur6h6AJCcDlzB8NG11Vd0wRr8kSZqTXifsjTOb/R+Z/nz32m3scxpw2jTta7e1nyRJmpmXc5UkqXOGuSRJnTPMJUnqnGEuSVLn/NY0SVJ3ep11PikemUuS1DnDXJKkzhnmkiR1zjCXJKlzToCTdkFO7pG0IzwylySpcx6ZS1InHLHRTDwylySpc4a5JEmdM8wlSeqcYS5JUucMc0mSOmeYS5LUOcNckqTOGeaSJHXOMJckqXNeAU4LnlfNkrTQeWQuSVLnDHNJkjpnmEuS1DnDXJKkzhnmkiR1zjCXJKlzhrkkSZ0zzCVJ6pxhLklS5wxzSZI6t8tczjXJMcC7gEXAX1fVqp3cJUnaLXjJ4/7tEkfmSRYB7waeBxwOnJDk8J3bK0mS+rBLhDlwJLCxqm6qqh8C5wHH7uQ+SZLUhV1lmP0g4GsjjzcBT91JfdF2OCQnSbuWVNXO7gNJXgw8t6p+qz0+ETiyql631XYrgBXt4eOBL87yKfYD/mWeunt/1J1k7d7qTrJ2b3UnWbu3upOsbd3J1+6t7iRr70jdn66qxdOt2FWOzDcBh4w8Phi4deuNqupM4MwdLZ5kQ1Utn3v37t+6k6zdW91J1u6t7iRr91Z3krWtO/navdWdZO35qrurnDNfDyxNcliSPYHjgTU7uU+SJHVhlzgyr6q7k5wMXMLw0bTVVXXDTu6WJEld2CXCHKCq1gJrJ1R+h4fmd3LdSdbure4ka/dWd5K1e6s7ydrWnXzt3upOsva81N0lJsBJkqS521XOmUuSpDkyzCVJ6pxhLklS53aZCXCStCtIsn9V3b6z+7E9SR7JcCnsAtZX1Td2cpe0Ey24I/Mki5K8Ksnbkjx9q3VvHqPug5P8fpLfS/JTSV6eZE2SP0nykPF7fq/n+tI81fm5keUHJnlz6/MfJXnwGHVPTrJfW35skk8n+VaSK5I8aYy6H0ryG/P9ei4kSR4xDzUenmRVki8kuaPdPt/a9p6Pfk7znB8dY99HJjkjybuTPCLJW5Jcl+T8JAeO2a99t7o9ArgyyT5J9h2ndqu/NMkFST6X5Kap2zzU/S3gSuDXgBcBlyf5zTFrHjOy/PAk70tybZIPJjlgjLpXt/eex4zTv2nqLk/yyST/O8khSdYl+XaS9UmePEbdiWRI239iObLgwhx4D/BLwB3A6Un+dGTdr41R9yzgAOAw4GJgOfB2IMAZcy2a5K4k32m3u5LcBTxmqn2M/k71ecoq4LHAO4AHAX81Rt3XVNXU5QffBbyzqvYGThmz7lOB44CvtjfqF7aLCI2ttwBr+68a+aNpeQuBK5J8JckvjVH6fOCbwFFV9YiqegTwzNb2d2P09ykz3I4Alo3R37OAzzF8f8Mnge8DLwA+w3j/3mC4jOZVI7cNDN8VcXVbHtf7Gd4f7mZ4jc8B/mYe6v4e8OSqenlVnQQcwfD/bxx/NLL8DmAz8CsMF/V6zxh19wH2Bj6Z5Mokv5PkUWPUm/KXwJ8wvB//X+A9VfVwYGVbN1eTyhCYUI4AUFUL6gZcO7K8B8Nn+D4E7AV8doy617T7AN/gJx/ry+hzzqHunzP8Bz9gpO3meXotPjuyfA3wwHnq8xdHltfP9PrPtb/AQ4ETGa47sIXhDfE5Y74WlzC82T1ypO2RrW3dGHWfMsPtCGDzmH2+bmT5k8C/b8uPAzbMx+9vR9bNou49wP9pfd369v1x/1205a9ute6aMV/j/wZ8DHjSSNvN49Tcqv5V0/wuPzMPdS8F9hx5vCfwiTFrXj3T6zrO67xV3WcwBO032r+LFRP6dzHOe/1EMmT0dZzvHKmqBXnO/MdHclV1N7AiyakMbzJjD99WVSVZW+030B7P+cP6VfW6duRybpKPAH/BcA5sPjw8yQsZRmD2qqp/a885Vp+BC5KcBbwV+HCS3wEuBI4GvjpG3anX9C6Go5e/aUOdL2H4a/vjY9ReUlV/fK8nG84x/vGYw5PrgU8x/Gfc2rhH/A9Mskf7d/ygqloPUFVfSrLXGHW/kuT3gbOr6jaANoz6cu797YU76vPAq6rqxq1XJBmn7ugI4jlbrVs0Rl2q6u1JzgPe2fp4KvP3/w/gX5M8ALgxw1Uuvw7sP9diSX63LX6dYZTmIob+Hssw7D6O/Vv9AA9Lkqn3OeZpFLeqPgN8JsnrgGcDL2XuF0351yTPAR4OVJLjquojbdTqnjG6OdEMaXXnNUdgYU6A25DkmKr62FRDVf2PJF9nvGGMDUkeUlXfraofv/m380B3jVGXqroqybOAkxmC4afGqTfiU8CvtuXLkhxQVbdlmDgz52//qao3JXk5cC7waIa/WH8b+Ajw62P097vTPNedDEOp4w6n9hZgAO8G1iZZBXwsyZ8xHCEczTDSMlcvZfjj6FPtNSjgNobvQ3jJGHXfwsxv+q+boX02Lhr5v/fjc5ZJHsvsvzlxRlW1CXhxkl8B1gFznk8yjTe0eq8H3sYw1H7SGPUe2u6/3G5TLhqj5pT3jtQ/m+HbvLa094tx/r3dZw5QVd3DMCLysftuPmuvZhhm/xHwXOA17SDj6wzvR3M1qQyZqj2RHFmQV4BL8jMMf6kexPBGdSuwpqo+P4m6wBdqjBdypO7BDMF4M/CRcfvbav/sSJ9/xPy+Fsdx79fiokm9xvNQdx+GADuWnxwZTQXYqqr65hzrvohhCPU+oTJ1pDDHLk/VOAp4DcPQ+h4Mf3h8hOH7C+4eo+7PMPx7u7yqvjvSfq83sTnWPQi4ooe6W9dmOKJ7TFVdPx+1t3qeicySn8+6vf3+2vvboyZQ90iGA+b1SQ4HjmF4nx/7kuMz1P4isHacHFlwE+Da0dd5DENFVzIMg4ZhGHvlJOoyxsSTJKeM1L0C+DRDiI3V35E+f7DVu4L5ey2m+jxVd2p4b2Kv8bivRVV9s6pOqaqfqap92+1nq+oUhj9K5lr3gumCvNlnrnVH6v9DVb20qp5cVU+qqufX8FXAJ861ZpLXMxzJnQxcn+TYkdV/NP1eO1T3dfNc93WTqNtq36vPDHMzrh+3diY0S36auvvOR91WeyKv8wTrvh748ATqngqcDpyR5H8ynPp8CLAyyZvmWnc7tU8B/mCc2nM+2b6r3hiGdB44TfuewI27S90e+zzJ12I7z/vVnuqOWxu4DnhIW17CMGv7v7TH40wc6qruhPv8I4YRttHbv7X7m3a1uj3+/iZcdxHD6ZHvAA9r7Q9izElqk6y9EM+Z/4hh2OUrW7Uf2NbtLnUnWbu3uiS5dqZVDB8V2aXqTrj2ompDklV1SxvKvyDJTzP9RL6FWneStX8feBbwe1V1HUCSm6vqsDH7O6m60N/vb1J1767hnP73kny5qr7TnuP7ScZ9T55Y7YUY5m8ALk1yIz+Z2HQow2esT96N6k6ydm91YQi/5zJ8lnpUGD6juqvVnWTtbyRZVlXXAFTVd5P8MrAamPNFfzqsO7HaNaFZ8pOq2/T2+5tU3R8meXBVfY/hI6bAcK0Kxj/AmljthToB7gEMlzk8iOGNbxPD56HH+bhCd3UnWbvDuu8D3l9V/zjNug9W1X/elepOsnaSgxmOEO5z+c8kT6+qf9od6k669kidXwHexPDxyEeOW29SdXv7/U2w7l5V9YNp2vcDDpwaEdnlai/EMJeknS0TmiU/qbrq24KbzS5JO9sEZ8lPpK76txDPmUvSzvbbwBHtPO4SholZS6rqXYw3OWtSddU5w1yS5l9vM7jVOYfZJWn+fSPJj78prgXwLzNcInXsGdwTqKvOOQFOkuZZbzO41T/DXJKkzjnMLklS5wxzSZI6Z5hLktQ5w1xawJK8IcmD56nWUUn+foz9lyS5fhbb3dIubylplgxzqXMZzPR/+Q0MX7coaQEzzKUOtaPczyf5S+Bq4MQklyW5OsnfJXlIu/Tno4BPJvnkDHUWJTkryfVJrkvyO639sUk+keSfW83HtF0ekuSCJF9I8oEkadsfkeRTSa5KckmSA0fa/znJZcBrR5735Un+YuTx37cLoGzdv99IcmWSa5K8J8mieXkBpQXGMJf69XjgHODZwCuBZ1XVU4ANwO9W1enArcAzq+qZM9RYBhxUVU+sqicB72/tHwDeXVU/D/wHYHNrfzLD0f7hwKOBpyd5IPDnwIuq6giGr6A8rW3/fuD1VfULO/rDJflZ4KXA06tqGcOXivz6jtaRdgdezlXq11eq6vL2Hc6HA//UDpT3BC6bZY2bgEcn+XPgYuDjSR7KEPAfBqiqfwVota+sqk3t8TXAEuBbwBOBdW2bRcDm9h3Ne1fVp9pz/Q3wvB34+Y5m+M7n9a3ug4Dbd2B/abdhmEv9+n/tPsC6qjphRwtU1TeT/DzwXIZh8JcwHHnPZPS7mO9heA8JcMPWR99J9gZmuirV3dx7ZPCnptkmwNlV9cZt/hCSHGaXFoDLGYa7HwuQ5MFJHtfW3QU8dKYd26zxB1TVhcB/B55SVd8BNiU5rm2z13ZmxH8RWJzkF9r2D0zyhKr6FvDtJP+xbTc6RH4LsCzJA5IcAhw5Td1LgRcl2b/V3bd9oYikrXhkLnWuqrYkeTlwbpK9WvObgS8BZwIfTbJ5hvPmBwHvH5kNP3UUfCLwniRvBf4NePE2nv+HSV4EnN6G1vcA/gy4AXgFsDrJ94BLRnb7J+Bm4DqG7+W+epq6n0vyZoah/we0frwW+Mo2XxBpN+S12SVJ6pzD7JIkdc5hdmk3keQKYK+tmk+squt2Rn8kzR+H2SVJ6pzD7JIkdc4wlySpc4a5JEmdM8wlSeqcYS5JUtFYNgcAAAAHSURBVOf+P5FXa3OgsjidAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "X.groupby('ret_schedule').tokens.count().plot.bar(ylim=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Text docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = X.tokens\n",
    "y = X.ret_schedule\n",
    "X_train, X_test, y_train, y_test = train_test_split(X1,y,test_size = 0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "accuracy 0.5938965468239307\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          02       0.44      0.89      0.59      4011\n",
      "          03       0.93      0.36      0.52      1519\n",
      "          04       0.96      0.77      0.85      2415\n",
      "          05       0.95      0.48      0.64      2685\n",
      "          06       0.00      0.00      0.00        21\n",
      "          07       1.00      0.01      0.02       786\n",
      "          10       0.00      0.00      0.00       294\n",
      "          11       1.00      0.10      0.18       792\n",
      "          16       1.00      0.00      0.01       487\n",
      "          20       0.99      0.15      0.26       813\n",
      "          21       1.00      0.00      0.01       775\n",
      "          23       0.99      0.42      0.59      2085\n",
      "          24       0.50      0.98      0.66      5787\n",
      "         24b       1.00      0.02      0.04       449\n",
      "          25       0.00      0.00      0.00        70\n",
      "          27       0.93      0.11      0.20       747\n",
      "          28       0.97      0.14      0.25       588\n",
      "          32       0.00      0.00      0.00       674\n",
      "          33       0.69      0.80      0.74      3150\n",
      "\n",
      "    accuracy                           0.59     28148\n",
      "   macro avg       0.70      0.28      0.29     28148\n",
      "weighted avg       0.73      0.59      0.53     28148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(X_test)\n",
    "print('accuracy %s' % accuracy_score(y_pred,y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Support Vector Machine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "accuracy 0.8010515844820236\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          02       0.80      0.75      0.77      4011\n",
      "          03       0.73      0.94      0.82      1519\n",
      "          04       0.92      0.88      0.90      2415\n",
      "          05       0.83      0.80      0.81      2685\n",
      "          06       1.00      0.10      0.17        21\n",
      "          07       0.84      0.60      0.70       786\n",
      "          10       0.87      0.30      0.45       294\n",
      "          11       0.52      0.63      0.57       792\n",
      "          16       0.92      0.89      0.90       487\n",
      "          20       0.82      0.70      0.76       813\n",
      "          21       0.86      0.48      0.62       775\n",
      "          23       0.86      0.87      0.87      2085\n",
      "          24       0.81      0.91      0.86      5787\n",
      "         24a       0.00      0.00      0.00         0\n",
      "         24b       0.88      0.95      0.91       449\n",
      "          25       0.89      0.24      0.38        70\n",
      "          27       0.82      0.65      0.72       747\n",
      "          28       0.86      0.67      0.75       588\n",
      "          32       0.81      0.51      0.63       674\n",
      "          33       0.72      0.84      0.77      3150\n",
      "\n",
      "    accuracy                           0.80     28148\n",
      "   macro avg       0.79      0.64      0.67     28148\n",
      "weighted avg       0.81      0.80      0.80     28148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', SGDClassifier(loss= 'hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "              ])\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "%time\n",
    "\n",
    "y_pred = sgd.predict(X_test)\n",
    "print('accuracy %s' % accuracy_score(y_pred,y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15.6 ms\n",
      "accuracy 0.8967955094500497\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          02       0.87      0.87      0.87      4011\n",
      "          03       0.83      0.87      0.85      1519\n",
      "          04       0.97      0.95      0.96      2415\n",
      "          05       0.92      0.91      0.91      2685\n",
      "          06       0.85      0.52      0.65        21\n",
      "          07       0.85      0.83      0.84       786\n",
      "          10       0.87      0.76      0.81       294\n",
      "          11       0.60      0.89      0.71       792\n",
      "          16       1.00      0.95      0.97       487\n",
      "          20       0.88      0.85      0.87       813\n",
      "          21       0.86      0.84      0.85       775\n",
      "          23       0.93      0.94      0.93      2085\n",
      "          24       0.94      0.94      0.94      5787\n",
      "         24b       0.99      0.96      0.97       449\n",
      "          25       0.90      0.67      0.77        70\n",
      "          27       0.89      0.85      0.87       747\n",
      "          28       0.93      0.76      0.84       588\n",
      "          32       0.77      0.74      0.75       674\n",
      "          33       0.93      0.90      0.91      3150\n",
      "\n",
      "    accuracy                           0.90     28148\n",
      "   macro avg       0.88      0.84      0.86     28148\n",
      "weighted avg       0.90      0.90      0.90     28148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression(n_jobs=1, C=1e5)),\n",
    "               ])\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "%time\n",
    "\n",
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       02    03    04    05  06   07   10   11   16   20   21    23    24  24b  25   27   28   32    33\n",
      "02   3501   183    24    20   0   23    2   49    1    8   29    18    50    0   0    4    5   24    70\n",
      "03    155  1322    14    11   0    0    0    1    0    0    0     1     6    0   0    0    0    7     2\n",
      "04     20    45  2287     2   0    2    0    5    0    0    2     0    13    0   0    0    0   33     6\n",
      "05     32     8     4  2432   2   22    4   43    1    7   16    10    52    0   0   16    0    9    27\n",
      "06      0     0     0     3  11    0    0    1    0    1    0     0     3    1   0    0    0    0     1\n",
      "07     26     2     2    14   0  651   11   16    0    1    8    22    14    0   1    2    2    5     9\n",
      "10      5     1     0    11   0    5  223    9    0    1    0     3    28    0   0    3    0    1     4\n",
      "11      7     1     0     5   0    3    0  707    0    6    2    16    23    0   0    2   14    1     5\n",
      "16     12     0     0     3   0    0    0    2  464    1    0     0     3    0   0    0    0    0     2\n",
      "20     15     0     2    10   0    1    3   22    0  692    3     4    11    1   0    1    1   24    23\n",
      "21     35     3     3    11   0   10    1    3    0    3  650    22    12    0   0    2    1    2    17\n",
      "23     16     4     5    13   0   14    0   24    0    3   18  1955    21    0   0    2    2    0     8\n",
      "24     33     8     6    32   0    4    7  104    0    7    7    31  5459    0   1   35    3   21    29\n",
      "24b     1     0     0     1   0    0    0    0    0    0    0     0    11  430   0    2    0    3     1\n",
      "25      2     0     0     3   0    3    0    5    0    4    0     1     2    0  47    0    1    0     2\n",
      "27      7     2     0    15   0    2    1   19    0    5    1     9    36    0   0  637    1    9     3\n",
      "28     24     3     0     9   0    6    2   76    0    0    0     4     8    0   0    1  448    3     4\n",
      "32     36    12    15    21   0    0    0   10    0   32    3     2    29    0   0    3    1  499    11\n",
      "33     75     7     1    17   0   17    2   91    0   12   13    12    54    3   3    3    4    8  2828\n"
     ]
    }
   ],
   "source": [
    "cm =confusion_matrix(y_test, y_pred)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "labels = np.unique(y_test)\n",
    "df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word2Vec and Logistic Regression**\n",
    "\n",
    "Use word similarities - word embeddingsconfusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "init_sims() got an unexpected keyword argument 'replce'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-92f4a0fb404c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mwv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"GoogleNews-vectors-negative300.bin.gz\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_sims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: init_sims() got an unexpected keyword argument 'replce'"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "wv.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us find a weighted average of the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, token) for token in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\svenkata\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The following function applies word2vec average to the 'tokens' column\n",
    "* This is taking very long time and throwing memory error\n",
    "* Need to work on Mark Bell's powerful GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 18.0 GiB for an array with shape (16066345, 300) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-0e1248d018d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mtrain_tokenized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[1;31m#train.apply(lambda r: w2v_tokenize_text(r['tokens']), axis=1).values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mX_train_word_average\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_averaging_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_tokenized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mX_test_word_average\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_averaging_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_tokenized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-93-8e62d3bd3095>\u001b[0m in \u001b[0;36mword_averaging_list\u001b[1;34m(wv, text_list)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mdef\u001b[0m  \u001b[0mword_averaging_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_averaging\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext_list\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-93-8e62d3bd3095>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mdef\u001b[0m  \u001b[0mword_averaging_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_averaging\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext_list\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-93-8e62d3bd3095>\u001b[0m in \u001b[0;36mword_averaging\u001b[1;34m(wv, words)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 18.0 GiB for an array with shape (16066345, 300) and data type float32"
     ]
    }
   ],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "X1 = X[['tokens','ret_schedule']].copy()\n",
    "train, test = train_test_split(X, test_size=0.3, random_state = 42)\n",
    "\n",
    "test_tokenized = test.tokens #test.apply(lambda r: w2v_tokenize_text(r['tokens']), axis=1).values\n",
    "train_tokenized = train.tokens #train.apply(lambda r: w2v_tokenize_text(r['tokens']), axis=1).values\n",
    "\n",
    "X_train_word_average = word_averaging_list(wv,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(wv,test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "log_reg = log_reg.fit(X_train_word_average, train['ret_schedule'])\n",
    "y_pred = log_reg.predict(X_test_word_average)\n",
    "print('accuracy %s' % accuracy_score(y_pred, test.ret_schedule))\n",
    "print(classification_report(test.ret_schedule, y_pred,target_names=np.unique(test.ret_schedule)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm =confusion_matrix(test.ret_schedule, y_pred)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "labels = np.unique(test.ret_schedule)\n",
    "df = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "print(df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Doc2vec and Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "\n",
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the post.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(TaggedDocument(v.split(), [label]))\n",
    "    return labeled\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.tokens, X.ret_schedule, random_state=0, test_size=0.3)\n",
    "X_train = label_sentences(X_train, 'Train')\n",
    "X_test = label_sentences(X_test, 'Test')\n",
    "all_data = X_train + X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 93824/93824 [00:00<00:00, 106180.61it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data)])\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors\n",
    "    \n",
    "train_vectors_dbow = get_vectors(model_dbow, len(X_train), 300, 'Train')\n",
    "test_vectors_dbow = get_vectors(model_dbow, len(X_test), 300, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg.fit(train_vectors_dbow, y_train)\n",
    "logreg = logreg.fit(train_vectors_dbow, y_train)\n",
    "y_pred = logreg.predict(test_vectors_dbow)\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finally some deep learning with keras**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#import numpy as np\n",
    "#import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils\n",
    "\n",
    "train_size = int(len(df) * .7)\n",
    "train_posts = X['tokens'][:train_size]\n",
    "train_tags = X['ret_schedule'][:train_size]\n",
    "\n",
    "test_posts = X['tokens'][train_size:]\n",
    "test_tags = X['ret_schedule'][train_size:]\n",
    "\n",
    "max_words = 1000\n",
    "tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
    "tokenize.fit_on_texts(train_posts) # only fit on train\n",
    "\n",
    "x_train = tokenize.texts_to_matrix(train_posts)\n",
    "x_test = tokenize.texts_to_matrix(test_posts)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
